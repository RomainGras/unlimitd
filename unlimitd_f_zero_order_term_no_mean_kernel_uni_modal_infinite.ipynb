{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e490cb-34a9-4163-9053-f24161a24b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_UNIFIED_MEMORY=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_UNIFIED_MEMORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f9178a-a7e8-45a5-a963-d606cbdfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unlimtd_f\n",
    "import time\n",
    "from jax import random, jit, pmap, value_and_grad, lax, vmap\n",
    "import dataset_multi_infinite\n",
    "import dataset_lines_infinite\n",
    "import test\n",
    "import plots\n",
    "import ntk\n",
    "import nll\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "import pickle\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a7d732-78fc-4f92-8ca7-d8f7cb2a07fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655235988902897757\n"
     ]
    }
   ],
   "source": [
    "seed = 1655235988902897757\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815a68c8-dec0-4665-be22-23a13d4f2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise):\n",
    "    \"\"\"\n",
    "    Computes the NLL of this data (one task only) wrt the kernel\n",
    "    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\n",
    "    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\n",
    "    \"\"\"\n",
    "    cov_a_a = kernel_self(x_a)\n",
    "    K = cov_a_a.shape[0]\n",
    "    cov_a_a = cov_a_a + maddox_noise ** 2 * np.eye(K)\n",
    "    \n",
    "    # prior mean is 0\n",
    "    y_a = np.reshape(y_a, (-1))\n",
    "\n",
    "    L = jax.scipy.linalg.cho_factor(cov_a_a)\n",
    "    ypred_a = np.reshape(part_apply_fn(inputs = x_a), (-1,))\n",
    "    alpha = jax.scipy.linalg.cho_solve(L, y_a - ypred_a)\n",
    "    return 0.5 * (y_a - ypred_a).T @ alpha + np.sum(np.log(np.diag(L[0]))) + 0.5 * K * np.log(2 * np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f4dea8a-7338-4799-8dc1-21fbf6e80e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian):\n",
    "    \"\"\"\n",
    "    NLL for a batch of tasks, when there is only one kernel (singGP)\n",
    "    x_a is (n_tasks, batch_size, input_dims) (input_dims are (128, 128, 1) for vision, (1,) for toy problems)\n",
    "    y_a is (n_tasks, batch_size, reg_dim)\n",
    "    \"\"\"\n",
    "    def f(carry, task_data):\n",
    "        x_a, y_a = task_data\n",
    "        loss_here = nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise)\n",
    "        return None, loss_here\n",
    "\n",
    "    _, losses = lax.scan(f, None, (x_a, y_a))\n",
    "\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b278f0ab-5999-4750-96ac-3e9bd2472864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x1, x2, theta):\n",
    "    l = theta\n",
    "    # Now x1 and x2 are compatible for broadcasting\n",
    "    # Compute squared Euclidean distance\n",
    "    squared_diff = (x1 - x2) ** 2 / (2*l**2)\n",
    "    return np.exp(-np.mean(squared_diff, axis=-1))\n",
    "\n",
    "def CosSim_kernel(x1, x2, theta):\n",
    "    normalized_factor = np.linalg.norm(x1, axis=-1)*np.linalg.norm(x2, axis=-1)\n",
    "    return theta * np.dot(x1, x2)/normalized_factor\n",
    "\n",
    "# Apply vmap to vectorize kernel function over pairs of inputs\n",
    "# vectorized_kernel = vmap(kernel, in_axes=(0,0, None))\n",
    "vectorized_kernel = vmap(vmap(CosSim_kernel, in_axes=(None, 0, None)), in_axes=(0, None, None))\n",
    "\n",
    "\n",
    "def kernel_matrix(x1, x2, theta):\n",
    "    matrix = vectorized_kernel(x1[:, np.newaxis], x2[np.newaxis, :], theta)\n",
    "    # For matern_kernel\n",
    "    # matrix[np.isnan(matrix)] = sigma**2\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7eb549f-9988-4c10-a690-b1ec850b8123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_and_jac_identity_cov(apply_fn, current_params, theta, current_batch_stats):\n",
    "    \"\"\"\n",
    "    Returns the kernel in case of an identity covariance matrix (JJ^\\top)\n",
    "    Also returns the jacobian J\n",
    "    \"\"\"\n",
    "    jacobian = ntk.get_jacobian(apply_fn, current_params, current_batch_stats)\n",
    "\n",
    "    def kernel(x1, x2):\n",
    "        return kernel_matrix(jacobian(x1), jacobian(x2), theta)\n",
    "\n",
    "    def kernel_self(x1):\n",
    "        j1 = jacobian(x1)\n",
    "        return kernel_matrix(j1, j1, theta)\n",
    "\n",
    "    return kernel, kernel_self, jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b4f811d-a0ac-49ea-ad24-39f3f56142b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def nll_batch_average_identity_cov(current_params, apply_fn, theta, current_batch_stats, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_identity_cov(apply_fn, current_params, theta, current_batch_stats)\n",
    "    part_apply_fn = (partial(apply_fn, current_params, current_batch_stats))\n",
    "    \n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86f2b355-f145-46fb-9e58-299ac9a43530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_identity_cov(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_t) = value_and_grad(nll_batch_average_identity_cov, argnums = (0, 2) )(current_state.params,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.theta,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4f8da8f-2182-47e9-9af3-9aa84fcd0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_stats_updater(current_state, x_a):\n",
    "    # shape of x_a is (n_tasks, batch_size, inputs_dims...)\n",
    "    \n",
    "    batch_stats = current_state.batch_stats\n",
    "    \n",
    "    def f(old_batch_stats, _x_a):\n",
    "        # shape of _x_a is (batch_size, input_dims)\n",
    "        _, mutated_vars = current_state.apply_fn_raw({\"params\":current_state.params,\n",
    "                                                      \"batch_stats\": old_batch_stats},\n",
    "                                                     _x_a,\n",
    "                                                     mutable=[\"batch_stats\"])\n",
    "        \n",
    "        new_batch_stats = mutated_vars[\"batch_stats\"]\n",
    "        return new_batch_stats, None\n",
    "\n",
    "    batch_stats = dict(batch_stats)\n",
    "    print(type(batch_stats))\n",
    "    batch_stats, _ = lax.scan(f, batch_stats, x_a)\n",
    "    return batch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24a6bfef-9868-4c45-aece-3f4514c99f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def grad_applier_identity_cov(current_state, gradients_p, gradients_t, new_batch_stats):\n",
    "    return current_state.apply_gradients(grads_params=gradients_p, grads_theta=gradients_t, new_batch_stats=new_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b951430-4a3a-477f-947c-c0b71b69d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_identity_cov(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_t) = pmap(pmapable_loss_identity_cov,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_t = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_t)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_identity_cov(current_state, current_gradients_p, current_gradients_t, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad8598-b20a-4235-9a59-2b29010815f5",
   "metadata": {},
   "source": [
    "## Unlimitd f training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "868685ae-ec97-4fbf-9721-b9e476d8d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_and_jac_lowdim_cov(apply_fn, current_params, theta, current_scale, current_batch_stats, proj):\n",
    "    \"\"\"\n",
    "    Returns the kernel in case of a low-dimensional covariance matrix (J P^\\top s P J^\\top)\n",
    "    Also returns the jacobian J\n",
    "    \"\"\"\n",
    "\n",
    "    jacobian = ntk.get_jacobian(apply_fn, current_params, current_batch_stats)\n",
    "\n",
    "    def kernel(x1, x2):\n",
    "        A1 = np.linalg.multi_dot([jacobian(x1), proj.T, np.diag(current_scale)])\n",
    "        A2 = np.linalg.multi_dot([jacobian(x2), proj.T, np.diag(current_scale)])\n",
    "        return kernel_matrix(A1, A2, theta)\n",
    "\n",
    "    def kernel_self(x1):\n",
    "        A1 = np.linalg.multi_dot([jacobian(x1), proj.T, np.diag(current_scale)])\n",
    "        return kernel_matrix(A1, A1, theta)\n",
    "\n",
    "    return kernel, kernel_self, jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c5acfed-43d4-48d9-ab79-44472a07f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_batch_average_lowdim_cov_singGP(current_params, current_scale, apply_fn, theta, current_batch_stats, proj, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_lowdim_cov(apply_fn, current_params, theta, current_scale, current_batch_stats, proj)\n",
    "    part_apply_fn = (partial(apply_fn, current_params, current_batch_stats))\n",
    "    \n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72165a52-b07d-4f90-b308-0773e747a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_lowdim_cov_singGP(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_s, gradients_t) = value_and_grad(nll_batch_average_lowdim_cov_singGP, argnums = (0, 1, 3) )(current_state.params,\n",
    "                                                              current_state.scale,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.theta,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              current_state.proj,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_s, gradients_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b845a68b-ee0f-473f-bf0f-1b105fa0bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def grad_applier_lowdim_cov_singGP(current_state, gradients_p, gradients_s, gradients_t, new_batch_stats):\n",
    "    return current_state.apply_gradients(grads_params=gradients_p, grads_scale=gradients_s, grads_theta=gradients_t, new_batch_stats=new_batch_stats)\n",
    "\n",
    "def step_lowdim_cov_singGP(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_s, unaveraged_gradients_t) = pmap(pmapable_loss_lowdim_cov_singGP,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_s = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_s)\n",
    "    current_gradients_t = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_t)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_lowdim_cov_singGP(current_state, current_gradients_p, current_gradients_s, current_gradients_t, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "154087b2-a619-4bc9-8b46-8ebf39ae7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(key, step, n_epochs, state, n_tasks, K, data_noise, maddox_noise, get_train_batch_fn, eval_during_training_fn):\n",
    "    \"\"\"\n",
    "    Available step functions:\n",
    "    * step_identity_cov\n",
    "    * step_lowdim_cov_singGP\n",
    "    * step_lowdim_cov_mixture\n",
    "\n",
    "    Available get_train_batch_fn functions:\n",
    "    * dataset_sines_infinite.get_training_batch\n",
    "    * dataset_sines_finite.get_training_batch\n",
    "    * dataset_lines_infinite.get_training_batch\n",
    "    * dataset_multi_infinite.get_training_batch\n",
    "    * dataset_shapenet1d.get_training_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "    print(\"Starting training with:\")\n",
    "    print(f\"-n_epochs={n_epochs}\")\n",
    "    print(f\"-n_tasks={n_tasks}\")\n",
    "    print(f\"-K={K}\")\n",
    "    print(f\"-data_noise={data_noise}\")\n",
    "    print(f\"-maddox_noise={maddox_noise}\")\n",
    "\n",
    "    losses = []\n",
    "    evals = []\n",
    "    t = time.time_ns()\n",
    "\n",
    "    for epoch_index in range(n_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        state, current_loss = step(subkey, state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn)\n",
    "\n",
    "        if(np.isnan(current_loss)):\n",
    "            print(\"Nan, aborting\")\n",
    "            break\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "\n",
    "        if epoch_index % 10 == 0:\n",
    "            print(f\"{epoch_index}  | {current_loss:.4f} ({(time.time_ns() - t)/ 10**9:.4f} s)\")\n",
    "        t = time.time_ns()\n",
    "\n",
    "        if epoch_index % 200 == 0:\n",
    "            key, subkey = random.split(key)\n",
    "            current_eval = eval_during_training_fn(subkey, state)\n",
    "            evals.append( current_eval )\n",
    "            print(f\"Eval: {current_eval}\")\n",
    "    print(\"Completed training\")\n",
    "\n",
    "    return state, losses, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc73a5c7-6d2d-4e1a-af84-86dccdf1019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "\n",
    "from flax import core\n",
    "from flax import struct\n",
    "from jax import numpy as np\n",
    "import optax\n",
    "\n",
    "#Train state for the identity covariance training (UNLIMTD-I, or the first part of UNLIMTD-F)\n",
    "\n",
    "class TrainStateIdentityCovariance(struct.PyTreeNode):\n",
    "    step: int\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    apply_fn_raw: Callable = struct.field(pytree_node=False)\n",
    "    params: core.FrozenDict[str, Any]\n",
    "    tx_params: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    theta: np.ndarray\n",
    "    tx_theta: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    batch_stats: core.FrozenDict[str, Any]\n",
    "    opt_state_params: optax.OptState\n",
    "    opt_state_theta: optax.OptState\n",
    "    \n",
    "    def apply_gradients(self, *, grads_params, new_batch_stats, grads_theta, **kwargs):\n",
    "        \"\"\"\n",
    "        Updates both the params and the scaling matrix\n",
    "        Also requires new_batch_stats to keep track of what has been seen by the network\n",
    "        \"\"\"\n",
    "\n",
    "        # params part\n",
    "        updates_params, new_opt_state_params = self.tx_params.update(grads_params, self.opt_state_params, self.params)\n",
    "        new_params = optax.apply_updates(self.params, updates_params)\n",
    "\n",
    "        # theta part\n",
    "        updates_theta, new_opt_state_theta = self.tx_theta.update(grads_theta, self.opt_state_theta, self.theta)\n",
    "        new_theta = optax.apply_updates(self.theta, updates_theta)\n",
    "\n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state_params=new_opt_state_params,\n",
    "            theta=new_theta,\n",
    "            opt_state_theta=new_opt_state_theta,\n",
    "            batch_stats=new_batch_stats,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, apply_fn_raw, params, theta, tx_params, tx_theta, batch_stats, **kwargs):\n",
    "        opt_state_params = tx_params.init(params)\n",
    "        opt_state_theta = tx_theta.init(theta)\n",
    "        return cls(\n",
    "            step=0,\n",
    "            apply_fn=apply_fn,\n",
    "            apply_fn_raw=apply_fn_raw,\n",
    "            params=params,\n",
    "            theta=theta,\n",
    "            tx_params=tx_params,\n",
    "            tx_theta=tx_theta,\n",
    "            batch_stats=batch_stats,\n",
    "            opt_state_params=opt_state_params,\n",
    "            opt_state_theta=opt_state_theta,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "466bcd57-c23a-4237-a150-99dab7505141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainStateLowDimCovSingGP(struct.PyTreeNode):\n",
    "    step: int\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    apply_fn_raw: Callable = struct.field(pytree_node=False)\n",
    "    params: core.FrozenDict[str, Any]\n",
    "    scale: np.ndarray\n",
    "    theta: np.ndarray\n",
    "    tx_params: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    tx_scale: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    tx_theta: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    batch_stats: core.FrozenDict[str, Any]\n",
    "    opt_state_params: optax.OptState\n",
    "    opt_state_scale: optax.OptState\n",
    "    opt_state_theta: optax.OptState\n",
    "    proj: np.ndarray\n",
    "    \n",
    "    def apply_gradients(self, *, grads_params, grads_theta, grads_scale, new_batch_stats, **kwargs):\n",
    "        \"\"\"\n",
    "        Updates both the params and the scaling matrix\n",
    "        Also requires new_batch_stats to keep track of what has been seen by the network\n",
    "        \"\"\"\n",
    "\n",
    "        # params part\n",
    "        updates_params, new_opt_state_params = self.tx_params.update(grads_params, self.opt_state_params, self.params)\n",
    "        new_params = optax.apply_updates(self.params, updates_params)\n",
    "\n",
    "        # scaling matrix part\n",
    "        updates_scale, new_opt_state_scale = self.tx_scale.update(grads_scale, self.opt_state_scale, self.scale)\n",
    "        new_scale = optax.apply_updates(self.scale, updates_scale)\n",
    "\n",
    "        # theta part\n",
    "        updates_theta, new_opt_state_theta = self.tx_theta.update(grads_theta, self.opt_state_theta, self.theta)\n",
    "        new_theta = optax.apply_updates(self.theta, updates_theta)\n",
    "\n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            scale=new_scale,\n",
    "            theta=new_theta,\n",
    "            opt_state_params=new_opt_state_params,\n",
    "            opt_state_scale=new_opt_state_scale,\n",
    "            opt_state_theta=new_opt_state_theta,\n",
    "            batch_stats=new_batch_stats,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, apply_fn_raw, params, scale, theta, tx_params, tx_scale, tx_theta, proj, batch_stats, **kwargs):\n",
    "        opt_state_params = tx_params.init(params)\n",
    "        opt_state_scale = tx_scale.init(scale)\n",
    "        opt_state_theta = tx_theta.init(theta)\n",
    "        return cls(\n",
    "            step=0,\n",
    "            apply_fn=apply_fn,\n",
    "            apply_fn_raw=apply_fn_raw,\n",
    "            params=params,\n",
    "            scale=scale,\n",
    "            theta=theta,\n",
    "            tx_params=tx_params,\n",
    "            tx_scale=tx_scale,\n",
    "            tx_theta=tx_theta,\n",
    "            batch_stats=batch_stats,\n",
    "            opt_state_params=opt_state_params,\n",
    "            opt_state_scale=opt_state_scale,\n",
    "            opt_state_theta=opt_state_theta,\n",
    "            proj=proj,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6ee0b3b9-4db5-4c4b-b438-8402aab5d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nll import gaussian_posterior\n",
    "\n",
    "def test_nll_one_kernel(key, part_apply_fn, kernel_self, jacobian, get_test_batch_fn, K, n_tasks, data_noise, maddox_noise):\n",
    "    \"\"\"\n",
    "    Returns the NLLs for n_tasks random tasks, in the singGP case.\n",
    "    \"\"\"\n",
    "    x_a, y_a, _, _ = get_test_batch_fn(key, n_tasks, K, 0, data_noise)\n",
    "    all_nlls = nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian)\n",
    "\n",
    "    return all_nlls\n",
    "\n",
    "def test_error_one_kernel(key, part_apply_fn, kernel, kernel_self, jacobian, get_test_batch_fn, error_fn, K, L, n_tasks, data_noise, maddox_noise):\n",
    "    \"\"\"\n",
    "    Returns the error for n_tasks random tasks, in the singGP case.\n",
    "    \"\"\"\n",
    "    x_a, y_a, x_b, y_b = get_test_batch_fn(key, n_tasks, K, L, data_noise)\n",
    "\n",
    "    def f(carry, task_data):\n",
    "        _x_a, _y_a, _x_b, _y_b = task_data\n",
    "        predictions = gaussian_posterior(kernel, kernel_self, _x_a, _y_a - part_apply_fn(_x_a), _x_b, maddox_noise)\n",
    "        return None, error_fn(predictions + part_apply_fn(_x_b), _y_b)\n",
    "    \n",
    "    _, all_errors = lax.scan(f, None, (x_a, y_a, x_b, y_b))\n",
    "\n",
    "    return all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa26b2e5-e61e-45e2-bc15-75c589e8a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer\n",
    "import ntk\n",
    "import test\n",
    "import train_states\n",
    "import models\n",
    "import utils\n",
    "import fim\n",
    "\n",
    "import dataset_sines_infinite\n",
    "import dataset_sines_finite\n",
    "import dataset_multi_infinite\n",
    "\n",
    "from jax import random\n",
    "from jax import numpy as np\n",
    "from flax.core import FrozenDict\n",
    "import optax\n",
    "\n",
    "def unlimtd_f_uni_modal_infinite(seed, pre_n_epochs, pre_n_tasks, pre_K, post_n_epochs, post_n_tasks, post_K, data_noise, maddox_noise, meta_lr, subspace_dimension):\n",
    "    key = random.PRNGKey(seed)\n",
    "    key_init, key = random.split(key)\n",
    "    \n",
    "    print(\"===============\")\n",
    "    print(\"This is UNLIMTD-F\")\n",
    "    print(\"For the uni-modal dataset: infinite sine dataset\")\n",
    "    print(\"This variant of UNLIMTD-F approaches the distribution with a single GP\")\n",
    "    print(\"===============\")\n",
    "    print(\"Creating model\")\n",
    "    model = models.small_network(20, \"relu\", 1)\n",
    "    batch = random.uniform(key_init, shape=(5,1), minval=-5, maxval=5)\n",
    "    init_vars = model.init(key_init, batch)\n",
    "    init_theta = np.ones(1)\n",
    "    apply_fn = utils.apply_fn_wrapper(model.apply, True)\n",
    "    apply_fn_raw = model.apply\n",
    "\n",
    "    # Training before finding the FIM matrix\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_identity_cov\n",
    "    get_train_batch_fn = dataset_sines_infinite.get_training_batch\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_theta = optax.adam(learning_rate = meta_lr)\n",
    "\n",
    "    pre_state = TrainStateIdentityCovariance.create(apply_fn=apply_fn, apply_fn_raw=apply_fn_raw, params=init_vars[\"params\"], theta=init_theta, tx_params=optimizer_params, tx_theta=optimizer_theta, batch_stats=FrozenDict())\n",
    "\n",
    "    def eval_during_pre_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_theta = state.theta\n",
    "        current_batch_stats = state.batch_stats\n",
    "        kernel, kernel_self, jacobian = get_kernel_and_jac_identity_cov(apply_fn, current_params, current_theta, current_batch_stats)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        part_apply_fn = partial(apply_fn, current_params, current_batch_stats)\n",
    "        nlls = test_nll_one_kernel(subkey_1, part_apply_fn, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "        mses = test_error_one_kernel(subkey_2, part_apply_fn, kernel, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, dataset_sines_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting first part of training (identity covariance)\")\n",
    "    key_pre, key = random.split(key)\n",
    "    pre_state, pre_losses, pre_evals = train_and_eval(key_pre, step, pre_n_epochs, pre_state, pre_n_tasks, pre_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_pre_training)\n",
    "    print(\"Finished first part of training\")\n",
    "\n",
    "    # FIM\n",
    "    print(\"Finding projection matrix\")\n",
    "    key_fim, key_data, key = random.split(key, 3)\n",
    "    # here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "    # P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "    P1 = fim.proj_sketch(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, batch_stats=pre_state.batch_stats, batches=random.uniform(key_data, shape=(100, 1761, 1), minval=-5, maxval=5), subspace_dimension=subspace_dimension)\n",
    "    print(\"Found projection matrix\")\n",
    "\n",
    "    # Usual training with projection\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_lowdim_cov_singGP\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_theta = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_scale = optax.adam(learning_rate = meta_lr)\n",
    "    init_scale = np.ones( (subspace_dimension,) )\n",
    "\n",
    "    post_state = TrainStateLowDimCovSingGP.create(apply_fn = apply_fn, apply_fn_raw=apply_fn_raw, params = pre_state.params, scale=init_scale, theta = pre_state.theta, tx_params = optimizer_params, tx_scale = optimizer_scale, tx_theta = optimizer_theta, batch_stats=pre_state.batch_stats, proj = P1)\n",
    "\n",
    "    def eval_during_post_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_theta = state.theta\n",
    "        current_batch_stats = state.batch_stats\n",
    "        current_scale = state.scale\n",
    "        kernel, kernel_self, jacobian = get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_theta, current_scale, current_batch_stats, P1)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        part_apply_fn = partial(apply_fn, current_params, current_batch_stats)\n",
    "        nlls = test_nll_one_kernel(subkey_1, part_apply_fn, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "        mses = test_error_one_kernel(subkey_2,part_apply_fn,  kernel, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, dataset_sines_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    key_post, key = random.split(key)\n",
    "    post_state, post_losses, post_evals = train_and_eval(key_post, step, post_n_epochs, post_state, post_n_tasks, post_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_post_training)\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # Returning everything\n",
    "    return init_vars, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24763136-051f-4eb5-aa90-7d962166d704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "This is UNLIMTD-F\n",
      "For the uni-modal dataset: infinite sine dataset\n",
      "This variant of UNLIMTD-F approaches the distribution with a single GP\n",
      "===============\n",
      "Creating model\n",
      "Creating optimizers\n",
      "Starting first part of training (identity covariance)\n",
      "Starting training with:\n",
      "-n_epochs=5000\n",
      "-n_tasks=24\n",
      "-K=10\n",
      "-data_noise=0.05\n",
      "-maddox_noise=0.05\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (481,) and (10,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m init_params, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals \u001b[38;5;241m=\u001b[39m \u001b[43munlimtd_f_uni_modal_infinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpre_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpre_n_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpre_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpost_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpost_n_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpost_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mdata_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mmeta_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43msubspace_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[79], line 59\u001b[0m, in \u001b[0;36munlimtd_f_uni_modal_infinite\u001b[0;34m(seed, pre_n_epochs, pre_n_tasks, pre_K, post_n_epochs, post_n_tasks, post_K, data_noise, maddox_noise, meta_lr, subspace_dimension)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting first part of training (identity covariance)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m key_pre, key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 59\u001b[0m pre_state, pre_losses, pre_evals \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_n_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_n_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_train_batch_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_during_pre_training\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished first part of training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# FIM\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[75], line 31\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(key, step, n_epochs, state, n_tasks, K, data_noise, maddox_noise, get_train_batch_fn, eval_during_training_fn)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     30\u001b[0m     key, subkey \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 31\u001b[0m     state, current_loss \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_train_batch_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misnan(current_loss)):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNan, aborting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m, in \u001b[0;36mstep_identity_cov\u001b[0;34m(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn)\u001b[0m\n\u001b[1;32m      3\u001b[0m x_a, y_a, x_a_div, y_a_div \u001b[38;5;241m=\u001b[39m get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compute loss and gradient through gpu parallelization\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_t) \u001b[38;5;241m=\u001b[39m \u001b[43mpmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpmapable_loss_identity_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                         \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mstatic_broadcasted_argnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_a_div\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a_div\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(unaveraged_losses)\n\u001b[1;32m     12\u001b[0m current_gradients_p \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m array: np\u001b[38;5;241m.\u001b[39mmean(array, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), unaveraged_gradients_p)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m, in \u001b[0;36mpmapable_loss_identity_cov\u001b[0;34m(current_state, x_a, y_a, maddox_noise)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpmapable_loss_identity_cov\u001b[39m(current_state, x_a, y_a, maddox_noise):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# we can't pass current_state because we have to explicitely show the variable\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     loss, (gradients_p, gradients_t) \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnll_batch_average_identity_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, (gradients_p, gradients_t)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[66], line 7\u001b[0m, in \u001b[0;36mnll_batch_average_identity_cov\u001b[0;34m(current_params, apply_fn, theta, current_batch_stats, x_a, y_a, maddox_noise)\u001b[0m\n\u001b[1;32m      4\u001b[0m _, kernel_self, jacobian \u001b[38;5;241m=\u001b[39m get_kernel_and_jac_identity_cov(apply_fn, current_params, theta, current_batch_stats)\n\u001b[1;32m      5\u001b[0m part_apply_fn \u001b[38;5;241m=\u001b[39m (partial(apply_fn, current_params, current_batch_stats))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnll_batch_one_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_apply_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjacobian\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[44], line 12\u001b[0m, in \u001b[0;36mnll_batch_one_kernel\u001b[0;34m(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian)\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss_here \u001b[38;5;241m=\u001b[39m nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, loss_here\n\u001b[0;32m---> 12\u001b[0m _, losses \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(losses)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m, in \u001b[0;36mnll_batch_one_kernel.<locals>.f\u001b[0;34m(carry, task_data)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(carry, task_data):\n\u001b[1;32m      8\u001b[0m     x_a, y_a \u001b[38;5;241m=\u001b[39m task_data\n\u001b[0;32m----> 9\u001b[0m     loss_here \u001b[38;5;241m=\u001b[39m \u001b[43mnll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_apply_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, loss_here\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mnll\u001b[0;34m(kernel_self, part_apply_fn, x_a, y_a, maddox_noise)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnll\u001b[39m(kernel_self, part_apply_fn, x_a, y_a, maddox_noise):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Computes the NLL of this data (one task only) wrt the kernel\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     cov_a_a \u001b[38;5;241m=\u001b[39m \u001b[43mkernel_self\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     K \u001b[38;5;241m=\u001b[39m cov_a_a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m     cov_a_a \u001b[38;5;241m=\u001b[39m cov_a_a \u001b[38;5;241m+\u001b[39m maddox_noise \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(K)\n",
      "Cell \u001b[0;32mIn[81], line 14\u001b[0m, in \u001b[0;36mget_kernel_and_jac_identity_cov.<locals>.kernel_self\u001b[0;34m(x1)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkernel_self\u001b[39m(x1):\n\u001b[1;32m     13\u001b[0m     j1 \u001b[38;5;241m=\u001b[39m jacobian(x1)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 18\u001b[0m, in \u001b[0;36mkernel_matrix\u001b[0;34m(x1, x2, theta)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkernel_matrix\u001b[39m(x1, x2, theta):\n\u001b[0;32m---> 18\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorized_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# For matern_kernel\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# matrix[np.isnan(matrix)] = sigma**2\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matrix\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[64], line 10\u001b[0m, in \u001b[0;36mCosSim_kernel\u001b[0;34m(x1, x2, theta)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCosSim_kernel\u001b[39m(x1, x2, theta):\n\u001b[1;32m      9\u001b[0m     normalized_factor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m theta \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mnormalized_factor\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3342\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   3340\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3341\u001b[0m     contract_dims \u001b[38;5;241m=\u001b[39m ((a_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (b_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m,))\n\u001b[0;32m-> 3342\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontract_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3343\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lax_internal\u001b[38;5;241m.\u001b[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/lax/lax.py:2610\u001b[0m, in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[1;32m   2608\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2609\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2610\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape, dimension_numbers)\n",
      "\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (481,) and (10,)."
     ]
    }
   ],
   "source": [
    "init_params, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals = unlimtd_f_uni_modal_infinite(seed=seed,\n",
    "                                                                                     pre_n_epochs=5000,\n",
    "                                                                                     pre_n_tasks=24,\n",
    "                                                                                     pre_K=10,\n",
    "                                                                                     post_n_epochs=5000,\n",
    "                                                                                     post_n_tasks=24,\n",
    "                                                                                     post_K=10,\n",
    "                                                                                     data_noise=0.05, \n",
    "                                                                                     maddox_noise=0.05,\n",
    "                                                                                     meta_lr=0.001,\n",
    "                                                                                     subspace_dimension=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2e0d5-c4c9-49eb-9c10-9b734e15a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output[\"seed\"] = seed\n",
    "output[\"pre_n_epochs\"]=30000\n",
    "output[\"pre_n_tasks\"]=24\n",
    "output[\"pre_K\"]=10\n",
    "output[\"post_n_epochs\"]=30000\n",
    "output[\"post_n_tasks\"]=24\n",
    "output[\"post_K\"]=10\n",
    "output[\"data_noise\"]=0.05\n",
    "output[\"maddox_noise\"]=0.05\n",
    "output[\"meta_lr\"]=0.001\n",
    "output[\"subspace_dimension\"]=10\n",
    "output[\"pre_losses\"]=pre_losses\n",
    "output[\"post_losses\"]=post_losses\n",
    "output[\"init_params\"]=init_params\n",
    "output[\"intermediate_params\"]=pre_state.params\n",
    "output[\"intermediate_theta\"]=pre_state.theta\n",
    "output[\"trained_params\"]=post_state.params\n",
    "output[\"trained_theta\"]=pre_state.theta\n",
    "output[\"intermediate_batch_stats\"]=pre_state.batch_stats\n",
    "output[\"trained_batch_stats\"]=post_state.batch_stats\n",
    "output[\"trained_scale\"]=post_state.scale\n",
    "output[\"proj\"]=post_state.proj\n",
    "output[\"pre_evals\"]=pre_evals\n",
    "output[\"post_evals\"]=post_evals\n",
    "\n",
    "print(output[\"trained_theta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2feddb-f127-4fa6-87c8-39c291866f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_zeroth_order_no_mean_kernel.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b512739-7cb9-4ada-b6e3-b6dfbdd81c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_zeroth_order_no_mean_kernel.pickle\", \"rb\") as handle:\n",
    "    output = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d670f-320d-4d4f-b4e8-c488e2396979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.small_network(20, \"relu\", 1)\n",
    "apply_fn = utils.apply_fn_wrapper(model.apply, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3980a3-ffcf-4e75-865c-3bea596accc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel, kernel_self, jac = get_kernel_and_jac_lowdim_cov(apply_fn, output[\"trained_params\"], output[\"trained_theta\"], output[\"trained_scale\"], output[\"trained_batch_stats\"], output[\"proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12543f-a9ef-4bf2-b379-2a95b8fa242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_posterior_full(kernel, kernel_self, x_a, y_a, x_b, maddox_noise):\n",
    "    \"\"\"\n",
    "    Computes the gaussian posterior with this kernel and this data, on the queried inputs.\n",
    "    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\n",
    "    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\n",
    "    Returns the posterior covariance matrix\n",
    "    \"\"\"\n",
    "    dim = y_a.shape[1]\n",
    "    y_a = np.reshape(y_a, (-1,))\n",
    "\n",
    "    cov_a_a = kernel_self(x_a)\n",
    "    cov_a_a = cov_a_a + maddox_noise ** 2 * np.eye(cov_a_a.shape[0])\n",
    "    cov_b_a = kernel(x_b, x_a)\n",
    "    print(cov_b_a.shape)\n",
    "    cov_b_b = kernel_self(x_b)\n",
    "    \n",
    "    L = scipy.linalg.cho_factor(cov_a_a)\n",
    "    alpha = scipy.linalg.cho_solve(L, y_a)\n",
    "    post_mean = cov_b_a @ alpha\n",
    "    \n",
    "    v = scipy.linalg.cho_solve(L, cov_b_a.T)\n",
    "    post_cov = cov_b_b - cov_b_a @ v\n",
    "    \n",
    "    return np.reshape(post_mean, (-1, dim) ), post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e894-9ccf-4674-a9e1-a93981b51b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from jax import scipy\n",
    "\n",
    "def plot_notebooks(key, part_apply_fn, kernel, kernel_self, jac, K, dataset_provider):\n",
    "    \"\"\"\n",
    "    Make an informative prediction plot in the singGP case (for the kernel specified)\n",
    "    K is the number of context inputs\n",
    "    Change dataset_provider to test on other datasets (e.g. dataset_sines_infinite)\n",
    "    \"\"\"\n",
    "    x, y, fun = dataset_provider.get_fancy_test_batch(key, K=10, L=0, data_noise=0.05)\n",
    "\n",
    "    x_a_all = x[0, :10]\n",
    "    y_a_all = y[0, :10]\n",
    "    x_b = np.linspace(-5, 5, 100)[:, np.newaxis]\n",
    "    y_b = fun(x_b)\n",
    "\n",
    "    y_min, y_max = np.min(y_b) - 0.5, np.max(y_b) + 0.5\n",
    "\n",
    "    x_a = x_a_all[:K]\n",
    "    y_a = y_a_all[:K]\n",
    "\n",
    "    affine_prediction, cov = gaussian_posterior_full(kernel, kernel_self, x_a, y_a - part_apply_fn(x_a), x_b, 0.05)\n",
    "    prediction = affine_prediction + part_apply_fn(x_b)\n",
    "\n",
    "    error = dataset_provider.error_fn(prediction, y_b)\n",
    "    loss = nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise=0.05)\n",
    "\n",
    "    variances = np.diag(cov)\n",
    "    stds = np.sqrt(variances)\n",
    "\n",
    "    plt.plot(x_b, y_b, \"g--\", label=\"Target\")\n",
    "    plt.plot(x_b, part_apply_fn(x_b), \"k--\", label=\"apply_fn\")\n",
    "    plt.plot(x_a, y_a, \"ro\", label=\"Context data\")\n",
    "    plt.plot(x_b, affine_prediction, \"r--\", label=\"affine_prediction\")\n",
    "    plt.plot(x_b, prediction, \"b\", label=\"Prediction\")\n",
    "    plt.fill_between(x_b[:, 0], prediction[:, 0] - 1.96 * stds, prediction[:, 0] + 1.96 * stds, color='blue', alpha=0.1, label=\"+/- 1.96$\\sigma$\")\n",
    "    plt.title(f\"NLL={loss:.4f}, MSE={error:.4f} ($K$={K})\")\n",
    "    plt.legend()\n",
    "    plt.gca().set_ylim([y_min, y_max])\n",
    "    plt.gca().set_xlabel(\"$x$\")\n",
    "    plt.gca().set_ylabel(\"$y$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a535e-3b83-4637-b609-47e65df82237",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ce2a6-e479-4d27-a69c-2f8e15b25782",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_apply_fn = partial(apply_fn, output[\"trained_params\"], output[\"trained_batch_stats\"])\n",
    "key, subkey = random.split(key)\n",
    "plot_notebooks(subkey, part_apply_fn, kernel, kernel_self, jac, 10, dataset_sines_infinite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "822931ff-66a0-4918-b769-043f7d79ae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretraining_nll_no_mean = [x[0] for x in output[\"pre_evals\"]]\n",
    "posttraining_nll_no_mean = [x[0] for x in output[\"post_evals\"]]\n",
    "\n",
    "n_samples_pretraining = len(pretraining_nll_no_mean)\n",
    "n_samples_posttraining = len(posttraining_nll_no_mean)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(range(n_samples_pretraining), pretraining_nll_no_mean, \"g\", label=\"pretraining_nll\")\n",
    "plt.plot(range(n_samples_posttraining-1, n_samples_posttraining+n_samples_posttraining-1), posttraining_nll_no_mean, \"r\", label=\"projtraining_nll\")\n",
    "plt.gca().set_ylim([-10, 20])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "78015f46-1245-49b5-88ba-f5ccbc9e2796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGiCAYAAAA1LsZRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABLnElEQVR4nO3de1wU5f4H8M+ysMt9UZCbImDeUMwLpqKZlxTzVp7T72SmoqklpaKS5jFTy2OHrLwc7WCZmukxs05WZpaSmuI9EYzQvCKgLiJedhGQhd35/TGH1Q1QFmFnL5/36zUvY/aZme+OvJqPzzPzjEwQBAFEREREEnGSugAiIiJybAwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCQps8PIvn37MHToUAQHB0Mmk+Hbb7994DZ79+5FVFQUXF1d0axZM3z00Ue1qZWIiIjskNlhpKioCO3bt8eHH35Yo/ZZWVkYNGgQevbsibS0NLzxxhuIj4/H119/bXaxREREZH9kD/OiPJlMhm+++QbDhg2rts2sWbOwdetWnDp1yrguLi4OJ06cwKFDh2p7aCIiIrITzvV9gEOHDiEmJsZk3YABA7BmzRqUlZXBxcWl0jalpaUoLS01/mwwGHDjxg34+vpCJpPVd8lERERUBwRBQGFhIYKDg+HkVP1gTL2Hkby8PAQEBJisCwgIQHl5OQoKChAUFFRpm8TERLz99tv1XRoRERFZQG5uLpo0aVLt5/UeRgBU6s2oGBmqrpdj9uzZSEhIMP6s0WjQtGlT5Obmwtvbu/4KrWOXtZfR5t9tIHeSo+D1AjjJ+PASERE5Dq1Wi5CQEHh5ed23Xb2HkcDAQOTl5Zmsy8/Ph7OzM3x9favcRqlUQqlUVlrv7e1tU2HE1cMV/dv0R6BnIJTuSri5uEldEhERkcU96BaLeg8j0dHR+P77703W7dy5E507d67yfhF7opArsHP0TqnLICIismpmjxvcvn0b6enpSE9PByA+upueno6cnBwA4hBLbGyssX1cXByys7ORkJCAU6dOYe3atVizZg1mzJhRN9+AiIiIbJrZPSPHjh1Dnz59jD9X3NsxZswYrFu3Dmq12hhMACA8PBzbt2/H9OnT8e9//xvBwcFYvnw5nn322Too3zbo9DroDXoO0xAREVXhoeYZsRStVguVSgWNRmNT94wAwLjvxuHT9E+xdMBSTOs2TepyiMjGCYKA8vJy6PV6qUshglwuh7Ozc7X3hNT0+m2Rp2kcmZdCvIP46u2rEldCRLZOp9NBrVajuLhY6lKIjNzd3REUFASFQlHrfTCM1LMAT3GOlatFDCNEVHsGgwFZWVmQy+UIDg6GQqHgJJAkKUEQoNPpcO3aNWRlZaFFixb3ndjsfhhG6lmAB8MIET08nU4Hg8GAkJAQuLu7S10OEQDAzc0NLi4uyM7Ohk6ng6ura632w1m46llFz0je7bwHtCQierDa/suTqL7Uxe8kf6vrWaBnIADeM0JERFQdhpF6VjFMk1+UDxt4cImIiMjiGEbqmb+HP2IeicGIdiNQqi998AZERCQJmUyGb7/9tsbt161bBx8fn3qrx5HwBtZ6pnRWYseoHVKXQURkd8LCwjBt2jRMmzatTvanVqvRoEGDGrcfPnw4Bg0aVCfHdnQMI0REZFV0Ot1DzVlxL71eD5lMVqObLAMDA83at5ubG9zcOLN2XeAwjYWUlpeipKxE6jKIyM4U6YqqXe6U36lx2z///6m6drXRu3dvTJ48GZMnT4aPjw98fX3x5ptvGu+jCwsLw8KFCzF27FioVCq89NJLAICDBw/iiSeegJubG0JCQhAfH4+ioiLjPrOzszF9+nTIZDLjnCsVQyfbtm1DmzZtoFQqkZ2djV9//RX9+/eHn58fVCoVevXqhePHj5vUee8wzcWLFyGTybBlyxb06dMH7u7uaN++PQ4dOmRs/+dhmrfeegsdOnTAhg0bEBYWBpVKheeffx6FhYXGNoWFhRg5ciQ8PDwQFBSEpUuXonfv3jXu3ak4V7GxsfD09ERoaCi+++47XLt2Dc888ww8PT3Rrl07HDt2zLhNdnY2hg4digYNGsDDwwNt27bF9u3bjZ+fPHkSgwYNgqenJwICAjB69GgUFBTUqJ66wjBiAWO/HQvXd1yxKnWV1KUQkZ3xTPSsdnn2S9N3gPl/4F9t24EbB5q0DftXWJXtauuzzz6Ds7Mzjhw5guXLl2Pp0qVYvXq18fP3338fkZGRSE1Nxdy5c5GRkYEBAwbgr3/9K3777Tds3rwZ+/fvx+TJkwEAW7ZsQZMmTbBgwQKo1Wqo1WrjvoqLi5GYmIjVq1cjMzMT/v7+KCwsxJgxY5CSkoLDhw+jRYsWGDRokElQqMqcOXMwY8YMpKeno2XLlhgxYgTKy8urbX/+/Hl8++232LZtG7Zt24a9e/fi3XffNX6ekJCAAwcOYOvWrUhOTkZKSkqlUPQgS5cuRY8ePZCWlobBgwdj9OjRiI2NxahRo3D8+HE0b94csbGxxrA3adIklJaWYt++fcjIyMCiRYvg6Sn+XarVavTq1QsdOnTAsWPH8NNPP+Hq1at47rnnzKrpYXGYxgK8leJ8/Jz4jIgcVUhICJYuXQqZTIZWrVohIyMDS5cuNfaC9O3b1+Rt7rGxsXjhhReMPQYtWrTA8uXL0atXL6xcuRINGzaEXC6Hl5dXpeGVsrIyJCUloX379sZ1ffv2NWnz8ccfo0GDBti7dy+GDBlSbd0zZszA4MGDAQBvv/022rZti3PnzqF169ZVtjcYDFi3bh28vMRXgYwePRq7du3CO++8g8LCQnz22Wf4/PPP8eSTTwIAPv30UwQHB9fkFBoNGjQIEydOBADMmzcPK1euxGOPPYa//e1vAIBZs2YhOjoaV69eRWBgIHJycvDss8+iXbt2AIBmzZoZ97Vy5Up06tQJ//znP43r1q5di5CQEJw5cwYtW7Y0q7baYhixAOMsrJxrhIjq2O3Zt6v9TO4kN/k5f0Z+tW2dZKYd5RenXnyouv6sW7duJtPXR0dHY/HixcYX/nXu3NmkfWpqKs6dO4eNGzca1wmCYJwWPyIiotpjKRQKPProoybr8vPzMW/ePOzevRtXr16FXq9HcXGxyVvmq3LvfoKCgoz7qi6MhIWFGYNIxTb5+eJ5v3DhAsrKytClSxfj5yqVCq1atbpvDferKSBAvL5UBI171+Xn5yMwMBDx8fF45ZVXsHPnTvTr1w/PPvuscR+pqanYs2ePsafkXufPn2cYsSd8Pw0R1RcPhYfkbeuCh4fp8QwGAyZOnIj4+PhKbZs2bXrffbm5uVV6b8/YsWNx7do1LFu2DKGhoVAqlYiOjoZOp7vvvlxcXIz/XbFPg8FQo/YV21S0rxg2+XNt5s5BVVVN96tzwoQJGDBgAH744Qfs3LkTiYmJWLx4MaZMmQKDwYChQ4di0aJFlY5TEb4sgWHEAip6RjglPBE5qsOHD1f6uUWLFpDL5VW279SpEzIzM9G8efNq96lQKIw9Kw+SkpKCpKQk46O4ubm5Fr9J85FHHoGLiwuOHj2KkJAQAIBWq8XZs2fRq1evej12SEgI4uLiEBcXh9mzZ+OTTz7BlClT0KlTJ3z99dcICwuDs7N0kYA3sFqAcUp49owQkYPKzc1FQkICTp8+jU2bNmHFihWYOnVqte1nzZqFQ4cOYdKkSUhPT8fZs2exdetWTJkyxdgmLCwM+/btw+XLlx8YLJo3b44NGzbg1KlTOHLkCEaOHGnxx3K9vLwwZswYzJw5E3v27EFmZibGjRsHJyenen0D87Rp07Bjxw5kZWXh+PHj2L17t3GYa9KkSbhx4wZGjBiBo0eP4sKFC9i5cyfGjRtX46BXFxhGLKBimIZTwhORo4qNjUVJSQm6dOmCSZMmYcqUKXj55Zerbf/oo49i7969OHv2LHr27ImOHTti7ty5JkMHCxYswMWLF/HII4+gUaNG9z3+2rVrcfPmTXTs2BGjR49GfHw8/P396+z71dSSJUsQHR2NIUOGoF+/fujRowciIiJq/bbbmtDr9Zg0aRIiIiLw1FNPoVWrVkhKSgIABAcH48CBA9Dr9RgwYAAiIyMxdepUqFQqi76UUSbYwNVRq9VCpVJBo9HA29tb6nLMVlpeimGbhyHAIwAfD/kYSmel1CURkY25c+cOsrKyEB4eXq8XrvrQu3dvdOjQAcuWLZO6FKtTVFSExo0bY/HixRg/frzU5dTK/X43a3r95j0jFqB0VuLHkT9KXQYREUksLS0Nf/zxB7p06QKNRoMFCxYAAJ555hmJK5MWwwgREZEFffDBBzh9+jQUCgWioqKQkpICPz8/pKSkYODAgdVud/t29Y9x2zqGEQu6U34HgiDAzYXvMiAix/HLL79IXYLV6NixI1JTU6v8rHPnzkhPT7dsQVaCYcRCxnw7ButPrMe/nvoX4rtWfm6eiIgcm5ub230fZbZnfJrGQrwV/5sSnrOwEhERmWAYsRDOwkpERFQ1hhEL4SysREREVWMYsRD2jBAREVWNYcRCjFPC854RIiIiEwwjFlIxTHO16CqnhCciqgO9e/fGtGnT6m3/MpkM3377bY3br1u3Dj4+PvVWjz3jo70WEuAZgJhHYtDYqzF0eh2nhCciekhbtmyBi4uL8eewsDBMmzatzgKKWq1GgwYNatx++PDhxrcCk3kYRizE1dkVO0btkLoMIiKrp9PpoFAoHtiuYcOGZu9br9dDJpPV6CVwgYGBZu3bzc3N4m8CthccpiEismVFRdUvd+7UvG1JSc3a1kLv3r0xefJkTJ48GT4+PvD19cWbb75pHLIOCwvDwoULMXbsWKhUKrz00ksAgK+//hpt27aFUqlEWFgYFi9eXGm/Fb0gvXv3RnZ2NqZPnw6ZTAaZTAbg7tDJtm3b0KZNGyiVSmRnZ+PXX39F//794efnB5VKhV69euH48eMm+793mObixYuQyWTYsmUL+vTpA3d3d7Rv3x6HDh0ytv/zMM1bb72FDh06YMOGDQgLC4NKpcLzzz+PwsJCY5vCwkKMHDkSHh4eCAoKwtKlS80afqo4d7GxsfD09ERoaCi+++47XLt2Dc888ww8PT3Rrl07HDt2zLhNdnY2hg4digYNGsDDwwNt27bF9u3bjZ+fPHkSgwYNgqenJwICAjB69GgUFBTUqJ7aYhixsJKyEtzW2e/7BYjIwjw9q1+efda0rb9/9W3//E6UsLCq29XSZ599BmdnZxw5cgTLly/H0qVLsXr1auPn77//PiIjI5Gamoq5c+ciNTUVzz33HJ5//nlkZGTgrbfewty5c7Fu3boq979lyxY0adIECxYsgFqthlqtNn5WXFyMxMRErF69GpmZmfD390dhYSHGjBmDlJQUHD58GC1atMCgQYNMgkJV5syZgxkzZiA9PR0tW7bEiBEjUF5eXm378+fP49tvv8W2bduwbds27N27F++++67x84SEBBw4cABbt25FcnIyUlJSKoWiB1m6dCl69OiBtLQ0DB48GKNHj0ZsbCxGjRqF48ePo3nz5oiNjTWGv0mTJqG0tBT79u1DRkYGFi1aBM///d2q1Wr06tULHTp0wLFjx/DTTz/h6tWreO6558yqyWyCDdBoNAIAQaPRSF3KQ5myfYqAtyC8/cvbUpdCRDampKREOHnypFBSUmL6AVD9MmiQaVt39+rb9upl2tbPr+p2tdCrVy8hIiJCMBgMxnWzZs0SIiIiBEEQhNDQUGHYsGEm27zwwgtC//79TdbNnDlTaNOmjcl+p06davw5NDRUWLp0qck2n376qQBASE9Pv2+N5eXlgpeXl/D9998b1wEQvvnmG0EQBCErK0sAIKxevdr4eWZmpgBAOHXqlPFYKpXK+Pn8+fMFd3d3QavVmnyHrl27CoIgCFqtVnBxcRG++uor4+e3bt0S3N3dTb7X/YSGhgqjRo0y/qxWqwUAwty5c43rDh06JAAQ1Gq1IAiC0K5dO+Gtt96qcn9z584VYmJiTNbl5uYKAITTp09XuU21v5tCza/f7BmxoAau4o1Ql7WXJa6EiOzG7dvVL19/bdo2P7/6tj/+aNr24sWq29VSt27djEMnABAdHY2zZ89Cr9cDEF8Sd69Tp06hR48eJut69Ohhsk1NKRQKPProoybr8vPzERcXh5YtW0KlUkGlUuH27dvIycm5777u3U9QUJBxX9UJCwuDl5eXyTYV7S9cuICysjJ06dLF+LlKpUKrVq1q/uX+VFNAgPjkZrt27SqtqzhufHw8Fi5ciB49emD+/Pn47bffjG1TU1OxZ88eeHp6GpfWrVsDEHt56gtvYLWgxt6NAQCXCxlGiKiOeHhI37YOePzpeIIgmISXinW14ebmVmlfY8eOxbVr17Bs2TKEhoZCqVQiOjoaOp3uvvu69+mdin0aDIYata/YpqJ9xfd52O9ZVU33q3PChAkYMGAAfvjhB+zcuROJiYlYvHgxpkyZAoPBgKFDh2LRokWVjlMRvuoDe0YsqLEXwwgROabDhw9X+rlFixaQy+VVtm/Tpg32799vsu7gwYNo2bJltdsoFIoa95qkpKQgPj4egwYNMt4kW983af7ZI488AhcXFxw9etS4TqvV4uzZs/V+7JCQEMTFxWHLli147bXX8MknnwAAOnXqhMzMTISFhaF58+Ymy58DY11iGLEgY88Ih2mIyMHk5uYiISEBp0+fxqZNm7BixQpMnTq12vavvfYadu3ahX/84x84c+YMPvvsM3z44YeYMWNGtduEhYVh3759uHz58gODRfPmzbFhwwacOnUKR44cwciRIy3+WK6XlxfGjBmDmTNnYs+ePcjMzMS4cePg5ORUqbekLk2bNg07duxAVlYWjh8/jt27dyMiIgKAeHPrjRs3MGLECBw9ehQXLlzAzp07MW7cOLOHx8zBMGJBFT0j14qvobS8VOJqiIgsJzY2FiUlJejSpQsmTZqEKVOm4OWXX662fadOnfDll1/iiy++QGRkJObNm4cFCxZg7Nix1W6zYMECXLx4EY888ggaNWp033rWrl2LmzdvomPHjhg9ejTi4+Ph7+9f269Xa0uWLEF0dDSGDBmCfv36oUePHoiIiICrq2u9HVOv12PSpEmIiIjAU089hVatWiEpKQkAEBwcjAMHDkCv12PAgAGIjIzE1KlToVKpajQ3S23JhNoOwlmQVquFSqWCRqOBt7e31OXUmiAIcH3HFTq9DllTsxDmEyZ1SURkI+7cuYOsrCyEh4fX64WqPvTu3RsdOnTAsmXL6nS/0dHRePLJJ7Fw4cI63a+UioqK0LhxYyxevBjjx4+Xupwaud/vZk2v37yB1YJkMhmej3wezjJnyGVVj3kSEdH9lZaWIiMjA5mZmYiPj5e6nIeSlpaGP/74A126dIFGo8GCBQsAAM8884zElVkWw4iFfTbsM6lLICKyaT/++CNiY2MxdOhQ/N///Z/U5Ty0Dz74AKdPn4ZCoUBUVBRSUlLg5+eHlJQUDPzzZHT3uP0Qj1pbGw7TEBHZAFsepqHaKSkpweXL1T/w0Lx5cwtWUz0O09io4rJilJSVwNfdV+pSiIjISrm5uVlN4KhvfJrGwlYcWQGPf3pg8o+TpS6FiGyQDXRmk4Opi99JhhELa+QhPm7GuUaIyBwVM2oWFxdLXAmRqYrfyT/PNmsODtNYGGdhJaLakMvl8PHxMb5fxN3dvV4nxiJ6EEEQUFxcjPz8fPj4+FQ7M25NMIxY2L2zsFb17gUiouoEBgYCuP+L2YgszcfHx/i7WVsMIxYW7BUMACjVl+JGyQ3exEpENSaTyRAUFAR/f3+UlZVJXQ4RXFxcHqpHpALDiIW5OrvC180X10uu43LhZYYRIjKbXC6vkwsAkbXgDawS4AvziIiI7mLPiASeafUMHgt+DAGeAVKXQkREJDmGEQks6LNA6hKIiIisBodpiIiISFIMIxIp0hXhSuEVqcsgIiKSHMOIBHac2wHPRE8M/nyw1KUQERFJjmFEAoGe4uQwfJqGiIiIYUQSFY/2Xiu+htLyUomrISIikhbDiAR83XyhlCsBAOrbaomrISIikhbDiARkMplxWngO1RARkaNjGJGIcRZWvr2XiIgcHMOIRNgzQkREJOIMrBLp36w/vBXeaOvfVupSiIiIJCUTBEGQuogH0Wq1UKlU0Gg08Pb2lrocIiIiqoGaXr9rNUyTlJSE8PBwuLq6IioqCikpKfdtv3HjRrRv3x7u7u4ICgrCiy++iOvXr9fm0ERERGRnzA4jmzdvxrRp0zBnzhykpaWhZ8+eGDhwIHJycqpsv3//fsTGxmL8+PHIzMzEV199hV9//RUTJkx46OJtXZGuCFk3s6Qug4iISFJmh5ElS5Zg/PjxmDBhAiIiIrBs2TKEhIRg5cqVVbY/fPgwwsLCEB8fj/DwcDz++OOYOHEijh079tDF27KLty7CM9ETEf+OgA2MlBEREdUbs8KITqdDamoqYmJiTNbHxMTg4MGDVW7TvXt3XLp0Cdu3b4cgCLh69Sr++9//YvDg6t/LUlpaCq1Wa7LYmyDPIABAqb4UN0puSFwNERGRdMwKIwUFBdDr9QgICDBZHxAQgLy8vCq36d69OzZu3Ijhw4dDoVAgMDAQPj4+WLFiRbXHSUxMhEqlMi4hISHmlGkTlM5K+Ln7AeBcI0RE5NhqdQOrTCYz+VkQhErrKpw8eRLx8fGYN28eUlNT8dNPPyErKwtxcXHV7n/27NnQaDTGJTc3tzZlWr3GXv+b+IxzjRARkQMza54RPz8/yOXySr0g+fn5lXpLKiQmJqJHjx6YOXMmAODRRx+Fh4cHevbsiYULFyIoKKjSNkqlEkql0pzSbFJj78Y4cfUEe0aIiMihmdUzolAoEBUVheTkZJP1ycnJ6N69e5XbFBcXw8nJ9DByuRwAHP7GTfaMEBER1WKYJiEhAatXr8batWtx6tQpTJ8+HTk5OcZhl9mzZyM2NtbYfujQodiyZQtWrlyJCxcu4MCBA4iPj0eXLl0QHBxcd9/EBhnDCHtGiIjIgZk9Hfzw4cNx/fp1LFiwAGq1GpGRkdi+fTtCQ0MBAGq12mTOkbFjx6KwsBAffvghXnvtNfj4+KBv375YtGhR3X0LG9U9pDsmRk1EdJNoqUshIiKSDKeDJyIionpRr9PBExEREdUVhhEJlRvKcb34Oq7evip1KURERJJhGJFQSnYK/N73Q5/P+khdChERkWQYRiTkofAAABSXFUtcCRERkXQYRiTk7uIOACgqK5K4EiIiIukwjEjIw0XsGSnSMYwQEZHjYhiRUMUwTUl5CQyCQeJqiIiIpMEwIqGKnhEAKCkrkbASIiIi6TCMSMjNxc3437xvhIiIHJXZ08FT3XGSOWFM+zFwcXKBsxP/KoiIyDHxCiixdcPWSV0CERGRpDhMQ0RERJJiGJFYmb4Mt+7cQml5qdSlEBERSYJhRGI91vZAg0UNkHwhWepSiIiIJMEwIjHjLKyc+IyIiBwUw4jEKiY+46O9RETkqBhGJMYp4YmIyNExjEiMPSNEROToGEYkxp4RIiJydAwjEjOGEfaMEBGRg2IYkVjHoI4Y3nY4OgR2kLoUIiIiScgEQRCkLuJBtFotVCoVNBoNvL29pS6HiIiIaqCm12/2jBAREZGkGEasgN6gR0lZidRlEBERSYJhRGLfnPoGzv9wRv8N/aUuhYiISBIMIxJzc3EDwKdpiIjIcTGMSKzi0d7ismKJKyEiIpIGw4jEjDOwctIzIiJyUAwjEjO+tZfDNERE5KAYRiTG6eCJiMjRMYxIrGKYpsxQhjJ9mcTVEBERWZ6z1AU4Ok+FJwa1GAQPFw+UG8rhIneRuiQiIiKLYhiRmEKuwA8v/CB1GURERJLhMA0RERFJimHEShgEAwyCQeoyiIiILI5hxAq0W9kO8gVyHMo9JHUpREREFscwYgWcZOJfA2dhJSIiR8QwYgU48RkRETkyhhErwInPiIjIkTGMWAHj+2nYM0JERA6IYcQK8M29RETkyBhGrACHaYiIyJExjFiBdgHtMOCRAQjzCZO6FCIiIouTCYIgSF3Eg2i1WqhUKmg0Gnh7e0tdDhEREdVATa/f7BkhIiIiSTGMEBERkaQYRqzAhhMb4JXohb9u/qvUpRAREVkcw4gVcJI54bbuNrSlWqlLISIisjiGESvASc+IiMiRMYxYAc4zQkREjoxhxAqwZ4SIiBwZw4gVYM8IERE5MoYRK8CeESIicmTOUhdAgEqpQveQ7vBSeEldChERkcUxjFiBRh6NcGDcAanLICIikgSHaYiIiEhSDCNEREQkKYYRK9E2qS1U76rwR8EfUpdCRERkUQwjVkJbqoW2VIvbuttSl0JERGRRDCNWomKukeKyYokrISIisiyGESvh7uIOgBOfERGR42EYsRKc+IyIiBwVw4iV4JTwRETkqGoVRpKSkhAeHg5XV1dERUUhJSXlvu1LS0sxZ84chIaGQqlU4pFHHsHatWtrVbC9Ys8IERE5KrNnYN28eTOmTZuGpKQk9OjRAx9//DEGDhyIkydPomnTplVu89xzz+Hq1atYs2YNmjdvjvz8fJSXlz908fYkwi8C3Zp0g5+7n9SlEBERWZRMEATBnA26du2KTp06YeXKlcZ1ERERGDZsGBITEyu1/+mnn/D888/jwoULaNiwYa2K1Gq1UKlU0Gg08Pb2rtU+iIiIyLJqev02a5hGp9MhNTUVMTExJutjYmJw8ODBKrfZunUrOnfujPfeew+NGzdGy5YtMWPGDJSUlFR7nNLSUmi1WpOFiIiI7JNZwzQFBQXQ6/UICAgwWR8QEIC8vLwqt7lw4QL2798PV1dXfPPNNygoKMCrr76KGzduVHvfSGJiIt5++21zSiMiIiIbVasbWGUymcnPgiBUWlfBYDBAJpNh48aN6NKlCwYNGoQlS5Zg3bp11faOzJ49GxqNxrjk5ubWpkyb8knqJ2iypAkmb58sdSlEREQWZVbPiJ+fH+RyeaVekPz8/Eq9JRWCgoLQuHFjqFQq47qIiAgIgoBLly6hRYsWlbZRKpVQKpXmlGbzSvWluFx4GflF+VKXQkREZFFm9YwoFApERUUhOTnZZH1ycjK6d+9e5TY9evTAlStXcPv23XeunDlzBk5OTmjSpEktSrZPxhlY+WgvERE5GLOHaRISErB69WqsXbsWp06dwvTp05GTk4O4uDgA4hBLbGyssf0LL7wAX19fvPjiizh58iT27duHmTNnYty4cXBzc6u7b2LjOOkZERE5KrPnGRk+fDiuX7+OBQsWQK1WIzIyEtu3b0doaCgAQK1WIycnx9je09MTycnJmDJlCjp37gxfX18899xzWLhwYd19CzvASc+IiMhRmT3PiBQcYZ6RPVl70Hd9X7Rp1AaZr2ZKXQ4REdFDq5d5Rqj+8K29RETkqMwepqH60cCtAdo2aosm3rypl4iIHAvDiJVo6dsSv7/6u9RlEBERWRyHaYiIiEhSDCNEREQkKYYRKyEIAtomtUXoslBcK7omdTlEREQWwzBiJWQyGbJuZiFHk4PbutsP3oCIiMhOMIxYEU58RkREjohhxIpwSngiInJEDCNWpKJnpLisWOJKiIiILIdhxIoYe0Y4TENERA6EYcSKGO8Z4TANERE5EM7AakXCfcKR55cHV2dXqUshIiKyGIYRK7L2mbVSl0BERGRxHKYhIiIiSTGMEBERkaQYRqzIh0c/RGRSJP6Z8k+pSyEiIrIYhhErcqPkBjKvZSJHkyN1KURERBbDMGJFOM8IERE5IoYRK+Lu4g6A84wQEZFjYRixInxRHhEROSKGESvCF+UREZEjYhixInxRHhEROSKGESvi4+qDxl6N4e/hL3UpREREFsPp4K1ItybdcCnhktRlEBERWRR7RoiIiEhSDCNEREQkKYYRK1JYWojoNdFo/1F7lOnLpC6HiIjIInjPiBVRyBU4fOkwAPGJGpVcJXFFRERE9Y89I1ZEIVfASSb+lXDiMyIichQMI1ZEJpNx4jMiInI4DCNWhlPCExGRo2EYsTJ8WR4RETkahhEr08C1AQDg5p2bEldCRERkGXyaxso09m4M9W01yg3lUpdCRERkEQwjVua757+TugQiIiKL4jANERERSYphhIiIiCTFMGJldp7ficfXPo5Xf3hV6lKIiIgsgveMWJkiXREO5B6AQTBIXQoREZFFsGfEyvi5+wEArhVfk7gSIiIiy2AYsTKNPBoBAAqKCySuhIiIyDIYRqxMRc/IrTu3UKYvk7gaIiKi+scwYmUaujU0vrn3esl1iashIiKqfwwjVsZJ5gRfN18AwLUi3jdCRET2j0/TWKEm3k2gdFaipLxE6lKIiIjqHcOIFTo+8bjUJRAREVkMh2mIiIhIUgwjREREJCmGESu08beNeHzt43hn3ztSl0JERFTveM+IFbpWfA0Hcg8gRBUidSlERET1jj0jVqiRuzgLKx/tJSIiR8AwYoUqZmHllPBEROQIGEasUMX7afiyPCIicgQMI1bo3p4RQRAkroaIiKh+MYxYoYowotPrcFt3W+JqiIiI6hefprFC7i7u8HXzhZuLG7SlWngpvaQuiYiIqN4wjFipgtd58yoRETkGDtMQERGRpBhGiIiISFIMI1Zq6aGl6L6mO9amrZW6FCIionrFMGKlcjQ5OHTpEM5cPyN1KURERPWKYcRKGSc+45TwRERk52oVRpKSkhAeHg5XV1dERUUhJSWlRtsdOHAAzs7O6NChQ20O61Aq5hrhLKxERGTvzA4jmzdvxrRp0zBnzhykpaWhZ8+eGDhwIHJycu67nUajQWxsLJ588slaF+tIKl6Wx/fTEBGRvTM7jCxZsgTjx4/HhAkTEBERgWXLliEkJAQrV66873YTJ07ECy+8gOjo6Aceo7S0FFqt1mRxNOwZISIiR2FWGNHpdEhNTUVMTIzJ+piYGBw8eLDa7T799FOcP38e8+fPr9FxEhMToVKpjEtISIg5ZdoFvrmXiIgchVlhpKCgAHq9HgEBASbrAwICkJeXV+U2Z8+exd///nds3LgRzs41m/B19uzZ0Gg0xiU3N9ecMu1CI49GcHV2hbfSG3qDXupyiIiI6k2tpoOXyWQmPwuCUGkdAOj1erzwwgt4++230bJlyxrvX6lUQqlU1qY0u+Hr5oviN4qrPK9ERET2xKww4ufnB7lcXqkXJD8/v1JvCQAUFhbi2LFjSEtLw+TJkwEABoMBgiDA2dkZO3fuRN++fR+ifPvFEEJERI7CrGEahUKBqKgoJCcnm6xPTk5G9+7dK7X39vZGRkYG0tPTjUtcXBxatWqF9PR0dO3a9eGqJyIiIptn9jBNQkICRo8ejc6dOyM6OhqrVq1CTk4O4uLiAIj3e1y+fBnr16+Hk5MTIiMjTbb39/eHq6trpfVU2eyfZ2Nv9l7M6zUPTzV/SupyiIiI6oXZYWT48OG4fv06FixYALVajcjISGzfvh2hoaEAALVa/cA5R6hmTl8/jUOXDiHrZpbUpRAREdUbmSAIgtRFPIhWq4VKpYJGo4G3t7fU5VjMxO8nYtXxVXi799uY12ue1OUQERGZpabXb76bxopxrhEiInIEDCNWzPiyPM7CSkREdoxhxIoZp4Tnm3uJiMiOMYxYMQ7TEBGRI2AYsWKN3BvB3cUdSmfHno2WiIjsW62mgyfL6BTUCUVvFEldBhERUb1iz4gV45TwRETkCBhGiIiISFIMI1ZuwtYJiF4TjRN5J6QuhYiIqF4wjFi5tLw0HL50GJe0l6QuhYiIqF4wjFi5Ru7ixGd8vJeIiOwVw4iVM058xllYiYjITjGMWDlOfEZERPaOYcTKVQzTcEp4IiKyVwwjVs7YM1LCnhEiIrJPDCNWrpGHOCW8k4x/VUREZJ84HbyV+0vrv3BKeCIismv857aV45TwRERk7xhGiIiISFIMIzbgw6Mfov1H7bHiyAqpSyEiIqpzDCM24GbJTfx29TccvXJU6lKIiIjqHMOIDegY1BEAkKZOk7gSIiKiuscwYgM6Boph5I+CP1BSViJxNURERHWLYcQGBHsFo5F7I+gFPTLyM6Quh4iIqE4xjNgAmUzGoRoiIrJbDCM2omKoJj0vXdpCiIiI6hjDiI2ICopCa7/W8Pfwl7oUIiKiOiUTBEGQuogH0Wq1UKlU0Gg08Pb2lrocIiIiqoGaXr/ZM0JERESSYhixMQbBgCIdX5xHRET2g2HEhiw+uBjeid6Y/8t8qUshIiKqMwwjNsTH1QdFZUVIy+PjvUREZD8YRmzIvXON2MB9x0RERDXCMGJD2jZqC2cnZ9y8cxO52lypyyEiIqoTDCM2ROmsRJtGbQBwJlYiIrIfDCM2pmImVt43QkRE9oJhxMYwjBARkb1hGLEx0SHRGNxiMPqE9ZG6FCIiojrB6eCJiIioXnA6eCIiIrIJDCM2Sl2oxrkb56Qug4iI6KExjNigfx/9N4KXBGPWz7OkLoWIiOihMYzYoHYB7QAARy4dkbgSIiKih8cwYoOigqIgl8lxufAyLmsvS10OERHRQ2EYsUEeCg9E+kcCAI5cZu8IERHZNoYRG9W1cVcAwOFLhyWuhIiI6OEwjNiork3EMMKeESIisnUMIzaqomfk2JVjKDeUS1wNERFR7TlLXQDVTkSjCEzrOg2dgztDb9DD2Yl/lUREZJt4BbNRTjInLH1qqdRlEBERPTQO0xAREZGkGEZsmE6vw/6c/VibtlbqUoiIiGqNwzQ27GbJTfT8tCdkkOH/2vwfvJV8ozEREdke9ozYsADPAISqQiFAwK+Xf5W6HCIiolphGLFxnG+EiIhsHcOIjevWuBsAhhEiIrJdDCM2ztgzcukIBEGQuBoiIiLzMYzYuI6BHeHs5IyrRVeRo8mRuhwiIiKzMYzYODcXN7QPaA+AQzVERGSb+GivHXi///twdXZFx6COUpdCRERkNoYRO9AnvI/UJRAREdUah2mIiIhIUgwjdmJV6iq8tuM1qAvVUpdCRERkllqFkaSkJISHh8PV1RVRUVFISUmptu2WLVvQv39/NGrUCN7e3oiOjsaOHTtqXTBVbfGhxVhyeAlOFZySuhQiIiKzmB1GNm/ejGnTpmHOnDlIS0tDz549MXDgQOTkVP1Y6b59+9C/f39s374dqamp6NOnD4YOHYq0tLSHLp7uCvcJBwBk3cySuBIiIiLzyAQzZ8rq2rUrOnXqhJUrVxrXRUREYNiwYUhMTKzRPtq2bYvhw4dj3rx5VX5eWlqK0tJS489arRYhISHQaDTw9ubL4KoSty0OH6d+jDd7vol/9P2H1OUQERFBq9VCpVI98PptVs+ITqdDamoqYmJiTNbHxMTg4MGDNdqHwWBAYWEhGjZsWG2bxMREqFQq4xISEmJOmQ7J2DNyiz0jRERkW8wKIwUFBdDr9QgICDBZHxAQgLy8vBrtY/HixSgqKsJzzz1XbZvZs2dDo9EYl9zcXHPKdEjhDcQwcvHWRWkLISIiMlOt5hmRyWQmPwuCUGldVTZt2oS33noL3333Hfz9/attp1QqoVQqa1OawwrzCQPAMEJERLbHrJ4RPz8/yOXySr0g+fn5lXpL/mzz5s0YP348vvzyS/Tr18/8Sum+KoZprhReQWl56QNaExERWQ+zwohCoUBUVBSSk5NN1icnJ6N79+7Vbrdp0yaMHTsWn3/+OQYPHly7Sum+/Nz9cHDcQVx57QoUcoXU5RAREdWY2cM0CQkJGD16NDp37ozo6GisWrUKOTk5iIuLAyDe73H58mWsX78egBhEYmNj8a9//QvdunUz9qq4ublBpVLV4VdxbDKZDNEh0VKXQUREZDazw8jw4cNx/fp1LFiwAGq1GpGRkdi+fTtCQ0MBAGq12mTOkY8//hjl5eWYNGkSJk2aZFw/ZswYrFu37uG/AREREdk0s+cZkUJNn1N2dPuy92Hr6a3oGNgRIx8dKXU5RETk4OplnhGybseuHMPiQ4vx/ZnvpS6FiIioxhhG7EjF472c+IyIiGwJw4gdqXi8l3ONEBGRLWEYsSMVs7DmF+WjSFckcTVEREQ1wzBiR3xcfaBSio9LZ2uyJa6GiIioZhhG7ExF70jWTd43QkREtoFhxM7wHTVERGRrOM+Incm+lQ2FXIEAzwA4yZg1iYhIOjW9ftfqrb1kvUJ9QqUugYiIyCz8pzMRERFJimHEztwsuYmZO2ci9ptYqUshIiKqEYYRO+Mid8EHhz7Aht82QHNHI3U5RERED8QwYmc8FZ7wc/cDwCdqiIjINjCM2CE+3ktERLaEYcQOVbyjhi/MIyIiW8AwYofYM0JERLaEYcQOMYwQEZEtYRixQxXDNJe0lySuhIiI6ME4A6sd6hXWC5cTLiPQM1DqUoiIiB6IYcQOubu4w93FXeoyiIiIaoTDNERERCQphhE7lfRrEv721d+wO2u31KUQERHdF8OIndqfsx//PflfHFcfl7oUIiKi+2IYsVMVT9Scv3Fe4kqIiIjuj2HETnUM6ggA+O70d9DpdRJXQ0REVD2GETv1dKunEegZCPVtNb4++bXU5RAREVWLYcROKeQKvNL5FQDA8qPLJa6GiIioegwjdmxi1EQo5AocvnQYRy8flbocIiKiKnHSMzsW4BmAke1GQlOq4SRoRERktRhG7Nzqp1fDScYOMCIisl68Stk5BhEiIrJ2vFI5iPM3zuONXW+gtLxU6lKIiIhMMIw4AINgQN/1fZG4PxFfZn4pWR2/Xf0Nk36YhLzbeZLVQERE1odhxAE4yZwwMWoiAOBfR/4FQRAsXoPeoMfgzwcj6VgSfjjzg8WPT0RE1othxEG8HPUylHIlUtWp2JW1y+LH3/T7JlzSXgIA/DXirxY/PhERWS+GEQfh5+5n7B1J2JEAvUH/wG2Ky4rr5Ng6vQ7z9swDACQ+mYgGbg3qZL9ERGQfGEYcyLxe89DAtQEy8jOw+vjqB7bv+HFHtFjRAmO+HfNQQytrjq9B1q0sBHgEYEqXKQCAAzkHkJKdUqmtTq/DD2d+wK07t2p9PCIisi0MIw7E190Xb/d+GwDw5p43K13wF+5biN/zfwcA3Cy5ibPXz+LcjXNYf2I9hmwagjXH15h9zOKyYvxj3z8AAHOfmAsPhQfWn1iPxz99HBO3TUS5odyk/YojKzBk0xBEr4lGflF+Lb4lERHZGoYRBxPXOQ5RQVGY/NhkKOQK4/rNv2/G3D1z0W11N+QX5aOBWwPcmHUDP478EbHtYwEAL297GV9lfmXW8VYcWQH1bTXCfMLwUtRLAMSX+DV0a4hTBaewLn2dSfvJXSYDAP4o+AP91vfD9eLrtfqeZfoyvHfgPby7/10YBEOt9lHh9/zfkZiSCM0dzUPth4iIqsYZWB2Mi9wFRyYcgdxJblyXdTMLL297GQAwvdt0+Hv4AwB8XH3wVPOnMOCRAXCVu2LV8VUYuWUkvJReeKr5UzU6XlRwFDoEdsD0btON4cfH1Qdv9nwTCTsTMP+X+egT1gdhPmGQO8mhdFbizOQzeGLdE8jIz0DMf2KwK3YXfFx9avwdr96+iuf++xz2Ze8DAKgL1Vj21DLIZLIa76PC5xmfY8LWCSgpL8Hp66exbtg6s/dBRET3x54RB3RvENHpdRjx9QhoS7XoEdID83vPr9ReJpMhaXASno98HmWGskq9GffTr1k/pL6cilGPjjJZ/+pjryLMJwxXCq+g9b9bY9L2ScZHjlv4tsCu2F1o5N4Ix9XHMXDjQBSWFtboeIdyD6HTqk7Yl70PHi4eAMS3Fi8+tLjK9tpSLU5eO1npht5yQzle2/EaRm4ZiZLyEgDAht824NS1UzX+7vbsdMFpvJ78Os7dOCd1KURkBxhGHNj+nP1QLlTiyOUjaODaAJ8/+zmcnaruLJM7ybF+2HosiVmCDX/ZYFx/8tpJk/s+7pTfwU/nfsLk7ZNxpfAKAHGekz9PS690VuKdvu8AEC/8B3IPQFuqNX7eplEb/Bz7Mxq6NcThS4cx+PPBuFN+577f5+z1s+i1rheuFF5Bm0ZtkPpyKpYNWIYQ7xAMaTnEpK1Or8PSQ0sRuiwUbZPaIuCDAKTnpQMArhVdw4D/DMCSw0sAAG88/gZe7PAiFvVbhFCf0PvWUB/K9GU4mHsQe7L2YH/Ofhy5dATH1cclu6fmZslNDPjPALx/8H10Xd0Vey/ulaQOIrIfHKZxYPfOxrrm6TVoqmp63/YuchdMj55u/Pl68XW0TWoLDxcPdGncBZ4KT+zK2mV8JLh9QHvjfSJVeT7yeey9uBe3y27jw4EfQuWqMvn80YBHsXPUTjy5/kn0DusNpVx53/pa+LbAy1EvI78oH2ufWQtPhSda+bXCix1fhLfSGwAgCAK++eMbvJ78Os7fPA8AcHZyRlFZEVr6thS/V8l1/Hr5V3i4eOCzYZ/h2TbP3ve49SVXk4v3DryHzZmbca34WqXPmzVohoxXMiz6RmZBEDB+63hka7Ihgww3Sm6g/4b+WDV0FcZ2GGuxOojIvsgEKabjNJNWq4VKpYJGo4G3t7fU5diNmyU3Mfqb0ege0h1v9HzD7O2PXDqCmP/EmPRoAEBjr8YY0nIIxnUchy6Nuzx0nflF+cb7WP5MEASUGcqM96OUG8ohl8mrvT9kxs4ZxiGbQM9ALOyzEKMeHYUz18+gXUA7Y7sd53agiXcTtPVvW+Uxa3P/iblyNbkIXRYKAQJ83XwR4BmAMn0ZygxlAICfR/+MRxo+Uu913OvirYvovKoztKVa7B6zGx8e/RCbMzcDAHbH7kaf8D4WrYeIrFtNr98MI/RQ9AY9ThWcwqHcQ9CUatCvWT+0D2hfbxfr4rJi7M7ajSEth0AQBMz6eRbS89Lx7fPfPrCH4EDOAfRd3xdOMifMiJ6B13u8Di+ll1nH33p6K+bsnoNVQ1YhOiS6yjZ3yu/A1dnVrP1WOHP9jLGHBgDe2fcOOgV1Qr9m/eAid6l2O22p1tj7U0FzR1Opt6kuXNJewpFLR/Bsm2dhEAyYv2c+crQ5WPfMutr/vX//PTBwIODMzloie8IwQnanuKwYT65/EkcuHcF/n/svUq+k4p/7/wkA+Gb4NxjWeth9ty/Tl2HbmW14rPFjaOLdpFY1jP9uPNamr0Xf8L7YFStOq19YWmgSahosagAXJxc0a9AMAZ4B0Ol1uFN+B3fK76Bn0554r/97Ve576aGlmJk8E+v/sh4vtHuhxjV9c+obTPh+AraN2IbokGgczD2IxP2JOHblGLKmZhmDUU17dARBwI2SG2jo1rDG4cIgGIz3Bf1R8Ade+v4lTO06FcNaD0O5oRwHcw9id9ZuaO5osGLQCtONP/kEePllYMgQ4IsvAA+Pygc4cQI4dw4IDweaNQN8fGpUFxFJq6bXb/4zhGyGm7MbOgV2wuFLh/HcV89BL4hPwCx/avkDgwgg3vPyl4i/PFQN83rNw4bfNmB31m6sSl2FbWe2IUeTg9SXUyF3kuPWnVvGyeSqus/Dz92vyv2uSl2FhJ0JAIALNy/UuB5BELA6bTVulNzAwI0D0S6gHfbn7AcAyCDD3ot7MaD5AAiCgKc2PoXeob0xtdvUanuRMq5mYPQ3o3Hi6gl4uHigecPmaOHbAqlXUrFkwJJqz/O9NyivOLIC+3P2Y3/OfgR4BODWnVso1ZcCABRyBRb1X2R6fF9fwNUV2LYN6NVL/DMw0PQAv/wCTJt29+eGDYEWLYBZs4C/PNzfKRFJjz0jZFPKDeX46+a/4vsz3wMAlsQsMbmp1hKmbJ+CD3/90PizXCbH/nH70a1JNwDi8EjWrSxcuHkB14quwdXZ1bg09m6MzsGdAQDHrhzDyl9X4rHGj+HVH16FAAGzesxC4pOJZg13FOmK8NTGp4whxMXJBWPaj8HMHjONQz4/nv0Rgz4fBADw9/DH1K5T8UrnVyq9J+jpTU8bz+2fOTs549yUcw98okhdqMbKYyvx0bGPjIEsyDMITzZ7En3D+mJ45HC4u7hDXajGvux9GB45HDh0CHj6aaCgAAgNBTZtAjw9gXb/u49HpwNiYoA//gCuXr17MCcn4LPPgFGjqqiEiKTGYRqyW0W6Isz/ZT7aB7TH6PajLX78vNt5aLmiJQp1hXim1TNY1G8RWvm1MmsfBsGATh93womrJ4zrJj82GcsHLq/VfRfaUi0SdiTA180X8V3j0di7caXjbcrYhLl75iLrVhYAwMPFAy9HvYzp3aYjRBUCALisvYxZP8/Cu/3eRXFZMc5cP4Oz18/i/M3zeDL8SbN6lu6U38G+7H1oqmqKVr6tTL5Xka4IPdb2wImrJ5D4ZCJm9ZgF2fnzwKBBwNmzYqMWLYDffwcUCtMd374NXLgALF8OrFkDKJXA+fNAY9PvTETSYxghqkfnbpxDka4I7QPb13of+3P2Y/zW8Thz/Qxe7PAiVj+9utJ8LHWtTF+GLzO/xHsH38NvV38DADwR+gR+GfOLRZ4QqmAQDJj982y8d1C8f+b7Ed+Lc8EUFADPPAMcPAhERAA//CDeJ1LlTgzi0E3fvsCwYRarnYhqjmGEyAbcKb+DU9dOoUNgB4uGAUEQsPP8Tiw6sAh7Lu7Brthd6Bve12LHr7Ds8DL8nv87Phn6yd3vX1YGpKUBnTqZ/3TN778DOTnArVt3Fy8vceinXbvqgw0R1QuGESKqEXWhGgGeAfXeK1OdOpu35eJFoGdP4NKlqj+fOhVYtuzhj0NENcanaYioRoK8giQ9fp31CP34o/hYcKdO4qO/Pj6ASgVoNEB2NtD2ngnsyssBuRywYG8UEVWPPSNE5Fg2bQLmzQP+9S/xhlkiqjc1vX7zRXlE5FiOHxcnUPvHPwDr/7cYkUNgGCEix/Laa+Ika4cPA7t2SV0NEYFhhIgcTWCgOP08IPaOEJHkGEaIyPG8/ro4mdq+feJCRJJiGCEix9O4MTBunPjf7B0hkhzDCBE5plmzxEnVfv4Z+O0387YVBPGR4dLS+qmNyMFwnhEickxhYcB77wEdO959Id+uXYCfH9CoEXDjhjiB2qVLQG6u2Ob//k9sd+UK0KSJOE9JkyZAs2biEhgoftanDzBmjNjWYABefBHo3RsYMkTctyCIc58cOCA+Xtzgfy8s3L4dWLRI/NzDQ3yjccXi5ye+LLB58wd/t9JScR6VihlsdTqxVheXujyDRHWGYYSIHNf0e974XFYG9O9f/eO+I0bcDSOBgeLFXq8Xg0puLrB3r2n7ijDy66/A+vXiIpMBnTuLgeXyZfHz774T31gMiG8kvt89LJs33w0j330HvPQS4O5+d1Eqxf3m5AC//AI88cTdtqNGAW3aAO3bi8FGrxcnfysvBxISxBcTAmIIu3ULaNhQbFOxGAziRHIeHpXrEgRxm9xc8Tv6+4sBytzp/Cvo9WIdBoO474o/y8vFv6fgYPH72jqdTvz92LcPKC4GoqOBXr2qPsd2jmGEiAgACgvF3o+8PODaNbG3okkTICRE/LN797tt5XJAq737BuELF8Q3B+fliRfKbt3utg0OBt56C9i6VZzj5NdfxfXOzmKvzL2zwPbpA3z5pbju9m3g+vW7y7VrQOvWd9vm5IjrqnPhwt0w8vvv4oUvPV1c/mz48Lth5MsvgVdeqX6/338v9vAAwNq1wAcfiLUUFVVuu2OH2JsDAF99JU7Hr1AAbm7i7LgqlRhwPD2BiRPFXiMAmD8feOed6mvYvVs8VwDw3/+Kj2tHRIhhy8tLrCcnR+x9+umnuwFu716xfn9/sfeopMR0efttsccMEAPcxx+LvwcNGojhTKcDsrLE5auvxHceAcBHH4l/xxXfqWHDuz1aDRsCEyaIv0MAkJoq/i4cOCC+ELKkxPS7/fbb3Z66FSvENteviy+RLCkR9+nvL56rDz4Qvy8AHDkCqNXiY+slJeLv8+3b4p/FxcDcuXfD4UcfiQFIpxOXsjLxu3fpUv05r2e1CiNJSUl4//33oVar0bZtWyxbtgw9e/astv3evXuRkJCAzMxMBAcH4/XXX0dcXFytiyYiqnMNGwInToj/LQgPniq+ojfC3980fPxZSIh4cZ0//24PSpMm4v/4//yv+7CwuxfDBxk9WrwgFxeLF5+KPwMDgUceEeuqMH++2FNz4oR4sSsqEi9Mzs5isKq4qALiBUypNL0fxslJPB96PRAQcHf9nTvAqVN3f/bzE8/djRvinw0b3v0sN1e8sFane3fxDcyAOOT152PLZGK9Li6mvVeZmXfDx44dlfebnW0aRhYvrr6GV165e/5PnRJfMVCdc+funrdr18ReratXq27bv//dMLJvH7Bgwd3P/PzE3hAvLyAjw/S1BSkpYuipzooVd/97+XLg88+rbztzphj6AODoUXEm4nvdL4BagNnTwW/evBmjR49GUlISevTogY8//hirV6/GyZMn0bRp00rts7KyEBkZiZdeegkTJ07EgQMH8Oqrr2LTpk149tlna3RMTgdPRGRBgiD+i9nZ+W4YEAQxqLi63r33JCcHOHtWDFwhIWKPB3B3mEWlEntCALHnKCND3G9RkXgD8K1b4p+FhWLPyGOPiW11OjEkyeUPrvXWLTGQnDwp/nnnDtC0qbiEhorDUhXXjZ9/Bn74QexpcHUV6713eeGFu6EhM1O8aN+4Ady8KS5OTuKbn8PDgccfv9uTc/26GLa0WrGe69fF7Sp6tWbMuNvz9NNPYsCIihLvI4qIqD74bt0q9nBV3DPk6iruLz9fPM4bb9xtO38+sHOnGEjd3cVw4+kp/unuDrz//t3hn507xfOlUIh/lwqFGGyruIY/rHp7a2/Xrl3RqVMnrFy50rguIiICw4YNQ2JiYqX2s2bNwtatW3HqnvQcFxeHEydO4NChQ1Ueo7S0FKX3pHKNRoOmTZsiNzeXYYSIiMhGaLVahISE4NatW1CpVNU3FMxQWloqyOVyYcuWLSbr4+PjhSeeeKLKbXr27CnEx8ebrNuyZYvg7Ows6HS6KreZP3++AIALFy5cuHDhYgdLbm7uffOFWfeMFBQUQK/XI+DeMUMAAQEByMvLq3KbvLy8KtuXl5ejoKAAQUGVX18+e/ZsJCQkGH82GAy4ceMGfH196+5147ib2NjjYhk835bF821ZPN+WxfNtebU554IgoLCwEMHBwfdtV6sbWP8cCARBuG9IqKp9VesrKJVKKJVKk3U+Pj61qLRmvL29+ctsQTzflsXzbVk835bF82155p7z+w7P/I9ZM7D6+flBLpdX6gXJz8+v1PtRITAwsMr2zs7O8PX1NefwREREZIfMCiMKhQJRUVFITk42WZ+cnIzu9z6Df4/o6OhK7Xfu3InOnTvDhbMBEhEROTyz302TkJCA1atXY+3atTh16hSmT5+OnJwc47whs2fPRmxsrLF9XFwcsrOzkZCQgFOnTmHt2rVYs2YNZsyYUXffopaUSiXmz59faUiI6gfPt2XxfFsWz7dl8XxbXn2ec7Mf7QXESc/ee+89qNVqREZGYunSpXjifzP9jR07FhcvXsQvv/xibL93715Mnz7dOOnZrFmzOOkZERERAahlGCEiIiKqK2YP0xARERHVJYYRIiIikhTDCBEREUmKYYSIiIgk5dBhJCkpCeHh4XB1dUVUVBRSUlKkLskuJCYm4rHHHoOXlxf8/f0xbNgwnD592qSNIAh46623EBwcDDc3N/Tu3RuZmZkSVWw/EhMTIZPJMG3aNOM6nuu6d/nyZYwaNQq+vr5wd3dHhw4dkJqaavyc57zulJeX480330R4eDjc3NzQrFkzLFiwAAaDwdiG57v29u3bh6FDhyI4OBgymQzffvutyec1ObelpaWYMmUK/Pz84OHhgaeffhqXLl0yr5AHvBvPbn3xxReCi4uL8MknnwgnT54Upk6dKnh4eAjZ2dlSl2bzBgwYIHz66afC77//LqSnpwuDBw8WmjZtKty+fdvY5t133xW8vLyEr7/+WsjIyBCGDx8uBAUFCVqtVsLKbdvRo0eFsLAw4dFHHxWmTp1qXM9zXbdu3LghhIaGCmPHjhWOHDkiZGVlCT///LNw7tw5Yxue87qzcOFCwdfXV9i2bZuQlZUlfPXVV4Knp6ewbNkyYxue79rbvn27MGfOHOHrr78WAAjffPONyec1ObdxcXFC48aNheTkZOH48eNCnz59hPbt2wvl5eU1rsNhw0iXLl2EuLg4k3WtW7cW/v73v0tUkf3Kz88XAAh79+4VBEEQDAaDEBgYKLz77rvGNnfu3BFUKpXw0UcfSVWmTSssLBRatGghJCcnC7169TKGEZ7rujdr1izh8ccfr/ZznvO6NXjwYGHcuHEm6/76178Ko0aNEgSB57su/TmM1OTc3rp1S3BxcRG++OILY5vLly8LTk5Owk8//VTjYzvkMI1Op0NqaipiYmJM1sfExODgwYMSVWW/NBoNAKBhw4YAgKysLOTl5Zmcf6VSiV69evH819KkSZMwePBg9OvXz2Q9z3Xd27p1Kzp37oy//e1v8Pf3R8eOHfHJJ58YP+c5r1uPP/44du3ahTNnzgAATpw4gf3792PQoEEAeL7rU03ObWpqKsrKykzaBAcHIzIy0qzzX6u39tq6goIC6PX6Si/3CwgIqPRSP3o4giAgISEBjz/+OCIjIwHAeI6rOv/Z2dkWr9HWffHFFzh+/Dh+/fXXSp/xXNe9CxcuYOXKlUhISMAbb7yBo0ePIj4+HkqlErGxsTzndWzWrFnQaDRo3bo15HI59Ho93nnnHYwYMQIAf8frU03ObV5eHhQKBRo0aFCpjTnXU4cMIxVkMpnJz4IgVFpHD2fy5Mn47bffsH///kqf8fw/vNzcXEydOhU7d+6Eq6trte14ruuOwWBA586d8c9//hMA0LFjR2RmZmLlypUm7+XiOa8bmzdvxn/+8x98/vnnaNu2LdLT0zFt2jQEBwdjzJgxxnY83/WnNufW3PPvkMM0fn5+kMvllVJbfn5+pQRItTdlyhRs3boVe/bsQZMmTYzrAwMDAYDnvw6kpqYiPz8fUVFRcHZ2hrOzM/bu3Yvly5fD2dnZeD55rutOUFAQ2rRpY7IuIiICOTk5APj7XddmzpyJv//973j++efRrl07jB49GtOnT0diYiIAnu/6VJNzGxgYCJ1Oh5s3b1bbpiYcMowoFApERUUhOTnZZH1ycjK6d+8uUVX2QxAETJ48GVu2bMHu3bsRHh5u8nl4eDgCAwNNzr9Op8PevXt5/s305JNPIiMjA+np6calc+fOGDlyJNLT09GsWTOe6zrWo0ePSo+qnzlzBqGhoQD4+13XiouL4eRkeqmSy+XGR3t5vutPTc5tVFQUXFxcTNqo1Wr8/vvv5p3/Wt92a+MqHu1ds2aNcPLkSWHatGmCh4eHcPHiRalLs3mvvPKKoFKphF9++UVQq9XGpbi42Njm3XffFVQqlbBlyxYhIyNDGDFiBB/FqyP3Pk0jCDzXde3o0aOCs7Oz8M477whnz54VNm7cKLi7uwv/+c9/jG14zuvOmDFjhMaNGxsf7d2yZYvg5+cnvP7668Y2PN+1V1hYKKSlpQlpaWkCAGHJkiVCWlqacZqLmpzbuLg4oUmTJsLPP/8sHD9+XOjbty8f7TXHv//9byE0NFRQKBRCp06djI+e0sMBUOXy6aefGtsYDAZh/vz5QmBgoKBUKoUnnnhCyMjIkK5oO/LnMMJzXfe+//57ITIyUlAqlULr1q2FVatWmXzOc153tFqtMHXqVKFp06aCq6ur0KxZM2HOnDlCaWmpsQ3Pd+3t2bOnyv9fjxkzRhCEmp3bkpISYfLkyULDhg0FNzc3YciQIUJOTo5ZdcgEQRAeqh+HiIiI6CE45D0jREREZD0YRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjBAREZGkGEaIiIhIUgwjREREJKn/ByPzs8vnoLwqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretraining_mse_no_mean = [x[1] for x in output[\"pre_evals\"]]\n",
    "posttraining_mse_no_mean = [x[1] for x in output[\"post_evals\"]]\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(range(n_samples_pretraining), pretraining_mse_no_mean, \"g--\", label=\"pretraining_mse\")\n",
    "plt.plot(range(n_samples_posttraining-1, n_samples_posttraining+n_samples_posttraining-1), posttraining_mse_no_mean, \"r--\", label=\"projtraining_mse\")\n",
    "plt.gca().set_ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804b9fe-81fa-4046-9438-99f114b53eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
