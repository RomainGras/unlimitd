{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e490cb-34a9-4163-9053-f24161a24b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_UNIFIED_MEMORY=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_UNIFIED_MEMORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f9178a-a7e8-45a5-a963-d606cbdfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unlimtd_f\n",
    "import time\n",
    "from jax import random, jit, pmap, value_and_grad, lax\n",
    "import dataset_multi_infinite\n",
    "import dataset_lines_infinite\n",
    "import test\n",
    "import plots\n",
    "import ntk\n",
    "import nll\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "import pickle\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a7d732-78fc-4f92-8ca7-d8f7cb2a07fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655235988902897757\n"
     ]
    }
   ],
   "source": [
    "seed = 1655235988902897757\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815a68c8-dec0-4665-be22-23a13d4f2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise):\n",
    "    \"\"\"\n",
    "    Computes the NLL of this data (one task only) wrt the kernel\n",
    "    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\n",
    "    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\n",
    "    \"\"\"\n",
    "    cov_a_a = kernel_self(x_a)\n",
    "    K = cov_a_a.shape[0]\n",
    "    cov_a_a = cov_a_a + maddox_noise ** 2 * np.eye(K)\n",
    "    \n",
    "    # prior mean is 0\n",
    "    y_a = np.reshape(y_a, (-1))\n",
    "\n",
    "    L = jax.scipy.linalg.cho_factor(cov_a_a)\n",
    "    ypred_a = np.reshape(part_apply_fn(inputs = x_a), (-1,))\n",
    "    alpha = jax.scipy.linalg.cho_solve(L, y_a - ypred_a)\n",
    "    return 0.5 * (y_a - ypred_a).T @ alpha + np.sum(np.log(np.diag(L[0]))) + 0.5 * K * np.log(2 * np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4dea8a-7338-4799-8dc1-21fbf6e80e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian, mean):\n",
    "    \"\"\"\n",
    "    NLL for a batch of tasks, when there is only one kernel (singGP)\n",
    "    x_a is (n_tasks, batch_size, input_dims) (input_dims are (128, 128, 1) for vision, (1,) for toy problems)\n",
    "    y_a is (n_tasks, batch_size, reg_dim)\n",
    "    \"\"\"\n",
    "    def f(carry, task_data):\n",
    "        x_a, y_a = task_data\n",
    "        y_a = y_a - utils.falseaffine_correction0(jacobian, mean, x_a)\n",
    "        loss_here = nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise)\n",
    "        return None, loss_here\n",
    "\n",
    "    _, losses = lax.scan(f, None, (x_a, y_a))\n",
    "\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4f811d-a0ac-49ea-ad24-39f3f56142b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def nll_batch_average_identity_cov(current_params, current_mean, apply_fn, current_batch_stats, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n",
    "    part_apply_fn = (partial(apply_fn, current_params, current_batch_stats))\n",
    "    \n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian, current_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f2b355-f145-46fb-9e58-299ac9a43530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_identity_cov(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_m) = value_and_grad(nll_batch_average_identity_cov, argnums = (0, 1) )(current_state.params,\n",
    "                                                              current_state.mean,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f8da8f-2182-47e9-9af3-9aa84fcd0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_stats_updater(current_state, x_a):\n",
    "    # shape of x_a is (n_tasks, batch_size, inputs_dims...)\n",
    "    \n",
    "    batch_stats = current_state.batch_stats\n",
    "    \n",
    "    def f(old_batch_stats, _x_a):\n",
    "        # shape of _x_a is (batch_size, input_dims)\n",
    "        _, mutated_vars = current_state.apply_fn_raw({\"params\":current_state.params,\n",
    "                                                      \"batch_stats\": old_batch_stats},\n",
    "                                                     _x_a,\n",
    "                                                     mutable=[\"batch_stats\"])\n",
    "        \n",
    "        new_batch_stats = mutated_vars[\"batch_stats\"]\n",
    "        return new_batch_stats, None\n",
    "\n",
    "    batch_stats = dict(batch_stats)\n",
    "    print(type(batch_stats))\n",
    "    batch_stats, _ = lax.scan(f, batch_stats, x_a)\n",
    "    return batch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a6bfef-9868-4c45-aece-3f4514c99f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def grad_applier_identity_cov(current_state, gradients_p, gradients_m, new_batch_stats):\n",
    "    return current_state.apply_gradients(grads_params=gradients_p, grads_mean=gradients_m, new_batch_stats=new_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b951430-4a3a-477f-947c-c0b71b69d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_identity_cov(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_m) = pmap(pmapable_loss_identity_cov,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_m = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_m)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_identity_cov(current_state, current_gradients_p, current_gradients_m, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad8598-b20a-4235-9a59-2b29010815f5",
   "metadata": {},
   "source": [
    "## Unlimitd f training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5acfed-43d4-48d9-ab79-44472a07f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntk import get_kernel_and_jac_lowdim_cov\n",
    "\n",
    "def nll_batch_average_lowdim_cov_singGP(current_params, current_mean, current_scale, apply_fn, current_batch_stats, proj, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_scale, current_batch_stats, proj)\n",
    "\n",
    "    part_apply_fn = (partial(apply_fn, current_params, current_batch_stats))\n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian, current_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72165a52-b07d-4f90-b308-0773e747a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_lowdim_cov_singGP(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_m, gradients_s) = value_and_grad(nll_batch_average_lowdim_cov_singGP, argnums = (0, 1, 2) )(current_state.params,\n",
    "                                                              current_state.mean,\n",
    "                                                              current_state.scale,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              current_state.proj,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_m, gradients_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b845a68b-ee0f-473f-bf0f-1b105fa0bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import grad_applier_lowdim_cov_singGP\n",
    "\n",
    "def step_lowdim_cov_singGP(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_m, unaveraged_gradients_s) = pmap(pmapable_loss_lowdim_cov_singGP,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_m = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_m)\n",
    "    current_gradients_s = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_s)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_lowdim_cov_singGP(current_state, current_gradients_p, current_gradients_m, current_gradients_s, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154087b2-a619-4bc9-8b46-8ebf39ae7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(key, step, n_epochs, state, n_tasks, K, data_noise, maddox_noise, get_train_batch_fn, eval_during_training_fn):\n",
    "    \"\"\"\n",
    "    Available step functions:\n",
    "    * step_identity_cov\n",
    "    * step_lowdim_cov_singGP\n",
    "    * step_lowdim_cov_mixture\n",
    "\n",
    "    Available get_train_batch_fn functions:\n",
    "    * dataset_sines_infinite.get_training_batch\n",
    "    * dataset_sines_finite.get_training_batch\n",
    "    * dataset_lines_infinite.get_training_batch\n",
    "    * dataset_multi_infinite.get_training_batch\n",
    "    * dataset_shapenet1d.get_training_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "    print(\"Starting training with:\")\n",
    "    print(f\"-n_epochs={n_epochs}\")\n",
    "    print(f\"-n_tasks={n_tasks}\")\n",
    "    print(f\"-K={K}\")\n",
    "    print(f\"-data_noise={data_noise}\")\n",
    "    print(f\"-maddox_noise={maddox_noise}\")\n",
    "\n",
    "    losses = []\n",
    "    evals = []\n",
    "    t = time.time_ns()\n",
    "\n",
    "    for epoch_index in range(n_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        state, current_loss = step(subkey, state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn)\n",
    "\n",
    "        if(np.isnan(current_loss)):\n",
    "            print(\"Nan, aborting\")\n",
    "            break\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "\n",
    "        if epoch_index % 10 == 0:\n",
    "            print(f\"{epoch_index}  | {current_loss:.4f} ({(time.time_ns() - t)/ 10**9:.4f} s)\")\n",
    "        t = time.time_ns()\n",
    "\n",
    "        if epoch_index % 500 == 0:\n",
    "            key, subkey = random.split(key)\n",
    "            current_eval = eval_during_training_fn(subkey, state)\n",
    "            evals.append( current_eval )\n",
    "            print(f\"Eval: {current_eval}\")\n",
    "    print(\"Completed training\")\n",
    "\n",
    "    return state, losses, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa26b2e5-e61e-45e2-bc15-75c589e8a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer\n",
    "import ntk\n",
    "import test\n",
    "import train_states\n",
    "import models\n",
    "import utils\n",
    "import fim\n",
    "\n",
    "import dataset_sines_infinite\n",
    "import dataset_sines_finite\n",
    "import dataset_multi_infinite\n",
    "\n",
    "from jax import random\n",
    "from jax import numpy as np\n",
    "from flax.core import FrozenDict\n",
    "import optax\n",
    "\n",
    "from ntk import get_kernel_and_jac_identity_cov\n",
    "from ntk import get_kernel_and_jac_lowdim_cov\n",
    "\n",
    "def unlimtd_f_multi_modal_singGP(seed, pre_n_epochs, pre_n_tasks, pre_K, post_n_epochs, post_n_tasks, post_K, data_noise, maddox_noise, meta_lr, subspace_dimension):\n",
    "    key = random.PRNGKey(seed)\n",
    "    key_init, key = random.split(key)\n",
    "\n",
    "    print(\"===============\")\n",
    "    print(\"This is UNLIMTD-F\")\n",
    "    print(\"For the multi-modal dataset: sine + line (both infinite)\")\n",
    "    print(\"This variant of UNLIMTD-F approaches the distribution with a single GP\")\n",
    "    print(\"===============\")\n",
    "    print(\"Creating model\")\n",
    "    model = models.small_network(40, \"relu\", 1)\n",
    "    batch = random.uniform(key_init, shape=(5,1), minval=-5, maxval=5)\n",
    "    init_vars = model.init(key_init, batch)\n",
    "    apply_fn = utils.apply_fn_wrapper(model.apply, True)\n",
    "    apply_fn_raw = model.apply\n",
    "\n",
    "    # Training before finding the FIM matrix\n",
    "    print(\"Creating optimizers\")\n",
    "    step = trainer.step_identity_cov\n",
    "    get_train_batch_fn = dataset_multi_infinite.get_training_batch\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_mean = optax.adam(learning_rate = meta_lr)\n",
    "    mean_init = np.zeros( (utils.get_param_size(init_vars[\"params\"]),) )\n",
    "\n",
    "    pre_state = train_states.TrainStateIdentityCovariance.create(apply_fn=apply_fn, apply_fn_raw=apply_fn_raw, params=init_vars[\"params\"], mean=mean_init, tx_params=optimizer_params, tx_mean=optimizer_mean, batch_stats=FrozenDict())\n",
    "\n",
    "    def eval_during_pre_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_mean = state.mean\n",
    "        current_batch_stats = state.batch_stats\n",
    "        kernel, kernel_self, jacobian = ntk.get_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        nlls = test.test_nll_one_kernel(subkey_1, kernel_self, jacobian, dataset_multi_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "        mses = test.test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_multi_infinite.get_test_batch, dataset_multi_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting first part of training (identity covariance)\")\n",
    "    key_pre, key = random.split(key)\n",
    "    pre_state, pre_losses, pre_evals = train_and_eval(key_pre, step, pre_n_epochs, pre_state, pre_n_tasks, pre_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_pre_training)\n",
    "    print(\"Finished first part of training\")\n",
    "\n",
    "    # FIM\n",
    "    print(\"Finding projection matrix\")\n",
    "    key_fim, key_data, key = random.split(key, 3)\n",
    "    # here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "    # P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "    P1 = fim.proj_sketch(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, batch_stats=pre_state.batch_stats, batches=random.uniform(key_data, shape=(100, 1761, 1), minval=-5, maxval=5), subspace_dimension=subspace_dimension)\n",
    "    print(\"Found projection matrix\")\n",
    "\n",
    "    # Usual training with projection\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_lowdim_cov_singGP\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_mean = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_scale = optax.adam(learning_rate = meta_lr)\n",
    "    init_scale = np.ones( (subspace_dimension,) )\n",
    "\n",
    "    post_state = train_states.TrainStateLowDimCovSingGP.create(apply_fn = apply_fn, apply_fn_raw=apply_fn_raw, params = pre_state.params, mean=pre_state.mean, scale=init_scale, tx_params = optimizer_params, tx_mean = optimizer_mean, tx_scale = optimizer_scale, batch_stats=pre_state.batch_stats, proj = P1)\n",
    "\n",
    "    def eval_during_post_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_batch_stats = state.batch_stats\n",
    "        current_mean = state.mean\n",
    "        current_scale = state.scale\n",
    "        kernel, kernel_self, jacobian = ntk.get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_scale, current_batch_stats, P1)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        nlls = test.test_nll_one_kernel(subkey_1, kernel_self, jacobian, dataset_multi_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "        mses = test.test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_multi_infinite.get_test_batch, dataset_multi_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    key_post, key = random.split(key)\n",
    "    post_state, post_losses, post_evals = train_and_eval(key_post, step, post_n_epochs, post_state, post_n_tasks, post_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_post_training)\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # Returning everything\n",
    "    return init_vars, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24763136-051f-4eb5-aa90-7d962166d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals = unlimtd_f_multi_modal_singGP(seed=seed,\n",
    "                                                                                     pre_n_epochs=10000,\n",
    "                                                                                     pre_n_tasks=24,\n",
    "                                                                                     pre_K=10,\n",
    "                                                                                     post_n_epochs=10000,\n",
    "                                                                                     post_n_tasks=24,\n",
    "                                                                                     post_K=10,\n",
    "                                                                                     data_noise=0.05, \n",
    "                                                                                     maddox_noise=0.05,\n",
    "                                                                                     meta_lr=0.001,\n",
    "                                                                                     subspace_dimension=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba2e0d5-c4c9-49eb-9c10-9b734e15a47b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta_lr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     12\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubspace_dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 13\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mpre_losses\u001b[49m\n\u001b[1;32m     14\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39mpost_losses\n\u001b[1;32m     15\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_params\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39minit_params\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_losses' is not defined"
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "output[\"seed\"] = seed\n",
    "output[\"pre_n_epochs\"]=30000\n",
    "output[\"pre_n_tasks\"]=24\n",
    "output[\"pre_K\"]=10\n",
    "output[\"post_n_epochs\"]=30000\n",
    "output[\"post_n_tasks\"]=24\n",
    "output[\"post_K\"]=10\n",
    "output[\"data_noise\"]=0.05\n",
    "output[\"maddox_noise\"]=0.05\n",
    "output[\"meta_lr\"]=0.001\n",
    "output[\"subspace_dimension\"]=10\n",
    "output[\"pre_losses\"]=pre_losses\n",
    "output[\"post_losses\"]=post_losses\n",
    "output[\"init_params\"]=init_params\n",
    "output[\"intermediate_params\"]=pre_state.params\n",
    "output[\"trained_params\"]=post_state.params\n",
    "output[\"intermediate_mean\"]=pre_state.mean\n",
    "output[\"trained_mean\"]=post_state.mean\n",
    "output[\"intermediate_batch_stats\"]=pre_state.batch_stats\n",
    "output[\"trained_batch_stats\"]=post_state.batch_stats\n",
    "output[\"trained_scale\"]=post_state.scale\n",
    "output[\"proj\"]=post_state.proj\n",
    "output[\"pre_evals\"]=pre_evals\n",
    "output[\"post_evals\"]=post_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2feddb-f127-4fa6-87c8-39c291866f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_zeroth_order.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b512739-7cb9-4ada-b6e3-b6dfbdd81c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_zeroth_order.pickle\", \"rb\") as handle:\n",
    "    output = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc4d670f-320d-4d4f-b4e8-c488e2396979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.small_network(40, \"relu\", 1)\n",
    "apply_fn = utils.apply_fn_wrapper(model.apply, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a3980a3-ffcf-4e75-865c-3bea596accc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'trained_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kernel, kernel_self, jac \u001b[38;5;241m=\u001b[39m ntk\u001b[38;5;241m.\u001b[39mget_kernel_and_jac_lowdim_cov(apply_fn, \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrained_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m], output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_batch_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m], output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'trained_params'"
     ]
    }
   ],
   "source": [
    "kernel, kernel_self, jac = ntk.get_kernel_and_jac_lowdim_cov(apply_fn, output[\"trained_params\"], output[\"trained_scale\"], output[\"trained_batch_stats\"], output[\"proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c20a535e-3b83-4637-b609-47e65df82237",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "476ce2a6-e479-4d27-a69c-2f8e15b25782",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m----> 2\u001b[0m plots\u001b[38;5;241m.\u001b[39mplot_notebooks(subkey, \u001b[43mkernel\u001b[49m, kernel_self, jac, output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m10\u001b[39m, dataset_sines_infinite)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kernel' is not defined"
     ]
    }
   ],
   "source": [
    "key, subkey = random.split(key)\n",
    "plots.plot_notebooks(subkey, kernel, kernel_self, jac, output[\"trained_mean\"], 10, dataset_sines_infinite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63391e8a-82da-4a8c-82dc-fb81ebea6858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
