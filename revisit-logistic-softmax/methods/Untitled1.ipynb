{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6953db-51bb-4cbb-bf7d-0cf9037dda74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'backbone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbackbone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'backbone'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import backbone\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.weight_norm import WeightNorm\n",
    "\n",
    "#Redefine Conv4 here :\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)    \n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    maml = False # Default\n",
    "\n",
    "    def __init__(self, depth, n_way=-1, flatten=True, padding=1, bn=False):\n",
    "        super(ConvNet, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            indim = 3 if i == 0 else 64\n",
    "            outdim = 64\n",
    "            if self.maml:\n",
    "                conv_layer = Conv2d_fw(indim, outdim, 3, padding=padding)\n",
    "                if bn:\n",
    "                    BN     = BatchNorm2d_fw(outdim)\n",
    "            else:\n",
    "                conv_layer = nn.Conv2d(indim, outdim, 3, stride=1, padding=padding, bias=False)\n",
    "                if bn:\n",
    "                    BN     = nn.BatchNorm2d(outdim)\n",
    "            \n",
    "            relu = nn.ReLU(inplace=True)\n",
    "            layers.append(conv_layer)\n",
    "            if bn:\n",
    "                layers.append(BN)\n",
    "            layers.append(relu)\n",
    "\n",
    "            if i < 4:  # Pooling only for the first 4 layers\n",
    "                pool = nn.MaxPool2d(2)\n",
    "                layers.append(pool)\n",
    "\n",
    "            # Initialize the layers\n",
    "            init_layer(conv_layer)\n",
    "            if bn:\n",
    "                init_layer(BN)\n",
    "\n",
    "        if flatten:\n",
    "            layers.append(Flatten())\n",
    "        \n",
    "        if n_way>0:\n",
    "            layers.append(nn.Linear(1600,n_way))\n",
    "            self.final_feat_dim = n_way\n",
    "        else:\n",
    "            self.final_feat_dim = 1600\n",
    "            \n",
    "        self.trunk = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.trunk(x)\n",
    "        return out\n",
    "\n",
    "def Conv4_mine():\n",
    "    print(\"Conv4 No Batch Normalization\")\n",
    "    return ConvNet(4, bn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d536a91-cd54-4431-8d8f-d86465517b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
