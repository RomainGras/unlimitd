{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded4809c-df55-4bb9-bfd9-5bbe25b8c7e7",
   "metadata": {},
   "source": [
    "# This is resnet NTK trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4ff0eb-0a60-4fc5-9206-9bb3ae087d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'type'>\n",
      "<class '__main__.simple_netC_0hl'>\n"
     ]
    }
   ],
   "source": [
    "from backbone import ResNet101, ResNet34\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "import time\n",
    "device = 'cuda'\n",
    "if device=='cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "class simple_netC_0hl(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simple_netC_0hl, self).__init__()\n",
    "        self.layer1 = nn.Linear(512, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        return out\n",
    "\n",
    "print(type(simple_netC_0hl))\n",
    "print(type(simple_netC_0hl()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e6fef2-a472-4ee3-919c-ff1fdb5f1f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[972, 36, 11664, 36, 11664, 36, 8820, 5]\n"
     ]
    }
   ],
   "source": [
    "import backbone\n",
    "\n",
    "combined_Conv3 = backbone.CombinedNetwork(backbone.Conv3(), nn.Linear(1764,1))\n",
    "print([p.numel() for p in combined_Conv3.parameters()])\n",
    "\n",
    "x1 = torch.randn(5, 3, 84, 84)\n",
    "\n",
    "def compute_jacobian(net, inputs):   # i is the class label, and corresponds to the output targeted\n",
    "    \"\"\"\n",
    "    Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "    \"\"\"\n",
    "    net.zero_grad()\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "\n",
    "    def fnet_single(params, x):\n",
    "        # Make sure output has the right dimensions\n",
    "        return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "    jac_values = jac.values()\n",
    "\n",
    "    reshaped_tensors = []\n",
    "    for j in jac_values:\n",
    "        if len(j.shape) == 3:  # For layers with weights\n",
    "            # Flatten parameters dimensions and then reshape\n",
    "            flattened = j.flatten(start_dim=1)  # Flattens to [batch, params]\n",
    "            reshaped = flattened.T  # Transpose to align dimensions as [params, batch]\n",
    "            reshaped_tensors.append(reshaped)\n",
    "        elif len(j.shape) == 2:  # For biases or single parameter components\n",
    "            reshaped_tensors.append(j.T)  # Simply transpose\n",
    "\n",
    "    # Concatenate all the reshaped tensors into one large matrix\n",
    "    return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2644e8-e0f6-459e-900a-3af289603514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in network : 21284672\n",
      "Number of params in network : 21284672\n",
      "Final dimension of the network : 512\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet34()\n",
    "\n",
    "print(f\"Number of params in network : {sum(p.numel() for p in resnet.parameters() if p.requires_grad_)}\")\n",
    "resnet.eval()\n",
    "print(f\"Number of params in network : {sum(p.numel() for p in resnet.parameters() if p.requires_grad_)}\")\n",
    "resnet.train()\n",
    "print(f\"Final dimension of the network : {resnet.final_feat_dim}\")\n",
    "# print(resnet.trunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a909b588-2e63-49b1-b381-362eb1427864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Number of params in network : 21287237\n"
     ]
    }
   ],
   "source": [
    "class CombinedNetwork(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super(CombinedNetwork, self).__init__()\n",
    "        self.networks = nn.Sequential(\n",
    "            net1,\n",
    "            net2\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.networks(x)\n",
    "\n",
    "combined_net = CombinedNetwork(resnet, simple_netC_0hl())\n",
    "print(type(combined_net) == CombinedNetwork)\n",
    "print(f\"Number of params in network : {sum(p.numel() for p in combined_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5af8484-9647-4cff-8627-f1a6ab2c3512",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (85x2048 and 512x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m input1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m])\n\u001b[1;32m     32\u001b[0m input2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m ntk_value \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ntk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNTK value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ntk_value)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mcompute_ntk\u001b[0;34m(model, x1, x2)\u001b[0m\n\u001b[1;32m      6\u001b[0m x2\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m y2 \u001b[38;5;241m=\u001b[39m model(x2)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Initialize NTK value\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mCombinedNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetworks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m, in \u001b[0;36msimple_netC_0hl.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (85x2048 and 512x5)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def compute_ntk(model, x1, x2):\n",
    "    # Ensure gradients are enabled\n",
    "    x1.requires_grad_(True)\n",
    "    x2.requires_grad_(True)\n",
    "\n",
    "    # Forward pass\n",
    "    y1 = model(x1)\n",
    "    y2 = model(x2)\n",
    "\n",
    "        # Initialize NTK value\n",
    "    ntk = 0.0\n",
    "\n",
    "    # Compute the NTK for each output element\n",
    "    # print(y1.shape)\n",
    "    for j in range(y1.shape[1]):  # Assuming y1 and y2 are of shape (batch_size, num_classes)\n",
    "        grad_y1 = torch.autograd.grad(y1[:, j].sum(), model.parameters(), retain_graph=True, create_graph=True)\n",
    "        grad_y2 = torch.autograd.grad(y2[:, j].sum(), model.parameters(), retain_graph=True, create_graph=True)\n",
    "        # print(sum([g1.numel() for g1 in grad_y1]))\n",
    "        # print(grad_y1[0].shape)\n",
    "        # print(grad_y2[0].shape)\n",
    "        # print(grad_y1[1].shape)\n",
    "        # print(grad_y2[1].shape)\n",
    "        # Compute the NTK (dot product of gradients) for the current output element\n",
    "        ntk += sum((g1 * g2).sum() for g1, g2 in zip(grad_y1, grad_y2))\n",
    "    print(ntk, ntk.item())\n",
    "    return ntk.item()\n",
    "\n",
    "# Example usage\n",
    "input1 = torch.randn([5*17, 3, 224, 224])\n",
    "input2 = torch.randn([5*17, 3, 224, 224])\n",
    "\n",
    "ntk_value = compute_ntk(combined_net, input1, input2)\n",
    "print(\"NTK value:\", ntk_value)\n",
    "print(f\"Total time : {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1cff1e-2cb5-4b95-8219-938ed3acfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1d38b-93d1-48b3-9102-ae59ed9ef186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def compute_jacobian_autodiff(net, inputs, c):\n",
    "    \"\"\"\n",
    "    Return the jacobian of a batch of inputs, using autodifferentiation\n",
    "    \"\"\"\n",
    "    inputs.requires_grad_(True)\n",
    "    outputs = net(inputs)\n",
    "    N = sum(p.numel() for p in net.parameters())\n",
    "    jac = torch.empty(outputs.size(0), N)\n",
    "    for j in range(outputs.size(0)):\n",
    "        # print(j)\n",
    "        grad_y1 = torch.autograd.grad(outputs[j, c], net.parameters(), retain_graph=True, create_graph=True) # We need to retain every single graph for the gradient to be able to run through\n",
    "        # print_memory_usage()\n",
    "        flattened_tensors = [t.flatten() for t in grad_y1]\n",
    "        jac[j] = torch.cat(flattened_tensors)\n",
    "        # print_memory_usage()\n",
    "        # if device == \"cuda\":\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     print_memory_usage()\n",
    "    return jac\n",
    "\n",
    "for_loop_jac = compute_jacobian_autodiff(combined_net.to(device), input1.to(device), 2)\n",
    "print(for_loop_jac.shape)\n",
    "print(f\"Total time : {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b5a30b-8c30-4aaa-bf4a-c9f78253bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Define a function to get parameters as a single tensor\n",
    "def get_params_tensor(net):\n",
    "    params = []\n",
    "    for param in net.parameters():\n",
    "        params.append(param.view(-1))\n",
    "    return torch.cat(params)\n",
    "\n",
    "# Define the function for which we want the Jacobian\n",
    "def net_with_params(params, input_tensor):\n",
    "    # Set the parameters to the network\n",
    "    start_idx = 0\n",
    "    for param in combined_net.parameters():\n",
    "        param_length = param.numel()\n",
    "        param.data.copy_(params[start_idx:start_idx + param_length].view(param.size()))\n",
    "        start_idx += param_length\n",
    "    \n",
    "    return combined_net(input_tensor)\n",
    "\n",
    "# Example input\n",
    "input_tensor = torch.randn(25,3,84,84, device = device, requires_grad=True)\n",
    "\n",
    "# Get the current parameters as a single tensor\n",
    "params = get_params_tensor(combined_net.to(device))\n",
    "\n",
    "# Compute the Jacobian\n",
    "jacobian = torch.autograd.functional.jacobian(lambda p: net_with_params(p, input_tensor), params)\n",
    "print(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa91e394-7e07-4bcb-8d4f-598793eb1162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.115832805633545\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def compute_jacobian_autodiff(net, inputs):\n",
    "    \"\"\"\n",
    "    Return the jacobian of a batch of inputs, using autodifferentiation\n",
    "    Useful for when dealing with models using batch normalization or other kind of running statistics\n",
    "    \"\"\"\n",
    "    inputs.requires_grad_(True)\n",
    "    outputs = net(inputs)\n",
    "    N = sum(p.numel() for p in net.parameters())\n",
    "    jac = torch.empty(outputs.size(0), N)\n",
    "    for j in range(outputs.size(0)):\n",
    "        # print(j)\n",
    "        grad_y1 = torch.autograd.grad(outputs[j, 2], net.parameters(), retain_graph=True, create_graph=True) # We need to create and retain every single graph for the gradient to be able to run through during backprop\n",
    "        # print_memory_usage()\n",
    "        flattened_tensors = [t.flatten() for t in grad_y1]\n",
    "        jac[j] = torch.cat(flattened_tensors)\n",
    "        # print_memory_usage()\n",
    "        # if device == \"cuda\":\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     print_memory_usage()\n",
    "    return jac\n",
    "\n",
    "input1 = torch.randn(80, 3, 84, 84)\n",
    "\n",
    "compute_jacobian_autodiff(combined_net.to(device), input1.to(device))\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123c20d9-0bf5-4326-b67e-aeba32dd2276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 502.18 MB\n",
      "Cached: 29484.00 MB\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'for_loop_jac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m print_memory_usage()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfor_loop_jac\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'for_loop_jac' is not defined"
     ]
    }
   ],
   "source": [
    "print_memory_usage()\n",
    "print(for_loop_jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84507793-72f2-41a6-a306-e8d9ce643cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def compute_jacobian_vmap_autodiff(net, inputs, c):\n",
    "    \"\"\"\n",
    "    Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "    \"\"\"\n",
    "    params_that_need_grad = []\n",
    "    for param in net.parameters():\n",
    "        if param.requires_grad:\n",
    "            params_that_need_grad.append(param.requires_grad)\n",
    "    \n",
    "    inputs = inputs.to(device, non_blocking=True)\n",
    "    inputs.requires_grad_(True)\n",
    "    outputs = net(inputs)\n",
    "    basis_vectors = torch.eye(len(inputs),device=device,dtype=torch.bool)\n",
    "    J_layer = []\n",
    "    for i,z in enumerate(net.named_parameters()):\n",
    "        if not(params_that_need_grad[i]): #if it didnt need a grad, we can skip it.\n",
    "            continue\n",
    "        name, param = z\n",
    "        outputsc = outputs[:, c]   \n",
    "        #Seems like for retain_graph=False, you might need to do multiple forward passes.\n",
    "        \n",
    "        def torch_row_Jacobian(v): #y would have to be a single piece of the batch\n",
    "            return torch.autograd.grad(outputsc,param,v, retain_graph=True, create_graph=True)[0].reshape(-1)\n",
    "        J_layer.append(vmap(torch_row_Jacobian)(basis_vectors).detach())\n",
    "        \n",
    "        del outputsc\n",
    "        if device=='cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        #print(name)\n",
    "    #for layer in J_layer:\n",
    "    #    print(layer.shape)\n",
    "    del params_that_need_grad\n",
    "    del inputs\n",
    "    del outputs\n",
    "    del basis_vectors\n",
    "    if device=='cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    J_layer = torch.cat(J_layer, axis=1)\n",
    "    return J_layer\n",
    "    \n",
    "\n",
    "vmap_jac = compute_jacobian_vmap_autodiff(combined_net.to(device), input1.to(device), 2)\n",
    "print(vmap_jac.shape)\n",
    "#print(vmap_jac.cpu() == for_loop_jac)\n",
    "print(f\"Total time : {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef1fc6c-c23d-4954-b2f4-100e810a4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 520.861328125 MB\n",
      "Cached: 30972.0 MB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 8            |        cudaMalloc retries: 8         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 533362 KiB |  29307 MiB | 551117 MiB | 550596 MiB |\n",
      "|       from large pool | 527032 KiB |  29288 MiB | 551040 MiB | 550526 MiB |\n",
      "|       from small pool |   6330 KiB |     19 MiB |     76 MiB |     70 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 533362 KiB |  29307 MiB | 551117 MiB | 550596 MiB |\n",
      "|       from large pool | 527032 KiB |  29288 MiB | 551040 MiB | 550526 MiB |\n",
      "|       from small pool |   6330 KiB |     19 MiB |     76 MiB |     70 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 526806 KiB |  29294 MiB | 550906 MiB | 550391 MiB |\n",
      "|       from large pool | 520508 KiB |  29275 MiB | 550829 MiB | 550321 MiB |\n",
      "|       from small pool |   6298 KiB |     19 MiB |     76 MiB |     70 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  30972 MiB |  30972 MiB |  77084 MiB |  46112 MiB |\n",
      "|       from large pool |  30952 MiB |  30952 MiB |  77064 MiB |  46112 MiB |\n",
      "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  11406 KiB |  12779 MiB | 689957 MiB | 689945 MiB |\n",
      "|       from large pool |   9544 KiB |  12777 MiB | 689869 MiB | 689860 MiB |\n",
      "|       from small pool |   1862 KiB |      2 MiB |     87 MiB |     85 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     375    |     648    |    2761    |    2386    |\n",
      "|       from large pool |      96    |     204    |    1491    |    1395    |\n",
      "|       from small pool |     279    |     444    |    1270    |     991    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     375    |     648    |    2761    |    2386    |\n",
      "|       from large pool |      96    |     204    |    1491    |    1395    |\n",
      "|       from small pool |     279    |     444    |    1270    |     991    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      55    |      55    |     103    |      48    |\n",
      "|       from large pool |      45    |      45    |      93    |      48    |\n",
      "|       from small pool |      10    |      10    |      10    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      11    |      37    |     944    |     933    |\n",
      "|       from large pool |       7    |      29    |     725    |     718    |\n",
      "|       from small pool |       4    |      11    |     219    |     215    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print total memory allocated and cached on the GPU\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024 ** 2} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024 ** 2} MB\")\n",
    "\n",
    "# For more detailed information, use:\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246c6be-e3cb-4424-855b-ec0962d085a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb96f08f-bcf6-405e-aa5a-4feffe3f75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21287237\n",
      "21287237\n",
      "torch.Size([85, 21287237])\n",
      "21287237\n",
      "21287237\n",
      "torch.Size([85, 21287237])\n",
      "21287237\n",
      "21287237\n",
      "torch.Size([85, 21287237])\n",
      "21287237\n",
      "21287237\n",
      "torch.Size([85, 21287237])\n",
      "21287237\n",
      "21287237\n",
      "torch.Size([85, 21287237])\n",
      "torch.Size([425, 21287237])\n",
      "NTK value: tensor([[311973.9375, 311973.9375, 311973.9375,  ...,  -6109.9062,\n",
      "          -6109.9062,  -6109.9062],\n",
      "        [311973.9375, 311973.9375, 311973.9375,  ...,  -6109.9062,\n",
      "          -6109.9062,  -6109.9062],\n",
      "        [311973.9375, 311973.9375, 311973.9375,  ...,  -6109.9062,\n",
      "          -6109.9062,  -6109.9062],\n",
      "        ...,\n",
      "        [ -6109.9062,  -6109.9062,  -6109.9062,  ..., 327817.5000,\n",
      "         327817.5000, 327817.5000],\n",
      "        [ -6109.9062,  -6109.9062,  -6109.9062,  ..., 327817.5000,\n",
      "         327817.5000, 327817.5000],\n",
      "        [ -6109.9062,  -6109.9062,  -6109.9062,  ..., 327817.5000,\n",
      "         327817.5000, 327817.5000]], grad_fn=<MmBackward0>)\n",
      "NTK sum value: tensor(1.3333e+10, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def compute_jacobian(net, inputs, c):\n",
    "    \"\"\"\n",
    "    Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "    \"\"\"\n",
    "    print(sum(p.numel() for p in net.parameters() if p.requires_grad_))\n",
    "    params = {k: v for k, v in net.named_parameters() if v.requires_grad_}\n",
    "\n",
    "    def fnet_single(params, x):\n",
    "        # Make sure output has the right dimensions\n",
    "        return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)[c]\n",
    "\n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "    jac_values = jac.values()\n",
    "\n",
    "    reshaped_tensors = []\n",
    "    for j in jac_values:\n",
    "        if len(j.shape) >= 3:  # For layers with weights\n",
    "            # Flatten parameters dimensions and then reshape\n",
    "            flattened = j.flatten(start_dim=1)  # Flattens to [batch, params]\n",
    "            reshaped = flattened.T  # Transpose to align dimensions as [params, batch]\n",
    "            reshaped_tensors.append(reshaped)\n",
    "        elif len(j.shape) == 2:  # For biases or single parameter components\n",
    "            reshaped_tensors.append(j.T)  # Simply transpose\n",
    "\n",
    "    # Concatenate all the reshaped tensors into one large matrix\n",
    "    return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "\n",
    "def compute_ntk2(model, x1, x2):\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    j1 = []\n",
    "    j2 = []\n",
    "    for c in range(5):\n",
    "        j1c = compute_jacobian(model, x1, c)\n",
    "        j2c = compute_jacobian(model, x2, c)\n",
    "        print(j1c.shape)\n",
    "        j1.append(j1c)\n",
    "        j2.append(j2c)\n",
    "    j1 = torch.cat(j1, dim=0)\n",
    "    j2 = torch.cat(j2, dim=0)\n",
    "    model.train()\n",
    "    \n",
    "    print(j1.shape)\n",
    "    return j1@j2.T\n",
    "\n",
    "# Example usage\n",
    "input1 = torch.ones([5*17, 3, 84, 84])\n",
    "input2 = torch.ones([5*17, 3, 84, 84])\n",
    "\n",
    "ntk_value = compute_ntk2(combined_net, input1, input2)\n",
    "print(\"NTK value:\", ntk_value)\n",
    "print(\"NTK sum value:\", sum(sum(ntk_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ec9e6-b17b-4345-9d3f-4a9f5f26fffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
