{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2716065c-80ac-407d-8875-c0aa2e21ec1b",
   "metadata": {},
   "source": [
    "# Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00f5ae6-52d4-475a-8ddf-9639663a22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import math\n",
    "import numpy as np\n",
    "device = 'cuda'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5c76c99-85f2-4e78-8de0-ccc0f25b6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine Conv4 here :\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)    \n",
    "    \n",
    "class ConvNetNoBN(nn.Module):\n",
    "    maml = False # Default\n",
    "\n",
    "    def __init__(self, depth, n_way=-1, flatten=True, padding=1):\n",
    "        super(ConvNetNoBN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            indim = 3 if i == 0 else 64\n",
    "            outdim = 64\n",
    "            if self.maml:\n",
    "                conv_layer = Conv2d_fw(indim, outdim, 3, padding=padding)\n",
    "                # BN     = BatchNorm2d_fw(outdim)\n",
    "            else:\n",
    "                conv_layer = nn.Conv2d(indim, outdim, 3, stride=1, padding=padding, bias=False)\n",
    "                # BN     = nn.BatchNorm2d(outdim)\n",
    "            \n",
    "            relu = nn.ReLU(inplace=True)\n",
    "            layers.append(conv_layer)\n",
    "            # layers.append(BN)\n",
    "            layers.append(relu)\n",
    "\n",
    "            if i < 4:  # Pooling only for the first 4 layers\n",
    "                pool = nn.MaxPool2d(2)\n",
    "                layers.append(pool)\n",
    "\n",
    "            # Initialize the layers\n",
    "            init_layer(conv_layer)\n",
    "            # init_layer(BN)\n",
    "\n",
    "        if flatten:\n",
    "            layers.append(Flatten())\n",
    "        \n",
    "        if n_way>0:\n",
    "            layers.append(nn.Linear(1600,n_way))\n",
    "            self.final_feat_dim = n_way\n",
    "        else:\n",
    "            self.final_feat_dim = 1600\n",
    "            \n",
    "        self.trunk = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.trunk[0].weight)\n",
    "        for i, layer in enumerate(self.trunk):\n",
    "            print(f\"Input shape before layer {i}: {x.shape}\")\n",
    "            print(layer.weight)\n",
    "            x = layer(x)\n",
    "            print(f\"Output shape after layer {i}: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "def Conv4NoBN():\n",
    "    print(\"Conv4 No Batch Normalization\")\n",
    "    return ConvNetNoBN(4)\n",
    "\n",
    "def Conv4NoBN_class(n_way=5):\n",
    "    print(\"Conv4 No Batch Normalization with final classifier layer of 5 way\")\n",
    "    return ConvNetNoBN(4, n_way=n_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8314e7b-fae3-4688-9aca-a6c0974bb0d5",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f91a2f4c-ad3c-4eab-a5d5-4ab0545af83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "\n",
    "n_support = 7\n",
    "n_query = 10\n",
    "\n",
    "n_inner_upd = 3\n",
    "n_task = 4\n",
    "\n",
    "eps = 1e-4\n",
    "lr_in = 1e-2\n",
    "lr_out = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4785d6c-2234-4c5c-8e84-5d77a386f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv4 No Batch Normalization with final classifier layer of 5 way\n"
     ]
    }
   ],
   "source": [
    "x_support = torch.randn(n_way * n_support, 3, 84, 84, device=device)\n",
    "y_support = Variable(torch.from_numpy(np.repeat(range(n_way), n_support)).cuda())\n",
    "x_query = torch.randn(n_way * n_query, 3, 84, 84, device=device)\n",
    "y_query = Variable(torch.from_numpy(np.repeat(range(n_way), n_query)).cuda())\n",
    "\n",
    "\n",
    "net = Conv4NoBN_class().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "355360fc-bade-437a-b2a3-4f08745ba26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params dict is equal to the net's params \n",
      "\n",
      "tensor([[ 0.1177,  0.0025,  0.0485],\n",
      "        [ 0.0203, -0.0412,  0.0553],\n",
      "        [ 0.0211,  0.0470,  0.1086]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[ 0.1177,  0.0025,  0.0485],\n",
      "        [ 0.0203, -0.0412,  0.0553],\n",
      "        [ 0.0211,  0.0470,  0.1086]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "params = {k: v for k, v in net.named_parameters()}\n",
    "s = {k: torch.ones_like(v) for (k, v) in params.items()}\n",
    "\n",
    "print(\"params dict is equal to the net's params \\n\")\n",
    "print(net.trunk[0].weight[0][0])\n",
    "print(list(params.values())[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e7576-5e8c-4bdb-be5e-7741bc3e8cc4",
   "metadata": {},
   "source": [
    "## INNER LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5231f7-9e3f-4f9d-aacd-215c91f8b965",
   "metadata": {},
   "source": [
    "## NTK computation\n",
    "\n",
    "Note :\n",
    "USE FAST WEIGHT AS THEY DID FOR MAML (nothing is supposed to pose a problem except for batch norm layers as usual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7cdd90a-cbdf-4ea2-b682-61d381e033aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before layer 0: torch.Size([35, 3, 84, 84])\n",
      "Parameter containing:\n",
      "tensor([[[[[ 0.0405, -0.0636,  0.0121],\n",
      "           [ 0.0634, -0.0317,  0.0556],\n",
      "           [-0.0851, -0.0781,  0.0151]],\n",
      "\n",
      "          [[-0.0455,  0.0024, -0.0177],\n",
      "           [-0.0468,  0.0569, -0.0198],\n",
      "           [ 0.1005,  0.0518, -0.0946]],\n",
      "\n",
      "          [[-0.0023,  0.0403,  0.0110],\n",
      "           [-0.0639,  0.1388,  0.0300],\n",
      "           [ 0.1657,  0.0036, -0.0303]]],\n",
      "\n",
      "\n",
      "         [[[-0.0527, -0.0552, -0.0272],\n",
      "           [ 0.0181,  0.0323, -0.1041],\n",
      "           [-0.0070,  0.0066, -0.1254]],\n",
      "\n",
      "          [[-0.1010, -0.0516, -0.0507],\n",
      "           [-0.0045, -0.0910,  0.1314],\n",
      "           [-0.0130,  0.0905, -0.1148]],\n",
      "\n",
      "          [[ 0.0230,  0.0835,  0.0607],\n",
      "           [ 0.1031,  0.0074,  0.0239],\n",
      "           [-0.1148, -0.0089, -0.0562]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0020, -0.0318,  0.0139],\n",
      "           [-0.0036,  0.0868, -0.0372],\n",
      "           [ 0.0691, -0.0390,  0.0578]],\n",
      "\n",
      "          [[ 0.0474, -0.0188, -0.0784],\n",
      "           [ 0.0341,  0.0010, -0.0400],\n",
      "           [ 0.0419,  0.0344, -0.0416]],\n",
      "\n",
      "          [[-0.0743,  0.0922,  0.0654],\n",
      "           [-0.0117,  0.0894,  0.0827],\n",
      "           [ 0.0572,  0.0231, -0.0075]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1239,  0.0361,  0.0144],\n",
      "           [-0.0825,  0.0138, -0.0328],\n",
      "           [-0.0135,  0.1050, -0.0638]],\n",
      "\n",
      "          [[ 0.0153,  0.0751,  0.0104],\n",
      "           [-0.0508, -0.0280, -0.0043],\n",
      "           [ 0.0017,  0.0306,  0.0950]],\n",
      "\n",
      "          [[-0.0703,  0.0386,  0.0157],\n",
      "           [-0.0065, -0.0609, -0.0259],\n",
      "           [-0.0975,  0.0964, -0.0428]]],\n",
      "\n",
      "\n",
      "         [[[-0.0316,  0.0301,  0.0056],\n",
      "           [ 0.0739, -0.0921,  0.0671],\n",
      "           [-0.0123, -0.1024, -0.0663]],\n",
      "\n",
      "          [[-0.0077, -0.0599, -0.1021],\n",
      "           [-0.0622, -0.0300,  0.0165],\n",
      "           [ 0.0381,  0.0030, -0.0209]],\n",
      "\n",
      "          [[ 0.0207, -0.0161,  0.0265],\n",
      "           [-0.0070,  0.0794, -0.1328],\n",
      "           [-0.0117,  0.1400, -0.0019]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0132, -0.0490,  0.0506],\n",
      "           [-0.0014, -0.0456,  0.0132],\n",
      "           [ 0.0248, -0.0372, -0.0192]],\n",
      "\n",
      "          [[ 0.0633,  0.0193,  0.0220],\n",
      "           [ 0.0513,  0.0488, -0.0260],\n",
      "           [-0.0443,  0.0265,  0.0434]],\n",
      "\n",
      "          [[ 0.0332, -0.1054, -0.0166],\n",
      "           [ 0.0921,  0.0092, -0.0363],\n",
      "           [ 0.0721,  0.0451, -0.0174]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0405, -0.0636,  0.0121],\n",
      "           [ 0.0635, -0.0317,  0.0555],\n",
      "           [-0.0851, -0.0781,  0.0152]],\n",
      "\n",
      "          [[-0.0455,  0.0024, -0.0177],\n",
      "           [-0.0468,  0.0570, -0.0199],\n",
      "           [ 0.1004,  0.0518, -0.0946]],\n",
      "\n",
      "          [[-0.0022,  0.0402,  0.0111],\n",
      "           [-0.0638,  0.1387,  0.0300],\n",
      "           [ 0.1656,  0.0036, -0.0302]]],\n",
      "\n",
      "\n",
      "         [[[-0.0527, -0.0552, -0.0272],\n",
      "           [ 0.0181,  0.0323, -0.1041],\n",
      "           [-0.0069,  0.0066, -0.1254]],\n",
      "\n",
      "          [[-0.1010, -0.0515, -0.0507],\n",
      "           [-0.0044, -0.0910,  0.1314],\n",
      "           [-0.0130,  0.0904, -0.1148]],\n",
      "\n",
      "          [[ 0.0230,  0.0835,  0.0607],\n",
      "           [ 0.1031,  0.0074,  0.0239],\n",
      "           [-0.1147, -0.0089, -0.0562]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0019, -0.0319,  0.0139],\n",
      "           [-0.0036,  0.0869, -0.0371],\n",
      "           [ 0.0690, -0.0390,  0.0578]],\n",
      "\n",
      "          [[ 0.0474, -0.0189, -0.0785],\n",
      "           [ 0.0341,  0.0010, -0.0400],\n",
      "           [ 0.0419,  0.0344, -0.0416]],\n",
      "\n",
      "          [[-0.0742,  0.0922,  0.0654],\n",
      "           [-0.0117,  0.0894,  0.0828],\n",
      "           [ 0.0572,  0.0230, -0.0075]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1240,  0.0360,  0.0144],\n",
      "           [-0.0825,  0.0138, -0.0328],\n",
      "           [-0.0135,  0.1049, -0.0638]],\n",
      "\n",
      "          [[ 0.0152,  0.0751,  0.0104],\n",
      "           [-0.0507, -0.0280, -0.0043],\n",
      "           [ 0.0018,  0.0306,  0.0950]],\n",
      "\n",
      "          [[-0.0702,  0.0387,  0.0157],\n",
      "           [-0.0065, -0.0609, -0.0258],\n",
      "           [-0.0975,  0.0964, -0.0428]]],\n",
      "\n",
      "\n",
      "         [[[-0.0315,  0.0302,  0.0056],\n",
      "           [ 0.0740, -0.0921,  0.0672],\n",
      "           [-0.0123, -0.1024, -0.0663]],\n",
      "\n",
      "          [[-0.0077, -0.0599, -0.1022],\n",
      "           [-0.0622, -0.0300,  0.0165],\n",
      "           [ 0.0381,  0.0030, -0.0209]],\n",
      "\n",
      "          [[ 0.0207, -0.0160,  0.0265],\n",
      "           [-0.0070,  0.0794, -0.1328],\n",
      "           [-0.0117,  0.1400, -0.0019]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0131, -0.0490,  0.0505],\n",
      "           [-0.0014, -0.0456,  0.0132],\n",
      "           [ 0.0248, -0.0372, -0.0193]],\n",
      "\n",
      "          [[ 0.0633,  0.0192,  0.0220],\n",
      "           [ 0.0512,  0.0487, -0.0260],\n",
      "           [-0.0443,  0.0265,  0.0434]],\n",
      "\n",
      "          [[ 0.0332, -0.1055, -0.0167],\n",
      "           [ 0.0921,  0.0093, -0.0363],\n",
      "           [ 0.0721,  0.0452, -0.0175]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0405, -0.0636,  0.0120],\n",
      "           [ 0.0635, -0.0317,  0.0555],\n",
      "           [-0.0851, -0.0781,  0.0152]],\n",
      "\n",
      "          [[-0.0455,  0.0024, -0.0177],\n",
      "           [-0.0469,  0.0570, -0.0198],\n",
      "           [ 0.1005,  0.0517, -0.0946]],\n",
      "\n",
      "          [[-0.0023,  0.0403,  0.0110],\n",
      "           [-0.0639,  0.1387,  0.0300],\n",
      "           [ 0.1657,  0.0035, -0.0302]]],\n",
      "\n",
      "\n",
      "         [[[-0.0526, -0.0552, -0.0272],\n",
      "           [ 0.0180,  0.0323, -0.1042],\n",
      "           [-0.0070,  0.0066, -0.1255]],\n",
      "\n",
      "          [[-0.1010, -0.0515, -0.0507],\n",
      "           [-0.0045, -0.0909,  0.1314],\n",
      "           [-0.0130,  0.0905, -0.1148]],\n",
      "\n",
      "          [[ 0.0230,  0.0835,  0.0607],\n",
      "           [ 0.1030,  0.0074,  0.0238],\n",
      "           [-0.1148, -0.0090, -0.0562]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0020, -0.0319,  0.0139],\n",
      "           [-0.0035,  0.0869, -0.0372],\n",
      "           [ 0.0691, -0.0390,  0.0578]],\n",
      "\n",
      "          [[ 0.0474, -0.0188, -0.0784],\n",
      "           [ 0.0340,  0.0010, -0.0401],\n",
      "           [ 0.0419,  0.0344, -0.0416]],\n",
      "\n",
      "          [[-0.0742,  0.0922,  0.0654],\n",
      "           [-0.0118,  0.0894,  0.0827],\n",
      "           [ 0.0572,  0.0230, -0.0074]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1239,  0.0360,  0.0145],\n",
      "           [-0.0826,  0.0138, -0.0328],\n",
      "           [-0.0135,  0.1049, -0.0638]],\n",
      "\n",
      "          [[ 0.0152,  0.0751,  0.0104],\n",
      "           [-0.0508, -0.0280, -0.0043],\n",
      "           [ 0.0017,  0.0306,  0.0950]],\n",
      "\n",
      "          [[-0.0702,  0.0387,  0.0157],\n",
      "           [-0.0065, -0.0609, -0.0258],\n",
      "           [-0.0974,  0.0964, -0.0428]]],\n",
      "\n",
      "\n",
      "         [[[-0.0316,  0.0301,  0.0056],\n",
      "           [ 0.0739, -0.0921,  0.0672],\n",
      "           [-0.0123, -0.1024, -0.0663]],\n",
      "\n",
      "          [[-0.0077, -0.0599, -0.1021],\n",
      "           [-0.0622, -0.0300,  0.0165],\n",
      "           [ 0.0381,  0.0030, -0.0209]],\n",
      "\n",
      "          [[ 0.0208, -0.0160,  0.0265],\n",
      "           [-0.0070,  0.0794, -0.1327],\n",
      "           [-0.0117,  0.1400, -0.0019]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0131, -0.0490,  0.0505],\n",
      "           [-0.0014, -0.0456,  0.0132],\n",
      "           [ 0.0247, -0.0372, -0.0192]],\n",
      "\n",
      "          [[ 0.0633,  0.0192,  0.0220],\n",
      "           [ 0.0512,  0.0488, -0.0260],\n",
      "           [-0.0443,  0.0265,  0.0434]],\n",
      "\n",
      "          [[ 0.0333, -0.1055, -0.0167],\n",
      "           [ 0.0921,  0.0093, -0.0363],\n",
      "           [ 0.0721,  0.0451, -0.0175]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0405, -0.0636,  0.0120],\n",
      "           [ 0.0635, -0.0317,  0.0555],\n",
      "           [-0.0851, -0.0782,  0.0151]],\n",
      "\n",
      "          [[-0.0455,  0.0024, -0.0177],\n",
      "           [-0.0469,  0.0569, -0.0198],\n",
      "           [ 0.1005,  0.0518, -0.0946]],\n",
      "\n",
      "          [[-0.0023,  0.0403,  0.0110],\n",
      "           [-0.0638,  0.1388,  0.0300],\n",
      "           [ 0.1657,  0.0035, -0.0303]]],\n",
      "\n",
      "\n",
      "         [[[-0.0527, -0.0552, -0.0272],\n",
      "           [ 0.0181,  0.0323, -0.1042],\n",
      "           [-0.0070,  0.0066, -0.1255]],\n",
      "\n",
      "          [[-0.1010, -0.0515, -0.0507],\n",
      "           [-0.0045, -0.0910,  0.1314],\n",
      "           [-0.0130,  0.0905, -0.1148]],\n",
      "\n",
      "          [[ 0.0230,  0.0835,  0.0607],\n",
      "           [ 0.1031,  0.0075,  0.0238],\n",
      "           [-0.1148, -0.0089, -0.0562]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0019, -0.0319,  0.0139],\n",
      "           [-0.0035,  0.0868, -0.0371],\n",
      "           [ 0.0691, -0.0390,  0.0579]],\n",
      "\n",
      "          [[ 0.0474, -0.0189, -0.0784],\n",
      "           [ 0.0341,  0.0010, -0.0400],\n",
      "           [ 0.0419,  0.0344, -0.0417]],\n",
      "\n",
      "          [[-0.0743,  0.0922,  0.0654],\n",
      "           [-0.0118,  0.0894,  0.0827],\n",
      "           [ 0.0572,  0.0231, -0.0075]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1239,  0.0360,  0.0145],\n",
      "           [-0.0825,  0.0138, -0.0329],\n",
      "           [-0.0134,  0.1049, -0.0638]],\n",
      "\n",
      "          [[ 0.0152,  0.0751,  0.0104],\n",
      "           [-0.0507, -0.0281, -0.0043],\n",
      "           [ 0.0017,  0.0305,  0.0950]],\n",
      "\n",
      "          [[-0.0702,  0.0387,  0.0157],\n",
      "           [-0.0065, -0.0610, -0.0259],\n",
      "           [-0.0975,  0.0964, -0.0427]]],\n",
      "\n",
      "\n",
      "         [[[-0.0315,  0.0301,  0.0056],\n",
      "           [ 0.0739, -0.0921,  0.0671],\n",
      "           [-0.0123, -0.1024, -0.0663]],\n",
      "\n",
      "          [[-0.0077, -0.0599, -0.1022],\n",
      "           [-0.0622, -0.0300,  0.0166],\n",
      "           [ 0.0381,  0.0030, -0.0209]],\n",
      "\n",
      "          [[ 0.0207, -0.0161,  0.0264],\n",
      "           [-0.0070,  0.0794, -0.1327],\n",
      "           [-0.0117,  0.1400, -0.0020]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0131, -0.0490,  0.0505],\n",
      "           [-0.0014, -0.0456,  0.0132],\n",
      "           [ 0.0248, -0.0372, -0.0192]],\n",
      "\n",
      "          [[ 0.0633,  0.0192,  0.0220],\n",
      "           [ 0.0512,  0.0488, -0.0260],\n",
      "           [-0.0443,  0.0265,  0.0435]],\n",
      "\n",
      "          [[ 0.0332, -0.1054, -0.0167],\n",
      "           [ 0.0921,  0.0093, -0.0363],\n",
      "           [ 0.0721,  0.0451, -0.0175]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0405, -0.0636,  0.0121],\n",
      "           [ 0.0635, -0.0317,  0.0555],\n",
      "           [-0.0851, -0.0782,  0.0151]],\n",
      "\n",
      "          [[-0.0456,  0.0024, -0.0177],\n",
      "           [-0.0468,  0.0569, -0.0198],\n",
      "           [ 0.1005,  0.0518, -0.0945]],\n",
      "\n",
      "          [[-0.0023,  0.0403,  0.0110],\n",
      "           [-0.0638,  0.1388,  0.0300],\n",
      "           [ 0.1657,  0.0036, -0.0303]]],\n",
      "\n",
      "\n",
      "         [[[-0.0527, -0.0552, -0.0272],\n",
      "           [ 0.0180,  0.0324, -0.1041],\n",
      "           [-0.0070,  0.0066, -0.1255]],\n",
      "\n",
      "          [[-0.1010, -0.0516, -0.0507],\n",
      "           [-0.0044, -0.0909,  0.1313],\n",
      "           [-0.0129,  0.0905, -0.1147]],\n",
      "\n",
      "          [[ 0.0230,  0.0835,  0.0608],\n",
      "           [ 0.1030,  0.0074,  0.0238],\n",
      "           [-0.1147, -0.0088, -0.0562]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0019, -0.0319,  0.0139],\n",
      "           [-0.0035,  0.0869, -0.0372],\n",
      "           [ 0.0691, -0.0390,  0.0578]],\n",
      "\n",
      "          [[ 0.0474, -0.0189, -0.0785],\n",
      "           [ 0.0341,  0.0010, -0.0400],\n",
      "           [ 0.0419,  0.0344, -0.0417]],\n",
      "\n",
      "          [[-0.0742,  0.0922,  0.0654],\n",
      "           [-0.0118,  0.0894,  0.0827],\n",
      "           [ 0.0572,  0.0230, -0.0074]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 0.1239,  0.0361,  0.0144],\n",
      "           [-0.0826,  0.0138, -0.0328],\n",
      "           [-0.0134,  0.1050, -0.0638]],\n",
      "\n",
      "          [[ 0.0152,  0.0751,  0.0104],\n",
      "           [-0.0508, -0.0280, -0.0043],\n",
      "           [ 0.0018,  0.0305,  0.0950]],\n",
      "\n",
      "          [[-0.0702,  0.0387,  0.0157],\n",
      "           [-0.0066, -0.0609, -0.0259],\n",
      "           [-0.0975,  0.0964, -0.0427]]],\n",
      "\n",
      "\n",
      "         [[[-0.0316,  0.0302,  0.0056],\n",
      "           [ 0.0739, -0.0921,  0.0671],\n",
      "           [-0.0123, -0.1024, -0.0663]],\n",
      "\n",
      "          [[-0.0078, -0.0599, -0.1022],\n",
      "           [-0.0622, -0.0299,  0.0165],\n",
      "           [ 0.0381,  0.0030, -0.0209]],\n",
      "\n",
      "          [[ 0.0207, -0.0161,  0.0264],\n",
      "           [-0.0070,  0.0793, -0.1327],\n",
      "           [-0.0117,  0.1401, -0.0020]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0131, -0.0490,  0.0506],\n",
      "           [-0.0014, -0.0456,  0.0132],\n",
      "           [ 0.0248, -0.0372, -0.0192]],\n",
      "\n",
      "          [[ 0.0632,  0.0192,  0.0220],\n",
      "           [ 0.0513,  0.0488, -0.0260],\n",
      "           [-0.0443,  0.0265,  0.0434]],\n",
      "\n",
      "          [[ 0.0332, -0.1055, -0.0167],\n",
      "           [ 0.0921,  0.0093, -0.0363],\n",
      "           [ 0.0721,  0.0451, -0.0175]]]]], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m new_params \u001b[38;5;241m=\u001b[39m {k : nn\u001b[38;5;241m.\u001b[39mParameter(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m new_params\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/functional_call.py:143\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/utils/stateless.py:262\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    258\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    260\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    261\u001b[0m ):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 65\u001b[0m, in \u001b[0;36mConvNetNoBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape before layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(layer\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 65\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape after layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]"
     ]
    }
   ],
   "source": [
    "new_params = {k : nn.Parameter(v) for k, v in new_params.items()}\n",
    "\n",
    "output = functional_call(net.to(device), new_params, (x_support,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048fa606-7133-4bd1-a4c1-20a7e7983ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before layer 0: torch.Size([1, 3, 84, 84])\n",
      "Output shape after layer 0: torch.Size([1, 64, 84, 84])\n",
      "Input shape before layer 1: torch.Size([1, 64, 84, 84])\n",
      "Output shape after layer 1: torch.Size([1, 64, 84, 84])\n",
      "Input shape before layer 2: torch.Size([1, 64, 84, 84])\n",
      "Output shape after layer 2: torch.Size([1, 64, 42, 42])\n",
      "Input shape before layer 3: torch.Size([1, 64, 42, 42])\n",
      "Output shape after layer 3: torch.Size([1, 64, 42, 42])\n",
      "Input shape before layer 4: torch.Size([1, 64, 42, 42])\n",
      "Output shape after layer 4: torch.Size([1, 64, 42, 42])\n",
      "Input shape before layer 5: torch.Size([1, 64, 42, 42])\n",
      "Output shape after layer 5: torch.Size([1, 64, 21, 21])\n",
      "Input shape before layer 6: torch.Size([1, 64, 21, 21])\n",
      "Output shape after layer 6: torch.Size([1, 64, 21, 21])\n",
      "Input shape before layer 7: torch.Size([1, 64, 21, 21])\n",
      "Output shape after layer 7: torch.Size([1, 64, 21, 21])\n",
      "Input shape before layer 8: torch.Size([1, 64, 21, 21])\n",
      "Output shape after layer 8: torch.Size([1, 64, 10, 10])\n",
      "Input shape before layer 9: torch.Size([1, 64, 10, 10])\n",
      "Output shape after layer 9: torch.Size([1, 64, 10, 10])\n",
      "Input shape before layer 10: torch.Size([1, 64, 10, 10])\n",
      "Output shape after layer 10: torch.Size([1, 64, 10, 10])\n",
      "Input shape before layer 11: torch.Size([1, 64, 10, 10])\n",
      "Output shape after layer 11: torch.Size([1, 64, 5, 5])\n",
      "Input shape before layer 12: torch.Size([1, 64, 5, 5])\n",
      "Output shape after layer 12: torch.Size([1, 1600])\n",
      "Input shape before layer 13: torch.Size([1, 1600])\n",
      "Output shape after layer 13: torch.Size([1, 5])\n",
      "Jacobian contraction time 0.7979888916015625\n",
      "torch.Size([5, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute J(x1)\n",
    "jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x_support)\n",
    "\n",
    "# print(net.trunk[0].weight)\n",
    "s_jac = {k : s[k]*j for (k, j) in jac1.items()}   # Useful for later\n",
    "ntk_jac1 = [s_j.flatten(2) for s_j in s_jac.values()]   # Useful for the NTK computation\n",
    "    \n",
    "# Compute J(x1) @ J(x2).T\n",
    "ntk = torch.stack([torch.einsum('Naf,Maf->aNM', j1, j2) for j1, j2 in zip(ntk_jac1, ntk_jac1)])\n",
    "ntk = ntk.sum(0)\n",
    "\n",
    "print(f\"Jacobian contraction time {time.time()-start_time}\")\n",
    "print(ntk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541c689-58b4-47a4-b434-a690a7ec36e3",
   "metadata": {},
   "source": [
    "## Computation of $\\text{Sol}_c = (NTK_c + \\epsilon I_k )^{-1} (Y - \\phi_{\\theta, c} (X))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee3c601-78f0-4674-b27e-a09320e27f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.], device='cuda:0')]\n",
      "torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "# Creation of Y\n",
    "\n",
    "target_list = list()\n",
    "samples_per_model = int(len(y_support) / n_way) #25 / 5 = 5\n",
    "for way in range(n_way):\n",
    "    target = torch.ones(len(y_support), dtype=torch.float32) * -1.0\n",
    "    start_index = way * samples_per_model\n",
    "    stop_index = start_index+samples_per_model\n",
    "    target[start_index:stop_index] = 1.0\n",
    "    target_list.append(target.cuda())\n",
    "    \n",
    "print(target_list)\n",
    "print(target_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80eed076-3f47-4706-a475-b6fb0a40dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before layer 0: torch.Size([35, 3, 84, 84])\n",
      "Output shape after layer 0: torch.Size([35, 64, 84, 84])\n",
      "Input shape before layer 1: torch.Size([35, 64, 84, 84])\n",
      "Output shape after layer 1: torch.Size([35, 64, 84, 84])\n",
      "Input shape before layer 2: torch.Size([35, 64, 84, 84])\n",
      "Output shape after layer 2: torch.Size([35, 64, 42, 42])\n",
      "Input shape before layer 3: torch.Size([35, 64, 42, 42])\n",
      "Output shape after layer 3: torch.Size([35, 64, 42, 42])\n",
      "Input shape before layer 4: torch.Size([35, 64, 42, 42])\n",
      "Output shape after layer 4: torch.Size([35, 64, 42, 42])\n",
      "Input shape before layer 5: torch.Size([35, 64, 42, 42])\n",
      "Output shape after layer 5: torch.Size([35, 64, 21, 21])\n",
      "Input shape before layer 6: torch.Size([35, 64, 21, 21])\n",
      "Output shape after layer 6: torch.Size([35, 64, 21, 21])\n",
      "Input shape before layer 7: torch.Size([35, 64, 21, 21])\n",
      "Output shape after layer 7: torch.Size([35, 64, 21, 21])\n",
      "Input shape before layer 8: torch.Size([35, 64, 21, 21])\n",
      "Output shape after layer 8: torch.Size([35, 64, 10, 10])\n",
      "Input shape before layer 9: torch.Size([35, 64, 10, 10])\n",
      "Output shape after layer 9: torch.Size([35, 64, 10, 10])\n",
      "Input shape before layer 10: torch.Size([35, 64, 10, 10])\n",
      "Output shape after layer 10: torch.Size([35, 64, 10, 10])\n",
      "Input shape before layer 11: torch.Size([35, 64, 10, 10])\n",
      "Output shape after layer 11: torch.Size([35, 64, 5, 5])\n",
      "Input shape before layer 12: torch.Size([35, 64, 5, 5])\n",
      "Output shape after layer 12: torch.Size([35, 1600])\n",
      "Input shape before layer 13: torch.Size([35, 1600])\n",
      "Output shape after layer 13: torch.Size([35, 5])\n",
      "torch.Size([35, 5])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "phi = net.forward(x_support)\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf01c62-ee28-4411-a05f-84be98a471dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inversions time 0.09073948860168457\n"
     ]
    }
   ],
   "source": [
    "# Do the actual computation\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sols = []\n",
    "\n",
    "for c in range(n_way):\n",
    "    inverse_term = ntk[c] + eps * torch.eye(n_way * n_support, device=device)\n",
    "    residual = target_list[c] - phi[:, c]  # phi is of shape [n_way*n_support, n_way]\n",
    "\n",
    "    # Solve the system (NTK_c + epsilon I_k) * result = residual\n",
    "    sols.append(torch.linalg.solve(inverse_term, residual))\n",
    "    \n",
    "print(f\"Total inversions time {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea7454-4183-4008-8e84-1fb90707e6ff",
   "metadata": {},
   "source": [
    "## Computation of $\\theta - \\eta_{in} \\sum_{c \\leq C} (s \\cdot \\nabla_\\theta \\phi_c) \\times \\text{Sol}_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b3d758-62f6-4d06-b315-70376442c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: v for k, v in net.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0980c33-b173-4da5-820e-48c6f4802e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tensor([[ 0.0405, -0.0636,  0.0120],\n",
      "        [ 0.0635, -0.0317,  0.0555],\n",
      "        [-0.0851, -0.0781,  0.0151]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0405, -0.0636,  0.0120], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Inner update time time 0.004956483840942383\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# We already have the first term computed as s_jac\n",
    "start_time = time.time()\n",
    "print(type(params))\n",
    "# print(params[\"trunk.0.weight\"])\n",
    "\n",
    "# for c in range(n_way):\n",
    "    # for (k, param) in params.items():\n",
    "    #     params[k] = param - lr_in * torch.tensordot(s_jac[k], sols[c], dims=([0], [0]))\n",
    "\n",
    "tensor_update = {k : sum(torch.tensordot(s_jac[k], sols[c], dims=([0], [0])) for c in range(n_way)) for k in params.keys()}\n",
    "new_params = {k: param - lr_in * tensor_update[k] for k, param in params.items()}\n",
    "    \n",
    "print(net.trunk[0].weight[0][0])\n",
    "print(list(params.values())[0][0][0][0])\n",
    "print(f\"Inner update time time {time.time()-start_time}\")\n",
    "print(type(params))\n",
    "# print(params[\"trunk.0.weight\"])\n",
    "# print(list( net.named_parameters() )[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022293c1-60a7-4b05-b7e2-ba4650057d6d",
   "metadata": {},
   "source": [
    "# Define inner_loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102bb7a4-9cc4-4b71-b97d-ade5602a587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contruct_target_list(N, n_way):  # N = n_support or n_query\n",
    "    target_list = list()\n",
    "    samples_per_model = int(N)\n",
    "    for c in range(n_way):\n",
    "        target = torch.ones(N * n_way, dtype=torch.float32) * -1.0\n",
    "        start_index = c * samples_per_model\n",
    "        stop_index = start_index+samples_per_model\n",
    "        target[start_index:stop_index] = 1.0\n",
    "        target_list.append(target.cuda())\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "def inner_loop(x_support, target_list_support):\n",
    "    \n",
    "    # Create a param dict\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "\n",
    "    for inner_epoch in range(n_inner_upd):\n",
    "        # Forward pass\n",
    "        phi = net.forward(x_support)\n",
    "\n",
    "        # Compute J(x1)\n",
    "        jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x_support)\n",
    "        s_jac = {k : s[k]*j for (k, j) in jac1.items()}   # Useful for later\n",
    "        ntk_jac1 = [s_j.flatten(2) for s_j in s_jac.values()]   # Useful for the NTK computation\n",
    "\n",
    "        # Compute J(x1) @ J(x2).T\n",
    "        ntk = torch.stack([torch.einsum('Naf,Maf->aNM', j1, j2) for j1, j2 in zip(ntk_jac1, ntk_jac1)])\n",
    "        ntk = ntk.sum(0)\n",
    "\n",
    "        # Compute solutions to (NTK_c + eps I)^-1 (Y_c - phi_c)\n",
    "        sols = []\n",
    "        for c in range(n_way):\n",
    "            inverse_term = ntk[c] + eps * torch.eye(n_way * n_support, device=device)\n",
    "            residual = target_list_support[c] - phi[:, c]  # phi is of shape [n_way*n_support, n_way]\n",
    "\n",
    "            # Solve the system (NTK_c + epsilon I_k) * result = residual\n",
    "            sols.append(torch.linalg.solve(inverse_term, residual))\n",
    "\n",
    "            # Update parameters \n",
    "            for (k, param) in params.items():\n",
    "                    params[k] = param - lr_in * torch.tensordot(s_jac[k], sols[c], dims=([0], [0]))\n",
    "                    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea973cef-c024-4d88-a180-2482c91019c8",
   "metadata": {},
   "source": [
    "# OUTER LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb05d51-3c08-41e8-abf1-5cdb5c6e31f3",
   "metadata": {},
   "source": [
    "## Outer tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02e4076-888e-430b-860f-8a7ba6e2f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those two are the same for every task and iteration, we compute them in advance\n",
    "\n",
    "target_list_support = contruct_target_list(n_support, n_way)\n",
    "target_list_query = contruct_target_list(n_query, n_way)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcd2c70a-a61c-447e-825a-ca94a5f36419",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m x_query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(n_way \u001b[38;5;241m*\u001b[39m n_query, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Inner updates\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m inner_params \u001b[38;5;241m=\u001b[39m \u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list_support\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36minner_loop\u001b[0;34m(x_support, target_list_support)\u001b[0m\n\u001b[1;32m     23\u001b[0m phi \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mforward(x_support)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute J(x1)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m jac1 \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfnet_single\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m s_jac \u001b[38;5;241m=\u001b[39m {k : s[k]\u001b[38;5;241m*\u001b[39mj \u001b[38;5;28;01mfor\u001b[39;00m (k, j) \u001b[38;5;129;01min\u001b[39;00m jac1\u001b[38;5;241m.\u001b[39mitems()}   \u001b[38;5;66;03m# Useful for later\u001b[39;00m\n\u001b[1;32m     28\u001b[0m ntk_jac1 \u001b[38;5;241m=\u001b[39m [s_j\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s_j \u001b[38;5;129;01min\u001b[39;00m s_jac\u001b[38;5;241m.\u001b[39mvalues()]   \u001b[38;5;66;03m# Useful for the NTK computation\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:434\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    431\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:619\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 619\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:489\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    488\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 489\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    491\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:291\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    289\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    290\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 291\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mfnet_single\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfnet_single\u001b[39m(params, x):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/functional_call.py:143\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/utils/stateless.py:262\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    258\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    260\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    261\u001b[0m ):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m, in \u001b[0;36mConvNetNoBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# print(self.trunk[0].weight)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]"
     ]
    }
   ],
   "source": [
    "print_freq = 10\n",
    "avg_loss = 0\n",
    "task_count = 0\n",
    "loss_all = []\n",
    "optimizer = torch.optim.Adam([{'params': net.parameters(), 'lr': lr_out},\n",
    "                              {'params': s.values(), 'lr': lr_out}])\n",
    "\n",
    "for i_task in range(n_task):\n",
    "    # New batch corresponding to a task\n",
    "    x_support = torch.randn(n_way * n_support, 3, 84, 84, device=device)\n",
    "    x_query = torch.randn(n_way * n_query, 3, 84, 84, device=device)\n",
    "    \n",
    "    # Inner updates\n",
    "    inner_params = inner_loop(x_support, target_list_support)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29d0f0-4cce-46a9-87d7-b9d0c6939040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf3f7d-8018-4dd7-a750-bfdcc8da0530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
