{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2716065c-80ac-407d-8875-c0aa2e21ec1b",
   "metadata": {},
   "source": [
    "# Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00f5ae6-52d4-475a-8ddf-9639663a22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import math\n",
    "import numpy as np\n",
    "device = 'cuda'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c76c99-85f2-4e78-8de0-ccc0f25b6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine Conv4 here :\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)    \n",
    "    \n",
    "class ConvNetNoBN(nn.Module):\n",
    "    maml = False # Default\n",
    "\n",
    "    def __init__(self, depth, n_way=-1, flatten=True, padding=1):\n",
    "        super(ConvNetNoBN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            indim = 3 if i == 0 else 64\n",
    "            outdim = 64\n",
    "            if self.maml:\n",
    "                conv_layer = Conv2d_fw(indim, outdim, 3, padding=padding)\n",
    "                # BN     = BatchNorm2d_fw(outdim)\n",
    "            else:\n",
    "                conv_layer = nn.Conv2d(indim, outdim, 3, padding=padding, bias=False)\n",
    "                # BN     = nn.BatchNorm2d(outdim)\n",
    "            \n",
    "            relu = nn.ReLU(inplace=True)\n",
    "            layers.append(conv_layer)\n",
    "            # layers.append(BN)\n",
    "            layers.append(relu)\n",
    "\n",
    "            if i < 4:  # Pooling only for the first 4 layers\n",
    "                pool = nn.MaxPool2d(2)\n",
    "                layers.append(pool)\n",
    "\n",
    "            # Initialize the layers\n",
    "            init_layer(conv_layer)\n",
    "            # init_layer(BN)\n",
    "\n",
    "        if flatten:\n",
    "            layers.append(Flatten())\n",
    "        \n",
    "        if n_way>0:\n",
    "            layers.append(nn.Linear(1600,n_way))\n",
    "            self.final_feat_dim = n_way\n",
    "        else:\n",
    "            self.final_feat_dim = 1600\n",
    "            \n",
    "        self.trunk = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.trunk[0].weight)\n",
    "        # for i, layer in enumerate(self.trunk):\n",
    "            # print(f\"Input shape before layer {i}: {x.shape}\")\n",
    "            # print(layer.weight)\n",
    "            # x = layer(x)\n",
    "            # print(f\"Output shape after layer {i}: {x.shape}\")\n",
    "        x = self.trunk(x)\n",
    "        return x\n",
    "\n",
    "def Conv4NoBN():\n",
    "    print(\"Conv4 No Batch Normalization\")\n",
    "    return ConvNetNoBN(4)\n",
    "\n",
    "def Conv4NoBN_class(n_way=5):\n",
    "    print(\"Conv4 No Batch Normalization with final classifier layer of 5 way\")\n",
    "    return ConvNetNoBN(4, n_way=n_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8314e7b-fae3-4688-9aca-a6c0974bb0d5",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91a2f4c-ad3c-4eab-a5d5-4ab0545af83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "\n",
    "n_support = 7\n",
    "n_query = 10\n",
    "\n",
    "n_inner_upd = 3\n",
    "n_task = 4\n",
    "\n",
    "eps = 1e-4\n",
    "lr_in = 1e-2\n",
    "lr_out = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4785d6c-2234-4c5c-8e84-5d77a386f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv4 No Batch Normalization with final classifier layer of 5 way\n"
     ]
    }
   ],
   "source": [
    "x_support = torch.randn(n_way * n_support, 3, 84, 84, device=device)\n",
    "y_support = Variable(torch.from_numpy(np.repeat(range(n_way), n_support)).cuda())\n",
    "x_query = torch.randn(n_way * n_query, 3, 84, 84, device=device)\n",
    "y_query = Variable(torch.from_numpy(np.repeat(range(n_way), n_query)).cuda())\n",
    "\n",
    "\n",
    "net = Conv4NoBN_class().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355360fc-bade-437a-b2a3-4f08745ba26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params dict is equal to the net's params \n",
      "\n",
      "tensor([[-0.0031,  0.0121, -0.0709],\n",
      "        [ 0.0624,  0.0210, -0.0120],\n",
      "        [ 0.0653, -0.0405, -0.0496]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0031,  0.0121, -0.0709],\n",
      "        [ 0.0624,  0.0210, -0.0120],\n",
      "        [ 0.0653, -0.0405, -0.0496]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "params = dict(net.named_parameters())\n",
    "s = {k: torch.ones_like(v) for (k, v) in params.items()}\n",
    "\n",
    "print(\"params dict is equal to the net's params \\n\")\n",
    "print(net.trunk[0].weight[0][0])\n",
    "print(list(params.values())[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c950fc3f-11d5-41da-8840-ad7fab691ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(net.trunk[0].stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60f975d-d3eb-4487-baf1-8d883b598877",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = functional_call(net.to(device), params, (x_support,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e7576-5e8c-4bdb-be5e-7741bc3e8cc4",
   "metadata": {},
   "source": [
    "## INNER LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5231f7-9e3f-4f9d-aacd-215c91f8b965",
   "metadata": {},
   "source": [
    "## NTK computation\n",
    "\n",
    "Note :\n",
    "USE FAST WEIGHT AS THEY DID FOR MAML (nothing is supposed to pose a problem except for batch norm layers as usual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048fa606-7133-4bd1-a4c1-20a7e7983ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian contraction time 0.16933584213256836\n",
      "torch.Size([5, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute J(x1)\n",
    "jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x_support)\n",
    "\n",
    "# print(net.trunk[0].weight)\n",
    "s_jac = {k : s[k]*j for (k, j) in jac1.items()}   # Useful for later\n",
    "ntk_jac1 = [s_j.flatten(2) for s_j in s_jac.values()]   # Useful for the NTK computation\n",
    "    \n",
    "# Compute J(x1) @ J(x2).T\n",
    "ntk = torch.stack([torch.einsum('Naf,Maf->aNM', j1, j2) for j1, j2 in zip(ntk_jac1, ntk_jac1)])\n",
    "ntk = ntk.sum(0)\n",
    "\n",
    "print(f\"Jacobian contraction time {time.time()-start_time}\")\n",
    "print(ntk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541c689-58b4-47a4-b434-a690a7ec36e3",
   "metadata": {},
   "source": [
    "## Computation of $\\text{Sol}_c = (NTK_c + \\epsilon I_k )^{-1} (Y - \\phi_{\\theta, c} (X))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee3c601-78f0-4674-b27e-a09320e27f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.], device='cuda:0')]\n",
      "torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "# Creation of Y\n",
    "\n",
    "target_list = list()\n",
    "samples_per_model = int(len(y_support) / n_way) #25 / 5 = 5\n",
    "for way in range(n_way):\n",
    "    target = torch.ones(len(y_support), dtype=torch.float32) * -1.0\n",
    "    start_index = way * samples_per_model\n",
    "    stop_index = start_index+samples_per_model\n",
    "    target[start_index:stop_index] = 1.0\n",
    "    target_list.append(target.cuda())\n",
    "    \n",
    "print(target_list)\n",
    "print(target_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80eed076-3f47-4706-a475-b6fb0a40dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 5])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "phi = functional_call(net, params, (x_support,))\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf01c62-ee28-4411-a05f-84be98a471dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inversions time 0.13020730018615723\n"
     ]
    }
   ],
   "source": [
    "# Do the actual computation\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sols = []\n",
    "\n",
    "for c in range(n_way):\n",
    "    inverse_term = ntk[c] + eps * torch.eye(n_way * n_support, device=device)\n",
    "    residual = target_list[c] - phi[:, c]  # phi is of shape [n_way*n_support, n_way]\n",
    "\n",
    "    # Solve the system (NTK_c + epsilon I_k) * result = residual\n",
    "    sols.append(torch.linalg.solve(inverse_term, residual))\n",
    "    \n",
    "print(f\"Total inversions time {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea7454-4183-4008-8e84-1fb90707e6ff",
   "metadata": {},
   "source": [
    "## Computation of $\\theta - \\eta_{in} \\sum_{c \\leq C} (s \\cdot \\nabla_\\theta \\phi_c) \\times \\text{Sol}_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b3d758-62f6-4d06-b315-70376442c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: v for k, v in net.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0980c33-b173-4da5-820e-48c6f4802e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "torch.Size([35])\n",
      "torch.Size([35, 64, 3, 3, 3])\n",
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([64, 3, 3, 3])\n",
      "tensor([[-0.0031,  0.0121, -0.0709],\n",
      "        [ 0.0624,  0.0210, -0.0120],\n",
      "        [ 0.0653, -0.0405, -0.0496]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([64, 3, 3, 3])\n",
      "Inner update time time 0.004975795745849609\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# We already have the first term computed as s_jac\n",
    "start_time = time.time()\n",
    "print(type(params))\n",
    "# print(params[\"trunk.0.weight\"])\n",
    "\n",
    "# for c in range(n_way):\n",
    "    # for (k, param) in params.items():\n",
    "    #     params[k] = param - lr_in * torch.tensordot(s_jac[k], sols[c], dims=([0], [0]))\n",
    "\n",
    "    \n",
    "print(sols[0].shape)\n",
    "print(s_jac[\"trunk.0.weight\"][:, 0].shape)\n",
    "print(torch.tensordot(s_jac[\"trunk.0.weight\"][:,0], sols[0], dims=([0], [0])).shape)\n",
    "print(sum(torch.tensordot(s_jac[\"trunk.0.weight\"][:,c], sols[c], dims=([0], [0])) for c in range(n_way)).shape)\n",
    "\n",
    "tensor_update = {k : s[k] * sum(torch.tensordot(s_jac[k][:,c], sols[c], dims=([0], [0])) for c in range(n_way)) for k in params.keys()}\n",
    "params = {k: param + lr_in * tensor_update[k] for k, param in params.items()}\n",
    "    \n",
    "print(net.trunk[0].weight[0][0])\n",
    "# print(list(new_params.values())[0].shape)\n",
    "print(list(params.values())[0].shape)\n",
    "print(f\"Inner update time time {time.time()-start_time}\")\n",
    "print(type(params))\n",
    "# print(params[\"trunk.0.weight\"])\n",
    "# print(list( net.named_parameters() )[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022293c1-60a7-4b05-b7e2-ba4650057d6d",
   "metadata": {},
   "source": [
    "# Define inner_loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "102bb7a4-9cc4-4b71-b97d-ade5602a587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contruct_target_list(N, n_way):  # N = n_support or n_query\n",
    "    target_list = list()\n",
    "    samples_per_model = int(N)\n",
    "    for c in range(n_way):\n",
    "        target = torch.ones(N * n_way, dtype=torch.float32) * -1.0\n",
    "        start_index = c * samples_per_model\n",
    "        stop_index = start_index+samples_per_model\n",
    "        target[start_index:stop_index] = 1.0\n",
    "        target_list.append(target.cuda())\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "def inner_loop(x_support, target_list_support):\n",
    "    \n",
    "    # Create a param dict\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "\n",
    "    for inner_epoch in range(n_inner_upd):\n",
    "        # Forward pass\n",
    "        phi = functional_call(net, params, (x_support,))\n",
    "\n",
    "        # Compute J(x1)\n",
    "        jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x_support)\n",
    "        s_jac = {k : s[k]*j for (k, j) in jac1.items()}   # Useful for later\n",
    "        ntk_jac1 = [s_j.flatten(2) for s_j in s_jac.values()]   # Useful for the NTK computation\n",
    "\n",
    "        # Compute J(x1) @ J(x2).T\n",
    "        ntk = torch.stack([torch.einsum('Naf,Maf->aNM', j1, j2) for j1, j2 in zip(ntk_jac1, ntk_jac1)])\n",
    "        ntk = ntk.sum(0)\n",
    "\n",
    "        # Compute solutions to (NTK_c + eps I)^-1 (Y_c - phi_c)\n",
    "        sols = []\n",
    "        for c in range(n_way):\n",
    "            inverse_term = ntk[c] + eps * torch.eye(n_way * n_support, device=device)\n",
    "            residual = target_list_support[c] - phi[:, c]  # phi is of shape [n_way*n_support, n_way]\n",
    "\n",
    "            # Solve the system (NTK_c + epsilon I_k) * result = residual\n",
    "            sols.append(torch.linalg.solve(inverse_term, residual))\n",
    "\n",
    "        # Update parameters \n",
    "        tensor_update = {k : s[k] * sum(torch.tensordot(s_jac[k][:,c], sols[c], dims=([0], [0])) for c in range(n_way)) for k in params.keys()}\n",
    "        params = {k: param + lr_in * tensor_update[k] for k, param in params.items()}\n",
    "                    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea973cef-c024-4d88-a180-2482c91019c8",
   "metadata": {},
   "source": [
    "# OUTER LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb05d51-3c08-41e8-abf1-5cdb5c6e31f3",
   "metadata": {},
   "source": [
    "## Outer tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02e4076-888e-430b-860f-8a7ba6e2f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those two are the same for every task and iteration, we compute them in advance\n",
    "\n",
    "target_list_support = contruct_target_list(n_support, n_way)\n",
    "target_list_query = contruct_target_list(n_query, n_way)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcd2c70a-a61c-447e-825a-ca94a5f36419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for one iter over 4 tasks : 0.9371540546417236\n"
     ]
    }
   ],
   "source": [
    "print_freq = 10\n",
    "avg_loss = 0\n",
    "task_count = 0\n",
    "loss_all = []\n",
    "optimizer = torch.optim.Adam([{'params': net.parameters(), 'lr': lr_out},\n",
    "                              {'params': s.values(), 'lr': lr_out}])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_task in range(n_task):\n",
    "    # New batch corresponding to a task\n",
    "    x_support = torch.randn(n_way * n_support, 3, 84, 84, device=device)\n",
    "    x_query = torch.randn(n_way * n_query, 3, 84, 84, device=device)\n",
    "    \n",
    "    # Inner updates\n",
    "    inner_params = inner_loop(x_support, target_list_support)\n",
    "    \n",
    "    # outer optimization\n",
    "    scores = functional_call(net, inner_params, (x_query,))\n",
    "    loss = loss_fn( scores, y_query )\n",
    "    loss_all.append(loss)\n",
    "    \n",
    "loss_q = torch.stack(loss_all).sum(0)\n",
    "loss_q.backward()\n",
    "optimizer.step\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(f\"Total time for one iter over 4 tasks : {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bd02e-7a6c-4d91-be23-98d43798dfee",
   "metadata": {},
   "source": [
    "# TEST LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfcde6-e852-4cbf-b0c6-db61c8c16928",
   "metadata": {},
   "source": [
    "## Test parameters\n",
    "\n",
    "The test loop is directly copied from differentialDKTIX\n",
    "\n",
    "/!\\ We will not redifine n_support, n_query or the optimizer lr.\n",
    "These quantities will not be the same as for the training procedure, but will be redefine in the test script. This works because we reconstruct a new neural net in the meta-test script, on which we paste the new values of the above hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b597f-c3b0-40c9-92d6-afdc06beb1f7",
   "metadata": {},
   "source": [
    "# Choleski safe decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bea27ef2-47a5-4387-ac58-ff491ab8e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd_safe_cholesky(A, upper=False, out=None, jitter=None):\n",
    "    \"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\n",
    "    Args:\n",
    "        :attr:`A` (Tensor):\n",
    "            The tensor to compute the Cholesky decomposition of\n",
    "        :attr:`upper` (bool, optional):\n",
    "            See torch.cholesky\n",
    "        :attr:`out` (Tensor, optional):\n",
    "            See torch.cholesky\n",
    "        :attr:`jitter` (float, optional):\n",
    "            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted, chosen\n",
    "            as 1e-6 (float) or 1e-8 (double)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if A.dim() == 2:\n",
    "            L = torch.linalg.cholesky(A, upper=upper, out=out)\n",
    "            return L\n",
    "        else:\n",
    "            L_list = []\n",
    "            for idx in range(A.shape[0]):\n",
    "                L = torch.linalg.cholesky(A[idx], upper=upper, out=out)\n",
    "                L_list.append(L)\n",
    "            return torch.stack(L_list, dim=0)\n",
    "    except:\n",
    "        isnan = torch.isnan(A)\n",
    "        if isnan.any():\n",
    "            raise NanError(\n",
    "                f\"cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN.\"\n",
    "            )\n",
    "\n",
    "        if jitter is None:\n",
    "            jitter = 1e-6 if A.dtype == torch.float32 else 1e-8\n",
    "        Aprime = A.clone()\n",
    "        jitter_prev = 0\n",
    "        for i in range(8):\n",
    "            jitter_new = jitter * (10 ** i)\n",
    "            Aprime.diagonal(dim1=-2, dim2=-1).add_(jitter_new - jitter_prev)\n",
    "            jitter_prev = jitter_new\n",
    "            try:\n",
    "                if Aprime.dim() == 2:\n",
    "                    L = torch.linalg.cholesky(Aprime, upper=upper, out=out)\n",
    "                    warnings.warn(\n",
    "                        f\"A not p.d., added jitter of {jitter_new} to the diagonal\",\n",
    "                        RuntimeWarning,\n",
    "                    )\n",
    "                    return L\n",
    "                else:\n",
    "                    L_list = []\n",
    "                    for idx in range(Aprime.shape[0]):\n",
    "                        L = torch.linalg.cholesky(Aprime[idx], upper=upper, out=out)\n",
    "                        L_list.append(L)\n",
    "                    warnings.warn(\n",
    "                        f\"A not p.d., added jitter of {jitter_new} to the diagonal\",\n",
    "                        RuntimeWarning,\n",
    "                    )\n",
    "                    return torch.stack(L_list, dim=0)\n",
    "            except:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a23dd2c6-d525-4619-beba-f5367a32f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 2])\n",
      "tensor([[2.0000, 0.0000],\n",
      "        [1.0000, 1.4142]])\n",
      "Solution Y: tensor([-0.1250,  0.7500])\n"
     ]
    }
   ],
   "source": [
    "def solve_using_cholesky(A, X):\n",
    "    # Step 1: Perform Cholesky decomposition to get L\n",
    "    L = psd_safe_cholesky(A)\n",
    "    \n",
    "    print(type(L))\n",
    "    print(L.shape)\n",
    "    print(L)\n",
    "    \n",
    "    X = X.unsqueeze(1)\n",
    "    \n",
    "    # Step 2: Solve L * Z = X for Z using forward substitution\n",
    "    Z = torch.linalg.solve_triangular(L, X, upper=False)  # 'upper=False' indicates L is lower triangular\n",
    "    \n",
    "    # Step 3: Solve L^T * Y = Z for Y using backward substitution\n",
    "    Y = torch.linalg.solve_triangular(L.T, Z, upper=True)  # 'upper=True' indicates L^T is upper triangular\n",
    "\n",
    "    return Y.squeeze()\n",
    "\n",
    "# Example usage\n",
    "A = torch.tensor([[4.0, 2.0], [2.0, 3.0]], dtype=torch.float32)\n",
    "X = torch.tensor([1.0, 2.0], dtype=torch.float32)\n",
    "\n",
    "# Solve A * Y = X using Cholesky decomposition\n",
    "Y = solve_using_cholesky(A, X)\n",
    "\n",
    "print(\"Solution Y:\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "369058a4-0cd9-4588-9633-2c89e1a60c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution Y: tensor([-0.1250,  0.7500])\n"
     ]
    }
   ],
   "source": [
    "# Solve AY = X using torch.linalg.solve\n",
    "Y = torch.linalg.solve(A, X)\n",
    "\n",
    "print(\"Solution Y:\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a95bcc42-fbd2-4f66-8058-95ce1e9e7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 3])\n",
      "tensor([[ 4.8990,  0.0000,  0.0000],\n",
      "        [ 3.2660,  1.8257,  0.0000],\n",
      "        [ 2.8577, -0.1826,  0.8944]])\n",
      "Solution Cholesky Y: tensor([-2.2969,  0.7187,  3.1875])\n",
      "Solution torch.linalg.solve Y: tensor([-2.2969,  0.7188,  3.1875])\n"
     ]
    }
   ],
   "source": [
    "# Define a 3x3 positive semi-definite matrix A\n",
    "A = torch.tensor([[4.0, 2.0, 2.0], \n",
    "                  [2.0, 3.0, 1.0], \n",
    "                  [2.0, 1.0, 2.0]], dtype=torch.float32)\n",
    "\n",
    "# Ensure A is symmetric and positive semi-definite\n",
    "A = A @ A.T  # This operation makes A symmetric and PSD\n",
    "\n",
    "# Define a 3-dimensional vector X\n",
    "X = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "\n",
    "# Solve A * Y = X using Cholesky decomposition\n",
    "Y = solve_using_cholesky(A, X)\n",
    "print(\"Solution Cholesky Y:\", Y)\n",
    "\n",
    "# Solve AY = X using torch.linalg.solve\n",
    "Y = torch.linalg.solve(A, X)\n",
    "print(\"Solution torch.linalg.solve Y:\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7214b9d0-b86e-41a0-ba90-ea105ee8118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([7, 7])\n",
      "tensor([[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 4.9167,  3.8828,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 3.7500,  2.7203,  5.0781,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 7.8333, -0.3899,  0.0366,  3.9352,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.6667, -0.9729,  2.9827,  1.8524,  5.4112,  0.0000,  0.0000],\n",
      "        [ 4.5000, -0.8048,  1.8342,  2.3809,  1.0785,  8.0098,  0.0000],\n",
      "        [ 3.9167,  0.7065, -0.1200, -0.2289,  0.8054,  3.3509,  8.9703]])\n",
      "Solution Cholesky Y: tensor([-0.3612,  0.0845,  0.0217,  0.1474,  0.0540,  0.0192,  0.0603])\n",
      "Solution linalg.solve Y: tensor([-0.3612,  0.0845,  0.0217,  0.1474,  0.0540,  0.0192,  0.0603])\n"
     ]
    }
   ],
   "source": [
    "# Define a 7x7 positive semi-definite matrix A\n",
    "A = torch.tensor([\n",
    "    [4.0, 2.0, 1.0, 3.0, 2.0, 1.0, 1.0], \n",
    "    [2.0, 5.0, 2.0, 2.0, 1.0, 0.5, 1.0], \n",
    "    [1.0, 2.0, 6.0, 1.0, 2.0, 1.0, 0.5], \n",
    "    [3.0, 2.0, 1.0, 7.0, 3.0, 2.0, 1.0], \n",
    "    [2.0, 1.0, 2.0, 3.0, 8.0, 2.0, 1.0], \n",
    "    [1.0, 0.5, 1.0, 2.0, 2.0, 9.0, 2.0],\n",
    "    [1.0, 1.0, 0.5, 1.0, 1.0, 2.0, 10.0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Ensure A is symmetric and positive semi-definite\n",
    "A = A @ A.T  # This operation makes A symmetric and PSD\n",
    "\n",
    "# Define a 7-dimensional vector X\n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0], dtype=torch.float32)\n",
    "\n",
    "# Solve A * Y = X using Cholesky decomposition\n",
    "Y = solve_using_cholesky(A, X)\n",
    "print(\"Solution Cholesky Y:\", Y)\n",
    "\n",
    "# Solve AY = X using torch.linalg.solve\n",
    "Y = torch.linalg.solve(A, X)\n",
    "print(\"Solution linalg.solve Y:\", Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
