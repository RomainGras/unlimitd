{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2716065c-80ac-407d-8875-c0aa2e21ec1b",
   "metadata": {},
   "source": [
    "# Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00f5ae6-52d4-475a-8ddf-9639663a22ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5c76c99-85f2-4e78-8de0-ccc0f25b6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine Conv4 here :\n",
    "\n",
    "def init_layer(L):\n",
    "    # Initialization using fan-in\n",
    "    if isinstance(L, nn.Conv2d):\n",
    "        n = L.kernel_size[0]*L.kernel_size[1]*L.out_channels\n",
    "        L.weight.data.normal_(0,math.sqrt(2.0/float(n)))\n",
    "    elif isinstance(L, nn.BatchNorm2d):\n",
    "        L.weight.data.fill_(1)\n",
    "        L.bias.data.fill_(0)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)    \n",
    "    \n",
    "class Conv2d_fw(nn.Conv2d): #used in MAML to forward input with fast weight\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,padding=0, bias = True):\n",
    "        super(Conv2d_fw, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.weight.fast = None\n",
    "        if not self.bias is None:\n",
    "            self.bias.fast = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.bias is None:\n",
    "            if self.weight.fast is not None:\n",
    "                out = F.conv2d(x, self.weight.fast, None, stride= self.stride, padding=self.padding)\n",
    "            else:\n",
    "                out = super(Conv2d_fw, self).forward(x)\n",
    "        else:\n",
    "            print(self.weight)\n",
    "            if self.weight.fast is not None and self.bias.fast is not None:\n",
    "                out = F.conv2d(x, self.weight.fast, self.bias.fast, stride= self.stride, padding=self.padding)\n",
    "            else:\n",
    "                out = super(Conv2d_fw, self).forward(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Linear_fw(nn.Linear): #used in MAML to forward input with fast weight\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear_fw, self).__init__(in_features, out_features)\n",
    "        self.weight.fast = None #Lazy hack to add fast weight link\n",
    "        self.bias.fast = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.weight.fast is not None and self.bias.fast is not None:\n",
    "            out = F.linear(x, self.weight.fast, self.bias.fast) #weight.fast (fast weight) is the temporaily adapted weight\n",
    "        else:\n",
    "            out = super(Linear_fw, self).forward(x)\n",
    "        return out\n",
    "    \n",
    "class ConvNetNoBN(nn.Module):\n",
    "    maml = True # Default\n",
    "\n",
    "    def __init__(self, depth, n_way=-1, flatten=True, padding=1):\n",
    "        super(ConvNetNoBN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            indim = 3 if i == 0 else 64\n",
    "            outdim = 64\n",
    "            if self.maml:\n",
    "                conv_layer = Conv2d_fw(indim, outdim, 3, padding=padding)\n",
    "                # BN     = BatchNorm2d_fw(outdim)\n",
    "            else:\n",
    "                conv_layer = nn.Conv2d(indim, outdim, 3, stride=1, padding=padding, bias=False)\n",
    "                # BN     = nn.BatchNorm2d(outdim)\n",
    "            \n",
    "            relu = nn.ReLU(inplace=True)\n",
    "            layers.append(conv_layer)\n",
    "            # layers.append(BN)\n",
    "            layers.append(relu)\n",
    "\n",
    "            if i < 4:  # Pooling only for the first 4 layers\n",
    "                pool = nn.MaxPool2d(2)\n",
    "                layers.append(pool)\n",
    "\n",
    "            # Initialize the layers\n",
    "            init_layer(conv_layer)\n",
    "            # init_layer(BN)\n",
    "\n",
    "        if flatten:\n",
    "            layers.append(Flatten())\n",
    "        \n",
    "        if n_way>0:\n",
    "            if self.maml:\n",
    "                classifier = Linear_fw(1600, n_way)\n",
    "                classifier.bias.data.fill_(0)\n",
    "                layers.append(classifier)\n",
    "            else:\n",
    "                classifier = nn.linear(1600, n_way)\n",
    "                layers.append(classifier)\n",
    "            self.final_feat_dim = n_way\n",
    "        else:\n",
    "            self.final_feat_dim = 1600\n",
    "          \n",
    "        self.trunk = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.trunk(x)\n",
    "        return out\n",
    "\n",
    "def Conv4NoBN():\n",
    "    print(\"Conv4 No Batch Normalization\")\n",
    "    return ConvNetNoBN(4)\n",
    "\n",
    "def Conv4NoBN_class(n_way=5):\n",
    "    print(\"Conv4 No Batch Normalization with final classifier layer of 5 way\")\n",
    "    return ConvNetNoBN(4, n_way=n_way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8314e7b-fae3-4688-9aca-a6c0974bb0d5",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f91a2f4c-ad3c-4eab-a5d5-4ab0545af83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "\n",
    "n_support = 7\n",
    "n_query = 10\n",
    "\n",
    "eps = 1e-4\n",
    "lr_in = 1e-2\n",
    "lr_out = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4785d6c-2234-4c5c-8e84-5d77a386f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv4 No Batch Normalization with final classifier layer of 5 way\n"
     ]
    }
   ],
   "source": [
    "x_support = torch.randn(n_way * n_support, 3, 84, 84, device=device)\n",
    "y_support = Variable(torch.from_numpy(np.repeat(range(n_way), n_support)).cuda())\n",
    "x_query = torch.randn(n_way * n_query, 3, 84, 84, device=device)\n",
    "y_query = Variable(torch.from_numpy(np.repeat(range(n_way), n_query)).cuda())\n",
    "\n",
    "\n",
    "net = Conv4NoBN_class().to(device)\n",
    "\n",
    "# def net_forward(x):\n",
    "#     out  = net.forward(x)\n",
    "#     scores  = classifier.forward(out)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "355360fc-bade-437a-b2a3-4f08745ba26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: v for k, v in net.named_parameters()}\n",
    "s = {k: torch.ones_like(v) for (k, v) in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "035dc271-7a83-486b-b03e-8a7ca54e6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fast parameters\n",
    "\n",
    "fast_parameters = list(net.parameters()) #the first gradient calcuated in line 45 is based on original weight\n",
    "fast_params_dict = {k: v for k, v in net.named_parameters()}\n",
    "for weight in net.parameters():\n",
    "    weight.fast = None\n",
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e7576-5e8c-4bdb-be5e-7741bc3e8cc4",
   "metadata": {},
   "source": [
    "## INNER LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5231f7-9e3f-4f9d-aacd-215c91f8b965",
   "metadata": {},
   "source": [
    "## NTK computation\n",
    "\n",
    "Note :\n",
    "USE FAST WEIGHT AS THEY DID FOR MAML (nothing is supposed to pose a problem except for batch norm layers as usual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "048fa606-7133-4bd1-a4c1-20a7e7983ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradTrackingTensor(lvl=2, value=\n",
      "    tensor([[[[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "            [[[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "            [[[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "            ...,\n",
      "\n",
      "\n",
      "            [[[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "            [[[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "            [[[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]],\n",
      "\n",
      "             [[1., 1., 1.],\n",
      "              [1., 1., 1.],\n",
      "              [1., 1., 1.]]]], device='cuda:0')\n",
      ")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'fast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Compute J(x1)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m jac1 \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfnet_single\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m s_jac \u001b[38;5;241m=\u001b[39m {k : s[k]\u001b[38;5;241m*\u001b[39mj \u001b[38;5;28;01mfor\u001b[39;00m (k, j) \u001b[38;5;129;01min\u001b[39;00m jac1\u001b[38;5;241m.\u001b[39mitems()}   \u001b[38;5;66;03m# Useful for later\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ntk_jac1 \u001b[38;5;241m=\u001b[39m [s_j\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s_j \u001b[38;5;129;01min\u001b[39;00m s_jac\u001b[38;5;241m.\u001b[39mvalues()]   \u001b[38;5;66;03m# Useful for the NTK computation\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:434\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    431\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:619\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 619\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:489\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    488\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 489\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    491\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:291\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    289\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    290\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 291\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m, in \u001b[0;36mfnet_single\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfnet_single\u001b[39m(params, x):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_functorch/functional_call.py:143\u001b[0m, in \u001b[0;36mfunctional_call\u001b[0;34m(module, parameter_and_buffer_dicts, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter_and_buffer_dicts to be a dict, or a list/tuple of dicts, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(parameter_and_buffer_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters_and_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtie_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtie_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/utils/stateless.py:262\u001b[0m, in \u001b[0;36m_functional_call\u001b[0;34m(module, parameters_and_buffers, args, kwargs, tie_weights, strict)\u001b[0m\n\u001b[1;32m    258\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _reparametrize_module(\n\u001b[1;32m    260\u001b[0m     module, parameters_and_buffers, tie_weights\u001b[38;5;241m=\u001b[39mtie_weights, strict\u001b[38;5;241m=\u001b[39mstrict\n\u001b[1;32m    261\u001b[0m ):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[70], line 102\u001b[0m, in \u001b[0;36mConvNetNoBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 102\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[70], line 34\u001b[0m, in \u001b[0;36mConv2d_fw.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mfast \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfast, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mfast, stride\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'fast'"
     ]
    }
   ],
   "source": [
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute J(x1)\n",
    "jac1 = vmap(jacrev(fnet_single), (None, 0))(s, x_support)\n",
    "s_jac = {k : s[k]*j for (k, j) in jac1.items()}   # Useful for later\n",
    "ntk_jac1 = [s_j.flatten(2) for s_j in s_jac.values()]   # Useful for the NTK computation\n",
    "    \n",
    "# Compute J(x1) @ J(x2).T\n",
    "ntk = torch.stack([torch.einsum('Naf,Maf->aNM', j1, j2) for j1, j2 in zip(ntk_jac1, ntk_jac1)])\n",
    "ntk = ntk.sum(0)\n",
    "\n",
    "print(f\"Jacobian contraction time {time.time()-start_time}\")\n",
    "print(ntk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541c689-58b4-47a4-b434-a690a7ec36e3",
   "metadata": {},
   "source": [
    "## Computation of $\\text{Sol}_c = (NTK_c + \\epsilon I_k )^{-1} (Y - \\phi_{\\theta, c} (X))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee3c601-78f0-4674-b27e-a09320e27f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.], device='cuda:0'), tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.], device='cuda:0')]\n",
      "torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "# Creation of Y\n",
    "\n",
    "target_list = list()\n",
    "samples_per_model = int(len(y_support) / n_way) #25 / 5 = 5\n",
    "for way in range(n_way):\n",
    "    target = torch.ones(len(y_support), dtype=torch.float32) * -1.0\n",
    "    start_index = way * samples_per_model\n",
    "    stop_index = start_index+samples_per_model\n",
    "    target[start_index:stop_index] = 1.0\n",
    "    target_list.append(target.cuda())\n",
    "    \n",
    "print(target_list)\n",
    "print(target_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80eed076-3f47-4706-a475-b6fb0a40dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 5])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "phi = net.forward(x_support)\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf01c62-ee28-4411-a05f-84be98a471dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inversions time 0.08805727958679199\n"
     ]
    }
   ],
   "source": [
    "# Do the actual computation\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sols = []\n",
    "\n",
    "for c in range(n_way):\n",
    "    inverse_term = ntk[c] + eps * torch.eye(n_way * n_support, device=device)\n",
    "    residual = target_list[c] - phi[:, c]  # phi is of shape [n_way*n_support, n_way]\n",
    "\n",
    "    # Solve the system (NTK_c + epsilon I_k) * result = residual\n",
    "    sols.append(torch.linalg.solve(inverse_term, residual))\n",
    "    \n",
    "print(f\"Total inversions time {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea7454-4183-4008-8e84-1fb90707e6ff",
   "metadata": {},
   "source": [
    "## Computation of $\\theta - \\eta_{in} \\sum_{c \\leq C} (s \\cdot \\nabla_\\theta \\phi_c) \\times \\text{Sol}_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0980c33-b173-4da5-820e-48c6f4802e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner update time time 0.002782583236694336\n"
     ]
    }
   ],
   "source": [
    "# We already have the first term computed as s_jac\n",
    "start_time = time.time()\n",
    "\n",
    "for c in range(n_way):\n",
    "    for (k, param) in net.named_parameters():\n",
    "        param.data = param - lr_in * torch.tensordot(s_jac[k], sols[c], dims=([0], [0]))\n",
    "\n",
    "print(f\"Inner update time time {time.time()-start_time}\")\n",
    "# print(list( net.named_parameters() )[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022293c1-60a7-4b05-b7e2-ba4650057d6d",
   "metadata": {},
   "source": [
    "# Define inner_loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102bb7a4-9cc4-4b71-b97d-ade5602a587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contruct_target_list(N, n_way):  # N = n_support or n_query\n",
    "    target_list = list()\n",
    "    samples_per_model = int(N)\n",
    "    for c in range(n_way):\n",
    "        target = torch.ones(N * n_way, dtype=torch.float32) * -1.0\n",
    "        start_index = c * samples_per_model\n",
    "        stop_index = start_index+samples_per_model\n",
    "        target[start_index:stop_index] = 1.0\n",
    "        target_list.append(target.cuda())\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "def inner_loop(x_support, target_list_support):\n",
    "    # Forward pass\n",
    "    phi = net.forward(x_support)\n",
    "    \n",
    "    # Create a param dict\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "    \n",
    "    # Compute J(x1)\n",
    "    jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x_support)\n",
    "    s_jac = {k : s[k]*j for (k, j) in jac1.items()}   # Useful for later\n",
    "    ntk_jac1 = [s_j.flatten(2) for s_j in s_jac.values()]   # Useful for the NTK computation\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    ntk = torch.stack([torch.einsum('Naf,Maf->aNM', j1, j2) for j1, j2 in zip(ntk_jac1, ntk_jac1)])\n",
    "    ntk = ntk.sum(0)\n",
    "    \n",
    "    # Compute solutions to (NTK_c + eps I)^-1 (Y_c - phi_c)\n",
    "    sols = []\n",
    "    for c in range(n_way):\n",
    "        inverse_term = ntk[c] + eps * torch.eye(n_way * n_support, device=device)\n",
    "        residual = target_list_support[c] - phi[:, c]  # phi is of shape [n_way*n_support, n_way]\n",
    "\n",
    "        # Solve the system (NTK_c + epsilon I_k) * result = residual\n",
    "        sols.append(torch.linalg.solve(inverse_term, residual))\n",
    "        \n",
    "        # Update parameters \n",
    "        for (k, param) in net.named_parameters():\n",
    "            param.data = params[k] - lr_in * torch.tensordot(s_jac[k], sols[c], dims=([0], [0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea973cef-c024-4d88-a180-2482c91019c8",
   "metadata": {},
   "source": [
    "# OUTER LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb05d51-3c08-41e8-abf1-5cdb5c6e31f3",
   "metadata": {},
   "source": [
    "## Outer tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7ef7ce-f505-4375-8647-2e1ef4e19766",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inner_upd = 3\n",
    "n_task = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02e4076-888e-430b-860f-8a7ba6e2f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those two are the same for every task and iteration, we compute them in advance\n",
    "\n",
    "target_list_support = contruct_target_list(n_support, n_way)\n",
    "target_list_query = contruct_target_list(n_query, n_way)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd2c70a-a61c-447e-825a-ca94a5f36419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_inner_upd \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_inner_upd):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i_inner_upd)\n\u001b[0;32m---> 16\u001b[0m     \u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_support\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list_support\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36minner_loop\u001b[0;34m(x_support, target_list_support)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_loop\u001b[39m(x_support, target_list_support):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     phi \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Create a param dict\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mnamed_parameters()}\n",
      "Cell \u001b[0;32mIn[2], line 61\u001b[0m, in \u001b[0;36mConvNetNoBN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 61\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected stride to be a single integer value or a list of 3 values to match the convolution dimensions, but got stride=[1, 1]"
     ]
    }
   ],
   "source": [
    "print_freq = 10\n",
    "avg_loss = 0\n",
    "task_count = 0\n",
    "loss_all = []\n",
    "optimizer = torch.optim.Adam([{'params': net.parameters(), 'lr': lr_out},\n",
    "                              {'params': s.values(), 'lr': lr_out}])\n",
    "\n",
    "for i_task in range(n_task):\n",
    "    # New batch corresponding to a task\n",
    "    x_support = torch.randn(n_way * n_support, 3, 84, 84, device=device)\n",
    "    x_query = torch.randn(n_way * n_query, 3, 84, 84, device=device)\n",
    "    \n",
    "    # Inner updates\n",
    "    for i_inner_upd in range(n_inner_upd):\n",
    "        print(i_inner_upd)\n",
    "        inner_loop(x_support, target_list_support)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29d0f0-4cce-46a9-87d7-b9d0c6939040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
