{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181e498f-35b4-448a-82e9-b45b43fc95ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] install tensorboardX to record simulation logs.\n",
      "Conv4S\n",
      "Conv6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import json\n",
    "import torch.utils.data.sampler\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import configs\n",
    "import backbone\n",
    "import data.feature_loader as feat_loader\n",
    "from data.datamgr import SetDataManager\n",
    "from methods.maml import MAML\n",
    "from methods.differentialDKTIXnogpytorch import differentialDKTIXnogpy\n",
    "from io_utils import model_dict, get_resume_file, parse_args, get_best_file , get_assigned_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca65902-81c2-4be2-8476-9150bda7698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_seed(seed, verbose=True):\n",
    "    if(seed!=0):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False \n",
    "        if(verbose): print(\"[INFO] Setting SEED: \" + str(seed))   \n",
    "    else:\n",
    "        if(verbose): print(\"[INFO] Setting SEED: None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b28fc59-1cdd-4111-8087-80fda4f2d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_state_dict(old_state_dict):\n",
    "    new_state_dict = {}\n",
    "    for old_key, value in old_state_dict.items():\n",
    "        # Remap keys to match the new model structure\n",
    "        if old_key.startswith(\"feature.trunk\"):\n",
    "            # Convert keys like \"feature.trunk.0.trunk.0.weight\" to \"feature_extractor.0.trunk.0.C.weight\"\n",
    "            parts = old_key.split(\".\")\n",
    "            layer_idx = parts[2]  # Extract layer index\n",
    "            layer_part = parts[3:]  # Rest of the parts\n",
    "            if \"trunk\" in layer_part:\n",
    "                layer_part.remove(\"trunk\")  # Remove extra \"trunk\" if present\n",
    "            new_key = f\"feature_extractor.{layer_idx}.trunk.0.\" + \".\".join(layer_part)\n",
    "            new_state_dict[new_key] = value\n",
    "        elif old_key.startswith(\"feature.trunk\") and \"num_batches_tracked\" not in old_key:\n",
    "            # Handle BatchNorm layers without extra \"trunk\"\n",
    "            parts = old_key.split(\".\")\n",
    "            layer_idx = parts[2]\n",
    "            layer_part = parts[3:]\n",
    "            new_key = f\"feature_extractor.{layer_idx}.trunk.0.\" + \".\".join(layer_part)\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            # Keep other keys unchanged\n",
    "            new_state_dict[old_key] = value\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cf2b4c-3c30-4c80-a304-f418087d1e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting SEED: 1\n",
      "n_query : 16\n",
      "Conv4_maml_diffDKTIX\n",
      "Normalization : False\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "    \n",
    "seed = 1\n",
    "_set_seed(seed)\n",
    "\n",
    "\n",
    "# First define loaders\n",
    "n_query = max(1, int(16 * 5 / 5))  # if test_n_way is smaller than train_n_way, reduce n_query to keep batch size small\n",
    "print(f\"n_query : {n_query}\")\n",
    "\n",
    "# Dataloader\n",
    "image_size = 84\n",
    "\n",
    "base_file = configs.data_dir['CUB'] + 'base.json'\n",
    "val_file = configs.data_dir['CUB'] + 'val.json'\n",
    "\n",
    "train_few_shot_params = dict(n_way=5, n_support=1)\n",
    "base_datamgr = SetDataManager(image_size, n_query=n_query, **train_few_shot_params) #n_eposide=100\n",
    "base_loader = base_datamgr.get_data_loader(base_file, aug=True)\n",
    "\n",
    "test_few_shot_params = dict(n_way=5, n_support=1)\n",
    "val_datamgr = SetDataManager(image_size, n_query=n_query, **test_few_shot_params)\n",
    "val_loader = val_datamgr.get_data_loader(val_file, aug=False)\n",
    "# a batch for SetDataManager: a [n_way, n_support + n_query, dim, w, h] tensor\n",
    "\n",
    "# WHERE WE NEED TO ADD BIAS\n",
    "# backbone.ConvBlock.maml = True\n",
    "# backbone.SimpleBlock.maml = True\n",
    "# backbone.BottleneckBlock.maml = True\n",
    "# backbone.ResNet.maml = True            \n",
    "model = differentialDKTIXnogpy(model_dict['Conv4_maml_diffDKTIX'], **train_few_shot_params)\n",
    "\n",
    "# Load state_dict\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee57961-1d9e-46f6-8a8f-d2711e4ea410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv4_maml_diffDKTIX\n",
      "Conv4 state dict :\n",
      " odict_keys(['trunk.0.C.weight', 'trunk.0.C.bias', 'trunk.0.BN.weight', 'trunk.0.BN.bias', 'trunk.0.BN.running_mean', 'trunk.0.BN.running_var', 'trunk.0.BN.num_batches_tracked', 'trunk.1.C.weight', 'trunk.1.C.bias', 'trunk.1.BN.weight', 'trunk.1.BN.bias', 'trunk.1.BN.running_mean', 'trunk.1.BN.running_var', 'trunk.1.BN.num_batches_tracked', 'trunk.2.C.weight', 'trunk.2.C.bias', 'trunk.2.BN.weight', 'trunk.2.BN.bias', 'trunk.2.BN.running_mean', 'trunk.2.BN.running_var', 'trunk.2.BN.num_batches_tracked', 'trunk.3.C.weight', 'trunk.3.C.bias', 'trunk.3.BN.weight', 'trunk.3.BN.bias', 'trunk.3.BN.running_mean', 'trunk.3.BN.running_var', 'trunk.3.BN.num_batches_tracked'])\n",
      "\n",
      "Conv4\n",
      "Conv4 backbone state dict:\n",
      " odict_keys(['trunk.0.C.weight', 'trunk.0.C.bias', 'trunk.0.BN.weight', 'trunk.0.BN.bias', 'trunk.0.BN.running_mean', 'trunk.0.BN.running_var', 'trunk.0.BN.num_batches_tracked', 'trunk.0.trunk.0.weight', 'trunk.0.trunk.0.bias', 'trunk.0.trunk.1.weight', 'trunk.0.trunk.1.bias', 'trunk.0.trunk.1.running_mean', 'trunk.0.trunk.1.running_var', 'trunk.0.trunk.1.num_batches_tracked', 'trunk.1.C.weight', 'trunk.1.C.bias', 'trunk.1.BN.weight', 'trunk.1.BN.bias', 'trunk.1.BN.running_mean', 'trunk.1.BN.running_var', 'trunk.1.BN.num_batches_tracked', 'trunk.1.trunk.0.weight', 'trunk.1.trunk.0.bias', 'trunk.1.trunk.1.weight', 'trunk.1.trunk.1.bias', 'trunk.1.trunk.1.running_mean', 'trunk.1.trunk.1.running_var', 'trunk.1.trunk.1.num_batches_tracked', 'trunk.2.C.weight', 'trunk.2.C.bias', 'trunk.2.BN.weight', 'trunk.2.BN.bias', 'trunk.2.BN.running_mean', 'trunk.2.BN.running_var', 'trunk.2.BN.num_batches_tracked', 'trunk.2.trunk.0.weight', 'trunk.2.trunk.0.bias', 'trunk.2.trunk.1.weight', 'trunk.2.trunk.1.bias', 'trunk.2.trunk.1.running_mean', 'trunk.2.trunk.1.running_var', 'trunk.2.trunk.1.num_batches_tracked', 'trunk.3.C.weight', 'trunk.3.C.bias', 'trunk.3.BN.weight', 'trunk.3.BN.bias', 'trunk.3.BN.running_mean', 'trunk.3.BN.running_var', 'trunk.3.BN.num_batches_tracked', 'trunk.3.trunk.0.weight', 'trunk.3.trunk.0.bias', 'trunk.3.trunk.1.weight', 'trunk.3.trunk.1.bias', 'trunk.3.trunk.1.running_mean', 'trunk.3.trunk.1.running_var', 'trunk.3.trunk.1.num_batches_tracked'])\n"
     ]
    }
   ],
   "source": [
    "print('Conv4 state dict :\\n', model_dict['Conv4_maml_diffDKTIX']().state_dict().keys())\n",
    "print('')\n",
    "print('Conv4 backbone state dict:\\n', model_dict['Conv4']().state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8f5df3-713c-4e0f-b9d5-b03c61282e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old keys: \n",
      " odict_keys(['feature.trunk.0.C.weight', 'feature.trunk.0.C.bias', 'feature.trunk.0.BN.weight', 'feature.trunk.0.BN.bias', 'feature.trunk.0.BN.running_mean', 'feature.trunk.0.BN.running_var', 'feature.trunk.0.BN.num_batches_tracked', 'feature.trunk.0.trunk.0.weight', 'feature.trunk.0.trunk.0.bias', 'feature.trunk.0.trunk.1.weight', 'feature.trunk.0.trunk.1.bias', 'feature.trunk.0.trunk.1.running_mean', 'feature.trunk.0.trunk.1.running_var', 'feature.trunk.0.trunk.1.num_batches_tracked', 'feature.trunk.1.C.weight', 'feature.trunk.1.C.bias', 'feature.trunk.1.BN.weight', 'feature.trunk.1.BN.bias', 'feature.trunk.1.BN.running_mean', 'feature.trunk.1.BN.running_var', 'feature.trunk.1.BN.num_batches_tracked', 'feature.trunk.1.trunk.0.weight', 'feature.trunk.1.trunk.0.bias', 'feature.trunk.1.trunk.1.weight', 'feature.trunk.1.trunk.1.bias', 'feature.trunk.1.trunk.1.running_mean', 'feature.trunk.1.trunk.1.running_var', 'feature.trunk.1.trunk.1.num_batches_tracked', 'feature.trunk.2.C.weight', 'feature.trunk.2.C.bias', 'feature.trunk.2.BN.weight', 'feature.trunk.2.BN.bias', 'feature.trunk.2.BN.running_mean', 'feature.trunk.2.BN.running_var', 'feature.trunk.2.BN.num_batches_tracked', 'feature.trunk.2.trunk.0.weight', 'feature.trunk.2.trunk.0.bias', 'feature.trunk.2.trunk.1.weight', 'feature.trunk.2.trunk.1.bias', 'feature.trunk.2.trunk.1.running_mean', 'feature.trunk.2.trunk.1.running_var', 'feature.trunk.2.trunk.1.num_batches_tracked', 'feature.trunk.3.C.weight', 'feature.trunk.3.C.bias', 'feature.trunk.3.BN.weight', 'feature.trunk.3.BN.bias', 'feature.trunk.3.BN.running_mean', 'feature.trunk.3.BN.running_var', 'feature.trunk.3.BN.num_batches_tracked', 'feature.trunk.3.trunk.0.weight', 'feature.trunk.3.trunk.0.bias', 'feature.trunk.3.trunk.1.weight', 'feature.trunk.3.trunk.1.bias', 'feature.trunk.3.trunk.1.running_mean', 'feature.trunk.3.trunk.1.running_var', 'feature.trunk.3.trunk.1.num_batches_tracked', 'classifier.weight', 'classifier.bias'])\n",
      "\n",
      "\n",
      "New keys:  \n",
      " dict_keys(['feature_extractor.0.trunk.0.C.weight', 'feature_extractor.0.trunk.0.C.bias', 'feature_extractor.0.trunk.0.BN.weight', 'feature_extractor.0.trunk.0.BN.bias', 'feature_extractor.0.trunk.0.BN.running_mean', 'feature_extractor.0.trunk.0.BN.running_var', 'feature_extractor.0.trunk.0.BN.num_batches_tracked', 'feature_extractor.0.trunk.0.0.weight', 'feature_extractor.0.trunk.0.0.bias', 'feature_extractor.0.trunk.0.1.weight', 'feature_extractor.0.trunk.0.1.bias', 'feature_extractor.0.trunk.0.1.running_mean', 'feature_extractor.0.trunk.0.1.running_var', 'feature_extractor.0.trunk.0.1.num_batches_tracked', 'feature_extractor.1.trunk.0.C.weight', 'feature_extractor.1.trunk.0.C.bias', 'feature_extractor.1.trunk.0.BN.weight', 'feature_extractor.1.trunk.0.BN.bias', 'feature_extractor.1.trunk.0.BN.running_mean', 'feature_extractor.1.trunk.0.BN.running_var', 'feature_extractor.1.trunk.0.BN.num_batches_tracked', 'feature_extractor.1.trunk.0.0.weight', 'feature_extractor.1.trunk.0.0.bias', 'feature_extractor.1.trunk.0.1.weight', 'feature_extractor.1.trunk.0.1.bias', 'feature_extractor.1.trunk.0.1.running_mean', 'feature_extractor.1.trunk.0.1.running_var', 'feature_extractor.1.trunk.0.1.num_batches_tracked', 'feature_extractor.2.trunk.0.C.weight', 'feature_extractor.2.trunk.0.C.bias', 'feature_extractor.2.trunk.0.BN.weight', 'feature_extractor.2.trunk.0.BN.bias', 'feature_extractor.2.trunk.0.BN.running_mean', 'feature_extractor.2.trunk.0.BN.running_var', 'feature_extractor.2.trunk.0.BN.num_batches_tracked', 'feature_extractor.2.trunk.0.0.weight', 'feature_extractor.2.trunk.0.0.bias', 'feature_extractor.2.trunk.0.1.weight', 'feature_extractor.2.trunk.0.1.bias', 'feature_extractor.2.trunk.0.1.running_mean', 'feature_extractor.2.trunk.0.1.running_var', 'feature_extractor.2.trunk.0.1.num_batches_tracked', 'feature_extractor.3.trunk.0.C.weight', 'feature_extractor.3.trunk.0.C.bias', 'feature_extractor.3.trunk.0.BN.weight', 'feature_extractor.3.trunk.0.BN.bias', 'feature_extractor.3.trunk.0.BN.running_mean', 'feature_extractor.3.trunk.0.BN.running_var', 'feature_extractor.3.trunk.0.BN.num_batches_tracked', 'feature_extractor.3.trunk.0.0.weight', 'feature_extractor.3.trunk.0.0.bias', 'feature_extractor.3.trunk.0.1.weight', 'feature_extractor.3.trunk.0.1.bias', 'feature_extractor.3.trunk.0.1.running_mean', 'feature_extractor.3.trunk.0.1.running_var', 'feature_extractor.3.trunk.0.1.num_batches_tracked', 'classifier.weight', 'classifier.bias'])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "missing keys ;  \n",
      " ['feature_extractor.0.trunk.0.C.weight', 'feature_extractor.0.trunk.0.C.bias', 'feature_extractor.0.trunk.0.BN.weight', 'feature_extractor.0.trunk.0.BN.bias', 'feature_extractor.0.trunk.0.BN.running_mean', 'feature_extractor.0.trunk.0.BN.running_var', 'feature_extractor.0.trunk.1.C.weight', 'feature_extractor.0.trunk.1.C.bias', 'feature_extractor.0.trunk.1.BN.weight', 'feature_extractor.0.trunk.1.BN.bias', 'feature_extractor.0.trunk.1.BN.running_mean', 'feature_extractor.0.trunk.1.BN.running_var', 'feature_extractor.0.trunk.2.C.weight', 'feature_extractor.0.trunk.2.C.bias', 'feature_extractor.0.trunk.2.BN.weight', 'feature_extractor.0.trunk.2.BN.bias', 'feature_extractor.0.trunk.2.BN.running_mean', 'feature_extractor.0.trunk.2.BN.running_var', 'feature_extractor.0.trunk.3.C.weight', 'feature_extractor.0.trunk.3.C.bias', 'feature_extractor.0.trunk.3.BN.weight', 'feature_extractor.0.trunk.3.BN.bias', 'feature_extractor.0.trunk.3.BN.running_mean', 'feature_extractor.0.trunk.3.BN.running_var', 'feature_extractor.1.weight', 'feature_extractor.1.bias']\n",
      "\n",
      "\n",
      "Unexpected keys ;  \n",
      " ['feature.trunk.0.trunk.0.weight', 'feature.trunk.0.trunk.0.bias', 'feature.trunk.0.trunk.1.weight', 'feature.trunk.0.trunk.1.bias', 'feature.trunk.0.trunk.1.running_mean', 'feature.trunk.0.trunk.1.running_var', 'feature.trunk.0.trunk.1.num_batches_tracked', 'feature.trunk.1.trunk.0.weight', 'feature.trunk.1.trunk.0.bias', 'feature.trunk.1.trunk.1.weight', 'feature.trunk.1.trunk.1.bias', 'feature.trunk.1.trunk.1.running_mean', 'feature.trunk.1.trunk.1.running_var', 'feature.trunk.1.trunk.1.num_batches_tracked', 'feature.trunk.2.trunk.0.weight', 'feature.trunk.2.trunk.0.bias', 'feature.trunk.2.trunk.1.weight', 'feature.trunk.2.trunk.1.bias', 'feature.trunk.2.trunk.1.running_mean', 'feature.trunk.2.trunk.1.running_var', 'feature.trunk.2.trunk.1.num_batches_tracked', 'feature.trunk.3.trunk.0.weight', 'feature.trunk.3.trunk.0.bias', 'feature.trunk.3.trunk.1.weight', 'feature.trunk.3.trunk.1.bias', 'feature.trunk.3.trunk.1.running_mean', 'feature.trunk.3.trunk.1.running_var', 'feature.trunk.3.trunk.1.num_batches_tracked']\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./save/checkpoints/CUB/Conv4_maml_aug_5way_1shot\"\n",
    "\n",
    "resume_file = get_resume_file(checkpoint_dir)\n",
    "# /!\\ CAUTION : get_resume_file does not give the same results in testing that get_best_file, that is used in the test.py\n",
    "\n",
    "tmp = torch.load(resume_file)\n",
    "\n",
    "old_state_dict = tmp['state']\n",
    "new_state_dict = remap_state_dict(old_state_dict)\n",
    "\n",
    "print(\"Old keys: \\n\", old_state_dict.keys())\n",
    "print('')\n",
    "print('')\n",
    "print(\"New keys:  \\n\", new_state_dict.keys())\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "start_epoch = tmp['epoch'] + 1\n",
    "missing, unexpected = model.load_state_dict(old_state_dict, strict=False)  # /!\\ cAUTION, VERY DANGEROUS STRICT = FALSE\n",
    "print('missing keys ;  \\n', missing)\n",
    "print('')\n",
    "print('')\n",
    "print('Unexpected keys ;  \\n', unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72aae67-1292-4ddd-84f7-3f6c4f55df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differentialDKTIXnogpy(\n",
      "  (feature): Conv4_maml_diffDKTIX_C(\n",
      "    (trunk): Sequential(\n",
      "      (0): ConvBlock_MAML_TO_DIFF(\n",
      "        (C): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (1): ConvBlock_MAML_TO_DIFF(\n",
      "        (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (2): ConvBlock_MAML_TO_DIFF(\n",
      "        (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (3): ConvBlock_MAML_TO_DIFF(\n",
      "        (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (4): Flatten()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1600, out_features=5, bias=True)\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv4_maml_diffDKTIX_C(\n",
      "      (trunk): Sequential(\n",
      "        (0): ConvBlock_MAML_TO_DIFF(\n",
      "          (C): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (1): ConvBlock_MAML_TO_DIFF(\n",
      "          (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (2): ConvBlock_MAML_TO_DIFF(\n",
      "          (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (3): ConvBlock_MAML_TO_DIFF(\n",
      "          (C): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (BN): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (4): Flatten()\n",
      "      )\n",
      "    )\n",
      "    (1): Linear(in_features=1600, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a823c7b2-b8ad-40d6-847d-0068e1227303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | Batch 0/100 | Loss 0.000000 | Acc 36.250000\n",
      "100 Test Acc = 46.64% +- 2.26%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46.6375, 11.525372824772306)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.test_loop( val_loader, optim_based=True, n_ft=10, lr=0.01, temp=1, return_std = True)\n",
    "# model.test_loop(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a6aefc5-e0ff-427e-8431-3d282ee01522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | Batch 0/100 | Loss 0.000000 | Acc 71.250000\n",
      "100 Test Acc = 57.39% +- 2.48%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(57.3875, 12.633159491987742)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define novel loader :\n",
    "few_shot_params = dict(n_way = 5, n_support = 1)\n",
    "datamgr         = SetDataManager(image_size, n_eposide = 600, n_query = 15 , **few_shot_params)\n",
    "loadfile    = configs.data_dir['CUB'] + 'novel.json'\n",
    "novel_loader     = datamgr.get_data_loader( loadfile, aug = False)\n",
    "\n",
    "model.test_loop( val_loader, optim_based=True, n_ft=100, lr=0.01, temp=1, return_std = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
