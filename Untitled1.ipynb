{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b738b6-c37e-4821-81ea-ba53df1a9de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as np\n",
    "from flax import linen as nn\n",
    "import trainer\n",
    "import ntk\n",
    "import test\n",
    "import train_states\n",
    "import models\n",
    "import utils\n",
    "\n",
    "import dataset_sines_infinite\n",
    "import dataset_sines_finite\n",
    "\n",
    "from jax import random\n",
    "from jax import numpy as np\n",
    "from flax.core import FrozenDict\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed55ce6-cb1c-4c87-9e44-c247595b1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_network(n_neurons, activation, reg_dim):\n",
    "    \"\"\"\n",
    "    Returns a small neural network (two layers, n_neurons per layer with specified activation)\n",
    "    Use reg_dim to control the dimension of the output\n",
    "\n",
    "    Compatible activations: \"relu\" and \"tanh\" (note: all experiments run with ReLU).\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        act_fn = nn.relu\n",
    "    elif activation == \"tanh\":\n",
    "        act_fn = nn.tanh\n",
    "\n",
    "    class Regressor(nn.Module):\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            x = nn.Dense(n_neurons)(x)\n",
    "            x = act_fn(x)\n",
    "\n",
    "            x = nn.Dense(n_neurons)(x)\n",
    "            x = act_fn(x)\n",
    "\n",
    "            x = nn.Dense(reg_dim)(x)\n",
    "\n",
    "            x = np.reshape(x, (-1, reg_dim))\n",
    "\n",
    "            return x\n",
    "    \n",
    "    return Regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9076e5a-ec6e-4fa0-84c6-b1bc2c7d2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.small_network(40, \"relu\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e719c13c-bd6c-43e3-a184-5e286249aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key_init, key = random.split(key)\n",
    "batch = random.uniform(key_init, shape=(5,1), minval=-5, maxval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd10163-9579-4926-ae81-0f343e805dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.2542226 ]\n",
      " [0.8597934 ]\n",
      " [4.922459  ]\n",
      " [0.13030052]\n",
      " [3.881017  ]]\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4044a5b9-3f24-405b-bfd2-68c9d4359f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vars = model.init(key_init, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0539dba0-3caa-460b-bcc7-1d7994befd71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'Dense_0': {'kernel': Array([[ 0.8108792 ,  1.1420408 , -1.5375005 ,  1.1752645 ,  0.98903   ,\n",
      "         0.7084738 ,  1.3740441 ,  0.26782113, -2.0145447 ,  0.68219405,\n",
      "        -0.55005306,  0.01865971,  1.3353693 ,  0.2529426 ,  0.96965194,\n",
      "         0.701057  ,  0.3524331 , -0.04911264, -0.16912822, -0.8375746 ,\n",
      "         0.5150138 , -1.8802063 , -0.48177794,  0.03080738, -0.9859298 ,\n",
      "        -0.5759436 ,  1.4276935 , -0.8445995 ,  0.20796399,  0.9777959 ,\n",
      "         0.8330394 , -0.5431388 ,  0.38610137,  0.59357536, -0.66677487,\n",
      "        -1.3662914 ,  0.93745106, -0.8884618 , -0.8272108 ,  0.9520352 ]],      dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'Dense_1': {'kernel': Array([[-0.09744288, -0.26018167, -0.1353759 , ..., -0.08283684,\n",
      "         0.01452957, -0.05381209],\n",
      "       [-0.35149738,  0.05002742,  0.3483839 , ...,  0.0916137 ,\n",
      "         0.1556775 , -0.11889914],\n",
      "       [-0.33027273, -0.0241636 , -0.21942686, ...,  0.23083718,\n",
      "        -0.02148556,  0.1912413 ],\n",
      "       ...,\n",
      "       [ 0.09482598,  0.13831006, -0.10595082, ..., -0.06867173,\n",
      "         0.0298217 ,  0.08890115],\n",
      "       [-0.08680293,  0.34720215,  0.13303356, ...,  0.00757984,\n",
      "         0.12922285, -0.03192221],\n",
      "       [ 0.09789236, -0.11470412,  0.0848557 , ..., -0.2527583 ,\n",
      "        -0.21127199,  0.02737993]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.], dtype=float32)}, 'Dense_2': {'kernel': Array([[-0.03181211],\n",
      "       [-0.0615486 ],\n",
      "       [ 0.0122979 ],\n",
      "       [ 0.2379153 ],\n",
      "       [-0.05876513],\n",
      "       [ 0.08366673],\n",
      "       [ 0.0158121 ],\n",
      "       [-0.17626038],\n",
      "       [-0.05756602],\n",
      "       [ 0.00233533],\n",
      "       [ 0.12142742],\n",
      "       [-0.1836524 ],\n",
      "       [ 0.06901909],\n",
      "       [-0.03684223],\n",
      "       [ 0.16642384],\n",
      "       [ 0.08136488],\n",
      "       [ 0.06159389],\n",
      "       [-0.04006878],\n",
      "       [-0.2821282 ],\n",
      "       [ 0.3520272 ],\n",
      "       [-0.07780003],\n",
      "       [-0.3045149 ],\n",
      "       [ 0.3252422 ],\n",
      "       [-0.06529135],\n",
      "       [-0.21534853],\n",
      "       [ 0.26488328],\n",
      "       [-0.10476757],\n",
      "       [ 0.17224513],\n",
      "       [ 0.02697052],\n",
      "       [ 0.18990979],\n",
      "       [-0.26495558],\n",
      "       [ 0.16886355],\n",
      "       [-0.11126126],\n",
      "       [-0.18430023],\n",
      "       [-0.3057543 ],\n",
      "       [ 0.01272719],\n",
      "       [-0.03146867],\n",
      "       [ 0.20125823],\n",
      "       [ 0.0964851 ],\n",
      "       [-0.03709185]], dtype=float32), 'bias': Array([0.], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "print(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "940290a2-6fa8-49b6-b39d-d688b7fc4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fn_wrapper(apply_fn, is_training):\n",
    "    \"\"\"\n",
    "    Wraps apply_fn(variables, inputs) into apply_fn_bis(params, batch_stats, inputs).\n",
    "    The is_training parameter is used to avoid errors:\n",
    "    * If is_training=True, then the keyword mutable is set to True for the batch_stats\n",
    "    * If is_training=False, then the keywork mutable is set to False.\n",
    "\n",
    "    In either cases, only the output of the network will be returned.\n",
    "    The updated batch_stats will be lost, and must be computed explicitely apart.\n",
    "    \"\"\"\n",
    "\n",
    "    if is_training:\n",
    "        def apply_fn2(params, batch_stats, inputs):\n",
    "            # mutable, but the updated batch_stats is not used\n",
    "            output, _ = apply_fn({\"params\": params, \"batch_stats\": batch_stats}, inputs, mutable=[\"batch_stats\"])\n",
    "            return output\n",
    "\n",
    "        return apply_fn2\n",
    "\n",
    "    else:\n",
    "        def apply_fn2(params, batch_stats, inputs):\n",
    "            # not mutable, no updated batch_stats\n",
    "            output = apply_fn({\"params\": params, \"batch_stats\": batch_stats}, inputs)\n",
    "            return output\n",
    "\n",
    "        return apply_fn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3f62e7-43cc-4d49-b183-c94fa1794018",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn = utils.apply_fn_wrapper(model.apply, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e133c0-6870-4824-9180-bce8b7a06563",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn_raw = model.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b52432-9dc9-4a7b-b3a6-eb3b6fb78822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "from jax import numpy as np\n",
    "from jax import pmap\n",
    "import jax\n",
    "from jax import value_and_grad\n",
    "from jax import jit\n",
    "import time\n",
    "from jax import lax\n",
    "\n",
    "import nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48d7e02d-6403-42b6-b359-f70b6bbe9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_identity_cov(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_m) = pmap(pmapable_loss_identity_cov,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_m = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_m)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_identity_cov(current_state, current_gradients_p, current_gradients_m, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc1239-27e8-4635-a567-38d5572d8a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
