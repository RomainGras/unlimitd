{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18541d29-09ee-47f8-a047-00e363e3ae6f",
   "metadata": {},
   "source": [
    "# Test with pre existing loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc79c1-d044-4e26-8fe3-3e6848bdaed5",
   "metadata": {},
   "source": [
    "## QMUL current loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7837e9-77a6-4a6c-a157-4f7bdc9f2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data.qmul_loader import train_people, test_people, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8e0d38-36a8-4588-aa8c-6a9e64581667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DennisPNoGlassesGrey',\n",
       " 'JohnGrey',\n",
       " 'SimonBGrey',\n",
       " 'SeanGGrey',\n",
       " 'DanJGrey',\n",
       " 'AdamBGrey',\n",
       " 'JackGrey',\n",
       " 'RichardHGrey',\n",
       " 'YongminYGrey',\n",
       " 'TomKGrey',\n",
       " 'PaulVGrey',\n",
       " 'DennisPGrey',\n",
       " 'CarlaBGrey',\n",
       " 'JamieSGrey',\n",
       " 'KateSGrey',\n",
       " 'DerekCGrey',\n",
       " 'KatherineWGrey',\n",
       " 'ColinPGrey',\n",
       " 'SueWGrey',\n",
       " 'GrahamWGrey',\n",
       " 'KrystynaNGrey',\n",
       " 'SeanGNoGlassesGrey',\n",
       " 'KeithCGrey',\n",
       " 'HeatherLGrey']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0384ce-6c7c-409e-b3d8-aaa2d728f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = get_batch(train_people=train_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49363f5f-d51f-45e2-b5b6-e238b9edd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 19, 3, 100, 100])\n",
      "torch.Size([24, 19])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a99098-09f8-4d16-9df0-89991cce6fde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333],\n",
      "        [ 0.3333,  0.0000, -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000,\n",
      "         -0.3333, -0.3333,  0.0000,  0.3333,  0.3333,  0.0000, -0.3333, -0.3333,\n",
      "         -0.3333,  0.3333,  0.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32027be0-629d-4d4c-b1d6-e3266d762538",
   "metadata": {},
   "source": [
    "## Regression loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91c175-3f33-4122-8e92-34d0c2504899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "<class 'tuple'>\n",
      "4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression_data_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_provider\n\u001b[0;32m----> 3\u001b[0m provider \u001b[38;5;241m=\u001b[39m \u001b[43mdata_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mberkeley\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/unlimitd/deep-kernel-transfer/data/regression_data_loader.py:256\u001b[0m, in \u001b[0;36mdata_provider.__init__\u001b[0;34m(self, dataset, n_samples)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples \u001b[38;5;241m=\u001b[39m n_samples  \u001b[38;5;66;03m# No way to change n_samples \u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_tasks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_tasks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_tasks \u001b[38;5;241m=\u001b[39m provide_data(dataset)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tuples_to_right_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not recognized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/unlimitd/deep-kernel-transfer/data/regression_data_loader.py:268\u001b[0m, in \u001b[0;36mdata_provider.convert_tuples_to_right_format\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(meta_data[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(meta_data[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (arrX, arrY) \u001b[38;5;129;01min\u001b[39;00m meta_data:\n\u001b[1;32m    269\u001b[0m     arrX_list\u001b[38;5;241m.\u001b[39mappend(arrX)  \u001b[38;5;66;03m# (30, 11)\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     arrY_list\u001b[38;5;241m.\u001b[39mappend(arrY)  \u001b[38;5;66;03m# (30,)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from data.regression_data_loader import data_provider\n",
    "\n",
    "provider = data_provider(\"Berkeley\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f192f-37c6-40e2-b3a5-d5bf9bfabd7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'provider' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mprovider\u001b[49m\u001b[38;5;241m.\u001b[39mget_train_batch(n_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'provider' is not defined"
     ]
    }
   ],
   "source": [
    "X, Y = provider.get_train_batch(n_tasks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55ccbd-af7d-4d36-b2ac-93be97dc96cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60ceaf-5f42-4146-8a7f-632986498b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "QMUL\n",
    "\"\"\"\n",
    "    \n",
    "train_people = ['DennisPNoGlassesGrey','JohnGrey','SimonBGrey','SeanGGrey','DanJGrey','AdamBGrey','JackGrey','RichardHGrey','YongminYGrey','TomKGrey','PaulVGrey','DennisPGrey','CarlaBGrey','JamieSGrey','KateSGrey','DerekCGrey','KatherineWGrey','ColinPGrey','SueWGrey','GrahamWGrey','KrystynaNGrey','SeanGNoGlassesGrey','KeithCGrey','HeatherLGrey']\n",
    "test_people  = ['RichardBGrey','TasosHGrey','SarahLGrey','AndreeaVGrey','YogeshRGrey']\n",
    "\n",
    "def num_to_str(num):\n",
    "    str_ = ''\n",
    "    if num == 0:\n",
    "        str_ = '000'\n",
    "    elif num < 100:\n",
    "        str_ = '0' + str(int(num))\n",
    "    else:\n",
    "        str_ = str(int(num))\n",
    "    return str_\n",
    "\n",
    "def get_person_at_curve(person, curve, prefix='filelists/QMUL/images/'):\n",
    "    faces   = []\n",
    "    targets = []\n",
    "\n",
    "    train_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "    for pitch, angle in curve:\n",
    "        fname  = prefix + person + '/' + person[:-4] + '_' + num_to_str(pitch) + '_' + num_to_str(angle) +'.jpg'\n",
    "        img    = Image.open(fname).convert('RGB')\n",
    "        img    = train_transforms(img)\n",
    "\n",
    "        faces.append(img)\n",
    "        pitch_norm = 2 * ((pitch - 60) /  (120 - 60)) -1\n",
    "        angle_norm = 2 * ((angle - 0)  / (180 - 0)) -1\n",
    "        targets.append(torch.Tensor([pitch_norm]))\n",
    "\n",
    "    faces   = torch.stack(faces)\n",
    "    targets = torch.stack(targets).squeeze()\n",
    "    return faces, targets\n",
    "\n",
    "def get_batch_qmul(train_people=train_people, num_samples=19):\n",
    "    ## generate trajectory\n",
    "    amp   = np.random.uniform(-3, 3)\n",
    "    phase = np.random.uniform(-5, 5)\n",
    "    wave  = [(amp * np.sin(phase + x)) for x in range(num_samples)]\n",
    "    ## map trajectory to angles/pitches\n",
    "    angles  = list(range(num_samples))\n",
    "    angles  = [x * 10 for x in angles]\n",
    "    pitches = [int(round(((y+3)*10 )+60,-1)) for y in wave]\n",
    "    curve   = [(p,a) for p, a in zip(pitches, angles)]\n",
    "\n",
    "    inputs  = []\n",
    "    targets = []\n",
    "    for person in train_people:\n",
    "        inps, targs = get_person_at_curve(person, curve)\n",
    "        inputs.append(inps)\n",
    "        targets.append(targs)\n",
    "\n",
    "    return torch.stack(inputs), torch.stack(targets)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "MARS datasets\n",
    "\"\"\"\n",
    "\n",
    "# This is taken from MARS implementation\n",
    "# TODO temporary remplacement\n",
    "BASE_DIR = '/home/gridsan/rgras/unlimitd/deep-kernel-transfer' # os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'filelists')\n",
    "\n",
    "BERKELEY_SENSOR_URL = 'https://www.dropbox.com/sh/y6egx20lod1gsrs/AACyXAk9Ua7SI-q1tpEb1SHba?dl=1'\n",
    "BERKELEY_SENSOR_DIR = os.path.join(DATA_DIR, 'sensor_data')\n",
    "\n",
    "ARGUS_CONTROL_URL = 'https://www.dropbox.com/sh/kdzqcw2b0rm34or/AAD2XFzgB2PSjGbNtfNER75Ba?dl=1'\n",
    "ARGUS_CONTROL_DIR = os.path.join(DATA_DIR, 'argus_data')\n",
    "\n",
    "class MetaDataset:\n",
    "    \"\"\"\n",
    "    To initiate a random_state, procedure in common for all datasets\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=None):\n",
    "        if random_state is None:\n",
    "            self.random_state = np.random\n",
    "        else:\n",
    "            self.random_state = random_state\n",
    "\n",
    "    def generate_meta_train_data(self, n_tasks: int, n_samples: int) -> list:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate_meta_test_data(self, n_tasks: int, n_samples_context: int, n_samples_test: int) -> list:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "\"\"\" Berkeley Sensor data \"\"\"\n",
    "\n",
    "\n",
    "class BerkeleySensorMetaDataset(MetaDataset):\n",
    "\n",
    "    def __init__(self, random_state=None, separate_train_test_days=True, berkeley_dir=None):\n",
    "        super().__init__(random_state)\n",
    "        task_ids = np.arange(46)\n",
    "        self.random_state.shuffle(task_ids)\n",
    "        self.train_task_ids = task_ids[:36]\n",
    "        self.test_task_ids = task_ids[36:]\n",
    "        self.separate_train_test_days = separate_train_test_days  # whether to also seperate the meta-train and meta-test set by days\n",
    "        self.data_path = berkeley_dir\n",
    "        if berkeley_dir is None:\n",
    "            if not os.path.isdir(BERKELEY_SENSOR_DIR):\n",
    "                print(\"Berkeley-Sensor data does not exist in %s\" % BERKELEY_SENSOR_DIR)\n",
    "                download_and_unzip_data(BERKELEY_SENSOR_URL, BERKELEY_SENSOR_DIR)\n",
    "\n",
    "    def generate_meta_test_data(self, n_tasks=10, n_samples_context=144, n_samples_test=-1):\n",
    "        task_tuples = self._load_data()\n",
    "\n",
    "        if n_samples_test == -1:\n",
    "            n_samples_test = min(2 * self.n_points_per_day, 3 * self.n_points_per_day - n_samples_context)\n",
    "        else:\n",
    "            assert n_samples_context + n_samples_test <= 3 * self.n_points_per_day\n",
    "\n",
    "        test_tuples = []\n",
    "        for task_id in self.test_task_ids[:n_tasks]:\n",
    "            x, y = task_tuples[task_id]\n",
    "            start_idx = -1 * (n_samples_test + n_samples_context)\n",
    "            x_context, y_context = x[start_idx:-n_samples_test], y[start_idx:-n_samples_test]\n",
    "            x_test, y_test = x[-n_samples_test:], y[-n_samples_test:]\n",
    "            test_tuples.append((x_context, y_context, x_test, y_test))\n",
    "        return test_tuples\n",
    "\n",
    "    def generate_meta_train_data(self, n_tasks=36, n_samples=-1):\n",
    "        task_tuples = self._load_data()\n",
    "        if self.separate_train_test_days:\n",
    "            if n_samples == -1:\n",
    "                n_samples = 2 * self.n_points_per_day\n",
    "            else:\n",
    "                assert n_samples <= 2 * self.n_points_per_day\n",
    "        train_tuples = []\n",
    "        for task_id in self.train_task_ids[:n_tasks]:\n",
    "            x, y = task_tuples[task_id]\n",
    "            indices = np.sort(np.random.randint(0, n_samples, 30))\n",
    "            train_tuples.append((x[indices], y[indices]))\n",
    "            # train_tuples.append((x[:n_samples], y[:n_samples]))\n",
    "        return train_tuples\n",
    "\n",
    "    def _load_data(self, lags=10):\n",
    "        from scipy.io import loadmat\n",
    "\n",
    "        if self.data_path is not None:\n",
    "            data_path = self.data_path + 'berkeley_data.mat'\n",
    "        else:\n",
    "            data_path = os.path.join(BERKELEY_SENSOR_DIR, 'berkeley_data.mat')\n",
    "\n",
    "        data = loadmat(data_path)['berkeley_data']['data'][0][0]\n",
    "        # replace outlier\n",
    "        data[4278, 6] = (data[4278 - 1, 6] + data[4278 + 1, 6]) / 2\n",
    "        n_points_per_day_raw = int(data.shape[0] / 5)\n",
    "        daytime = np.concatenate([np.arange(n_points_per_day_raw) / n_points_per_day_raw for _ in range(5)])\n",
    "\n",
    "        # remove first day since it has a break with the remaining 3 days (i.e. day 1, 5, 6, 7, 8]\n",
    "        data = data[n_points_per_day_raw:]\n",
    "        daytime = daytime[n_points_per_day_raw:]\n",
    "\n",
    "        data_tuples = []\n",
    "        for i in range(data.shape[-1]):\n",
    "            time_series = data[:, i]\n",
    "            y = time_series[lags:]\n",
    "            x = np.stack([time_series[lag: -lags + lag] for lag in range(lags)] + [daytime[lags:]], axis=-1)\n",
    "            assert x.shape[0] == y.shape[0] == len(time_series) - lags\n",
    "            # subsample every 5 minutes\n",
    "            x = x[::10]\n",
    "            y = y[::10]\n",
    "\n",
    "            data_tuples.append((x, y))\n",
    "\n",
    "        self.n_points_per_day = int(data_tuples[0][0].shape[0] / 4)\n",
    "        return data_tuples\n",
    "\n",
    "    \n",
    "\"\"\" Argus Control Dataset\"\"\"\n",
    "\n",
    "\n",
    "class ArgusMetaDataset(MetaDataset):\n",
    "\n",
    "    def __init__(self, random_state=None, task_of_interest='TV', argus_dir=None):\n",
    "        super().__init__(random_state)\n",
    "        task_ids_train = np.arange(20)\n",
    "        task_ids_test = np.arange(4)\n",
    "        self.random_state.shuffle(task_ids_train)\n",
    "        self.random_state.shuffle(task_ids_test)\n",
    "        self.train_task_ids = task_ids_train\n",
    "        self.test_task_ids = task_ids_test\n",
    "        self.data_path = argus_dir\n",
    "        self.task = task_of_interest\n",
    "\n",
    "        if argus_dir is not None:\n",
    "            self.data_dir = argus_dir\n",
    "        elif ARGUS_CONTROL_DIR is not None:\n",
    "            self.data_dir = ARGUS_CONTROL_DIR\n",
    "        else:\n",
    "            raise ValueError(\"No data directory provided.\")\n",
    "\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(\"Argus-Control data does not exist in %s\" % self.data_dir)\n",
    "            download_and_unzip_data(ARGUS_CONTROL_URL, self.data_dir)\n",
    "\n",
    "        f = open(self.data_dir + '/meta_data_argus_sim.json')\n",
    "\n",
    "        data = json.load(f)\n",
    "        self.train_data = data['meta_train'][self.task]\n",
    "        self.test_data = data['meta_test'][self.task]\n",
    "\n",
    "    def generate_meta_test_data(self, n_tasks=4, n_samples_context=100, n_samples_test=100):  # fixme\n",
    "        test_data = []\n",
    "        indices = np.arange(500)\n",
    "\n",
    "        for x_context, y_context, x_test, y_test in self.test_data[:n_tasks]:\n",
    "            self.random_state.shuffle(indices)\n",
    "            test_data.append((np.array(x_context)[indices[:n_samples_context]],\n",
    "                              np.array(y_context)[indices[:n_samples_context]],\n",
    "                              np.array(x_test)[indices[:n_samples_test]],\n",
    "                              np.array(y_test)[indices[:n_samples_test]]))\n",
    "        return test_data\n",
    "\n",
    "    def generate_meta_train_data(self, n_tasks=20, n_samples=100):\n",
    "        train_data = []\n",
    "        indices = np.arange(500)\n",
    "        for x_context, y_context in self.train_data[:n_tasks]:\n",
    "            self.random_state.shuffle(indices)\n",
    "            train_data.append((np.array(x_context)[indices[:n_samples], :],\n",
    "                               np.array(y_context)[indices[:n_samples]]))\n",
    "        return train_data\n",
    "    \n",
    "    \n",
    "    \n",
    "def download_and_unzip_data(url, target_dir):\n",
    "    from urllib.request import urlopen\n",
    "    from zipfile import ZipFile\n",
    "    print('Downloading %s' % url)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    tempfilepath = os.path.join(DATA_DIR, 'tempfile.zip')\n",
    "    zipresp = urlopen(url)\n",
    "    with open(tempfilepath, 'wb') as f:\n",
    "        f.write(zipresp.read())\n",
    "    zf = ZipFile(tempfilepath)\n",
    "    print('Extracting to %s' % target_dir)\n",
    "    zf.extractall(path=target_dir)\n",
    "    zf.close()\n",
    "    os.remove(tempfilepath)\n",
    "\n",
    "\n",
    "\"\"\" Data provider \"\"\"\n",
    "\n",
    "\n",
    "def provide_data(dataset, seed=28, n_train_tasks=None, n_samples=None, config=None, data_dir=None):\n",
    "    import numpy as np\n",
    "\n",
    "    N_TEST_TASKS = 20\n",
    "    N_VALID_TASKS = 20\n",
    "    N_TEST_SAMPLES = 200\n",
    "\n",
    "    # if specified, overwrite default settings\n",
    "    if config is not None:\n",
    "        if config['num_test_valid_tasks'] is not None: N_TEST_TASKS = config['num_test_valid_tasks']\n",
    "        if config['num_test_valid_tasks'] is not None: N_VALID_TASKS = config['num_test_valid_tasks']\n",
    "        if config['num_test_valid_samples'] is not None:  N_TEST_SAMPLES = config['num_test_valid_samples']\n",
    "\n",
    "    # \"\"\" Prepare Data \"\"\"       \n",
    "        \n",
    "    elif 'argus' in dataset:\n",
    "        if len(dataset.split('_')) == 2:\n",
    "            n_train_tasks = int(dataset.split('_')[-1])\n",
    "        else:\n",
    "            n_train_tasks = 20\n",
    "        n_samples_context = 100\n",
    "        task = 'TV'\n",
    "        dataset = ArgusMetaDataset(random_state=np.random.RandomState(seed), task_of_interest=task,\n",
    "                                   argus_dir=data_dir)\n",
    "        data_train = dataset.generate_meta_train_data(n_tasks=n_train_tasks, n_samples=n_samples_context)\n",
    "        data_test_valid = dataset.generate_meta_test_data(n_samples_context=n_samples_context, n_samples_test=-1)\n",
    "\n",
    "        return data_train, data_test_valid, data_test_valid\n",
    "        \n",
    "    elif 'berkeley' in dataset:\n",
    "        if len(dataset.split('_')) == 2:\n",
    "            n_train_tasks = int(dataset.split('_')[-1])\n",
    "\n",
    "        dataset = BerkeleySensorMetaDataset(random_state=np.random.RandomState(seed), berkeley_dir=data_dir)\n",
    "\n",
    "        assert n_samples is None\n",
    "        n_train_samples = 2 * 144\n",
    "        n_samples_context = 30  # 144 # corresponds to first day of measurements\n",
    "        data_train = dataset.generate_meta_train_data(n_tasks=n_train_tasks, n_samples=n_train_samples)\n",
    "        data_test_valid = dataset.generate_meta_test_data(n_samples_context=n_samples_context,\n",
    "                                                          n_samples_test=-1)\n",
    "        return data_train, data_test_valid, data_test_valid\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('Does not recognize dataset flag')\n",
    "\n",
    "    data_train = dataset.generate_meta_train_data(n_tasks=n_train_tasks, n_samples=n_train_samples)\n",
    "\n",
    "    data_test_valid = dataset.generate_meta_test_data(n_tasks=N_TEST_TASKS + N_VALID_TASKS,\n",
    "                                                      n_samples_context=n_context_samples,\n",
    "                                                      n_samples_test=N_TEST_SAMPLES)\n",
    "    data_valid = data_test_valid[N_VALID_TASKS:]\n",
    "    data_test = data_test_valid[:N_VALID_TASKS]\n",
    "    \n",
    "    return data_train, data_valid, data_test\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_right_format(meta_datasets, train_mode: bool = True):\n",
    "    new_meta_datasets = []\n",
    "    for meta_data in meta_datasets:\n",
    "        support_inputs = []\n",
    "        support_labels = []\n",
    "        query_inputs = []\n",
    "        query_labels = []\n",
    "\n",
    "        # Gather all support & query arrays\n",
    "        for data in meta_data:\n",
    "            if train_mode:\n",
    "                (sX, sY) = data\n",
    "            else:\n",
    "                # Only collect query data if not in train mode\n",
    "                (sX, sY, qX, qY) = data\n",
    "                query_inputs.append(qX)\n",
    "                query_labels.append(qY)\n",
    "                \n",
    "            support_inputs.append(sX)\n",
    "            support_labels.append(sY)\n",
    "\n",
    "\n",
    "        # Convert Python lists -> NumPy arrays -> torch tensors\n",
    "        support_inputs = torch.from_numpy(np.stack(support_inputs, axis=0))\n",
    "        support_labels = torch.from_numpy(np.stack(support_labels, axis=0))\n",
    "\n",
    "        if train_mode:\n",
    "            # For training, store just (support_inputs, support_labels)\n",
    "            meta_data = (support_inputs, support_labels)\n",
    "        else:\n",
    "            # For valid/test, also stack queries\n",
    "            query_inputs = torch.from_numpy(np.stack(query_inputs, axis=0))\n",
    "            query_labels = torch.from_numpy(np.stack(query_labels, axis=0))\n",
    "\n",
    "            meta_data = ((support_inputs, support_labels),\n",
    "                         (query_inputs, query_labels))\n",
    "            \n",
    "        new_meta_datasets.append(meta_data)\n",
    "        \n",
    "    return new_meta_datasets\n",
    "        \n",
    "            \n",
    "\n",
    "class data_provider():\n",
    "    def __init__(self, dataset, n_train_support=None, n_test_support=None, n_samples=None):\n",
    "        # If n_train_support is None, doesn't split meta train datasets\n",
    "        # Else, splits meta_train datasets into n_train_support support inputs, and the rest as query inputs\n",
    "        if 'qmul' in dataset:\n",
    "            self.n_samples = 19 if n_samples is None else n_samples\n",
    "            self.n_test_support = 5 if n_test_support is None else n_test_support\n",
    "        elif 'berkeley' in dataset or 'argus' in dataset:\n",
    "            # TODO\n",
    "            self.n_samples = n_samples  # No way to change n_samples \n",
    "            self.n_test_support = n_test_support  # No way to change n_test_support \n",
    "            \n",
    "            self.train_tasks, self.valid_tasks, self.test_tasks = provide_data(dataset)\n",
    "            [self.train_tasks] = convert_to_right_format([self.train_tasks], train_mode=True)\n",
    "            [self.valid_tasks, self.test_tasks] = convert_to_right_format([self.valid_tasks, self.test_tasks], train_mode=False)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Dataset not recognized\")\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        self.n_train_support = n_train_support # 9 for QMUL MAML\n",
    "\n",
    "    \n",
    "    def get_train_batch(self):\n",
    "        if 'qmul' in self.dataset:\n",
    "            inputs, targets = get_batch_qmul(train_people=train_people, num_samples=self.n_samples)\n",
    "            if self.n_train_support is None:\n",
    "                return inputs, targets\n",
    "            # else:\n",
    "            #    support_ind = list(np.random.choice(list(range(19)), replace=False, size=self.n_train_support))\n",
    "            #    query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "                # x_all = inputs.cuda()\n",
    "                # y_all = targets.cuda()\n",
    "\n",
    "                # x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "                # y_support = targets[:,support_ind].cuda()\n",
    "                # x_query   = inputs[:,query_ind,:,:,:]\n",
    "                # y_query   = targets[:,query_ind].cuda()\n",
    "                # return ((x_support, y_support), (x_query, y_query))\n",
    "            \n",
    "        elif 'berkeley' in self.dataset or 'argus' in self.dataset:\n",
    "            inputs, targets = self.train_tasks\n",
    "            if self.n_train_support is None:\n",
    "                return inputs, targets\n",
    "            # else:\n",
    "                # TODO : Do better split, these tasks may be splited in a specific way for their implementation in MAML\n",
    "                # support_ind = list(np.random.choice(list(range(19)), replace=False, size=self.n_train_support))\n",
    "                # query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "                # x_all = inputs.cuda()\n",
    "                # y_all = targets.cuda()\n",
    "\n",
    "                # x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "                # y_support = targets[:,support_ind].cuda()\n",
    "                # x_query   = inputs[:,query_ind,:,:,:]\n",
    "                # y_query   = targets[:,query_ind].cuda()\n",
    "                # return ((x_support, y_support), (x_query, y_query))\n",
    "        else:\n",
    "            raise ValueError(\"Dataset not recognized\")\n",
    "    \n",
    "    def get_test_batch(self):\n",
    "        if 'qmul' in self.dataset:\n",
    "            inputs, targets = get_batch_qmul(train_people=test_people, num_samples=self.n_samples)\n",
    "            support_ind = list(np.random.choice(list(range(19)), replace=False, size=self.n_test_support))\n",
    "            query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "            x_all = inputs.cuda()\n",
    "            y_all = targets.cuda()\n",
    "\n",
    "            x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "            y_support = targets[:,support_ind].cuda()\n",
    "            x_query   = inputs[:,query_ind,:,:,:]\n",
    "            y_query   = targets[:,query_ind].cuda()\n",
    "            return ((x_support, y_support), (x_query, y_query))\n",
    "        elif 'berkeley' in self.dataset or 'argus' in self.dataset:\n",
    "            return self.test_tasks\n",
    "        else:\n",
    "            raise ValueError(\"Dataset not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d6d315-f2ac-491d-ac0a-2a8dad6a5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "from data.regression_data_loader import data_provider\n",
    "provider = data_provider('QMUL')\n",
    "\n",
    "data = provider.get_train_batch()\n",
    "print(data[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fef4ec57-0179-4de5-b794-f8d768764c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([5, 5, 3, 100, 100]) torch.Size([5, 5])\n",
      "torch.Size([5, 14, 3, 100, 100]) torch.Size([5, 14])\n"
     ]
    }
   ],
   "source": [
    "((x_support, y_support), (x_query, y_query)) = provider.get_test_batch()\n",
    "print(x_support.dtype)\n",
    "\n",
    "print(x_support.shape, y_support.shape)\n",
    "print(x_query.shape, y_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760432d3-2f07-4beb-a7bb-25b0e67924dc",
   "metadata": {},
   "source": [
    "$\\textbf{QMUL OK}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441c809e-2ee9-445f-8f98-86fe860c6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = data_provider('berkeley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1e0f8d-6e62-4590-b304-be867045f10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = provider.get_train_batch()\n",
    "print(data[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a4b57a7-5ae5-4213-962b-6bf3a3b0310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a753aff-e362-44af-8c0b-1786c2bf73a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.Size([10, 30, 11]) torch.Size([10, 30])\n",
      "torch.Size([10, 288, 11]) torch.Size([10, 288])\n"
     ]
    }
   ],
   "source": [
    "((x_support, y_support), (x_query, y_query)) = provider.get_test_batch()\n",
    "\n",
    "print(x_support.dtype)\n",
    "\n",
    "print(x_support.shape, y_support.shape)\n",
    "print(x_query.shape, y_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167eb298-551c-4970-8274-ec95cc95e6a9",
   "metadata": {},
   "source": [
    "$\\textbf{BERKELEY OK}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33327c5-bfda-449d-afe0-bed8c2f0c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = data_provider('argus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6393627-4837-4283-b311-0e0d64150445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = provider.get_train_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f463d52-22ac-4f9a-ab5f-42f226dda648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fccf9850-6e0c-4395-a48f-eaca7dd10658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 100, 3])\n",
      "torch.Size([20, 100])\n"
     ]
    }
   ],
   "source": [
    "print(data[0].shape)\n",
    "print(data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "228c3714-725e-430c-9309-b62c13f8fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 3]) torch.Size([4, 100])\n",
      "torch.Size([4, 499, 3]) torch.Size([4, 499])\n"
     ]
    }
   ],
   "source": [
    "((x_support, y_support), (x_query, y_query)) = provider.get_test_batch()\n",
    "\n",
    "print(x_support.shape, y_support.shape)\n",
    "print(x_query.shape, y_query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bee80-9bac-4081-8396-3eccb7556c23",
   "metadata": {},
   "source": [
    "$\\textbf{ARGUS OK}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67af3f-4bba-41ea-849e-55e0b34f0e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
