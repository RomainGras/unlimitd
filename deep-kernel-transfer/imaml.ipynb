{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95152024-4f8e-4a94-9312-0468593612e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is iMAML, with 100 epochs, and kernel rbf\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import configs\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "from io_utils import parse_args_regression, get_resume_file\n",
    "from methods.maml import MAML\n",
    "from projection import create_random_projection_matrix, proj_sketch\n",
    "import backbone\n",
    "import os\n",
    "import numpy as np\n",
    "            \n",
    "class parameters():\n",
    "    def __init__(self):\n",
    "        self.seed = 0\n",
    "        self.model = \"Conv3\"\n",
    "        self.method = \"iMAML\"\n",
    "        self.dataset = \"QMUL\"\n",
    "        self.start_epoch = 0\n",
    "        self.stop_epoch = 100\n",
    "        \n",
    "params = parameters()\n",
    "\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "params.checkpoint_dir = '%scheckpoints/%s/' % (configs.save_dir, params.dataset)\n",
    "if not os.path.isdir(params.checkpoint_dir):\n",
    "    os.makedirs(params.checkpoint_dir)\n",
    "params.checkpoint_dir = '%scheckpoints/%s/%s_%s' % (configs.save_dir, params.dataset, params.model, params.method)\n",
    "\n",
    "bb               = backbone.Conv3().cuda()\n",
    "simple_net       = backbone.simple_net().cuda()\n",
    "simple_net_multi = backbone.simple_net_multi_output().cuda()\n",
    "\n",
    "combined_network       = backbone.CombinedNetwork(bb, simple_net).cuda()\n",
    "combined_network_multi = backbone.CombinedNetwork(bb, simple_net_multi).cuda()\n",
    "\n",
    "print(f\"This is {params.method}, with {params.stop_epoch} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d496125-a1b7-4df2-be74-4155ad94e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "\n",
    "def to_cuda(x):\n",
    "    try:\n",
    "        return x.cuda()\n",
    "    except:\n",
    "        return torch.from_numpy(x).float().cuda()\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    if type(x) == np.ndarray:\n",
    "        return torch.from_numpy(x).float()\n",
    "    elif type(x) == torch.Tensor:\n",
    "        return x\n",
    "    else:\n",
    "        print(\"Type error. Input should be either numpy array or torch tensor\")\n",
    "    \n",
    "\n",
    "def to_device(x, GPU=False):\n",
    "    if GPU:\n",
    "        return to_cuda(x)\n",
    "    else:\n",
    "        return to_tensor(x)\n",
    "    \n",
    "    \n",
    "def to_numpy(x):\n",
    "    if type(x) == np.ndarray:\n",
    "        return x\n",
    "    else:\n",
    "        try:\n",
    "            return x.data.numpy()\n",
    "        except:\n",
    "            return x.cpu().data.numpy()\n",
    "        \n",
    "\n",
    "def cg_solve(f_Ax, b, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10, x_init=None):\n",
    "    \"\"\"\n",
    "    Goal: Solve Ax=b equivalent to minimizing f(x) = 1/2 x^T A x - x^T b\n",
    "    Assumption: A is PSD, no damping term is used here (must be damped externally in f_Ax)\n",
    "    Algorithm template from wikipedia\n",
    "    Verbose mode works only with numpy\n",
    "    \"\"\"\n",
    "       \n",
    "    if type(b) == torch.Tensor:\n",
    "        x = torch.zeros(b.shape[0]) if x_init is None else x_init\n",
    "        x = x.to(b.device)\n",
    "        if b.dtype == torch.float16:\n",
    "            x = x.half()\n",
    "        r = b - f_Ax(x)\n",
    "        p = r.clone()\n",
    "    elif type(b) == np.ndarray:\n",
    "        x = np.zeros_like(b) if x_init is None else x_init\n",
    "        r = b - f_Ax(x)\n",
    "        p = r.copy()\n",
    "    else:\n",
    "        print(\"Type error in cg\")\n",
    "\n",
    "    fmtstr = \"%10i %10.3g %10.3g %10.3g\"\n",
    "    titlestr = \"%10s %10s %10s %10s\"\n",
    "    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\", \"obj fn\"))\n",
    "\n",
    "    for i in range(cg_iters):\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "        if verbose:\n",
    "            obj_fn = 0.5*x.dot(f_Ax(x)) - 0.5*b.dot(x)\n",
    "            norm_x = torch.norm(x) if type(x) == torch.Tensor else np.linalg.norm(x)\n",
    "            print(fmtstr % (i, r.dot(r), norm_x, obj_fn))\n",
    "\n",
    "        rdotr = r.dot(r)\n",
    "        Ap = f_Ax(p)\n",
    "        alpha = rdotr/(p.dot(Ap))\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * Ap\n",
    "        newrdotr = r.dot(r)\n",
    "        beta = newrdotr/rdotr\n",
    "        p = r + beta * p\n",
    "\n",
    "        if newrdotr < residual_tol:\n",
    "            # print(\"Early CG termination because the residual was small\")\n",
    "            break\n",
    "\n",
    "    if callback is not None:\n",
    "        callback(x)\n",
    "    if verbose: \n",
    "        obj_fn = 0.5*x.dot(f_Ax(x)) - 0.5*b.dot(x)\n",
    "        norm_x = torch.norm(x) if type(x) == torch.Tensor else np.linalg.norm(x)\n",
    "        print(fmtstr % (i, r.dot(r), norm_x, obj_fn))\n",
    "    return x\n",
    "\n",
    "\n",
    "def smooth_vector(vec, window_size=25):\n",
    "    svec = vec.copy()\n",
    "    if vec.shape[0] < window_size:\n",
    "        for i in range(vec.shape[0]):\n",
    "            svec[i,:] = np.mean(vec[:i, :], axis=0)\n",
    "    else:   \n",
    "        for i in range(window_size, vec.shape[0]):\n",
    "            svec[i,:] = np.mean(vec[i-window_size:i, :], axis=0)\n",
    "    return svec\n",
    "    \n",
    "    \n",
    "def save_data(agent, train_curve, other_data, save_file, itr=None):\n",
    "    data = dict(agent=agent,\n",
    "                losses=train_curve,\n",
    "                other_data=other_data,\n",
    "               )\n",
    "    pickle_file_name = save_file + '.pickle'\n",
    "    pickle.dump(data, open(pickle_file_name, 'wb'))\n",
    "    \n",
    "    plot_file_name = save_file + '.png'\n",
    "    plt.figure(figsize=(10,6))\n",
    "    if itr != None:\n",
    "        plt.plot(smooth_vector(train_curve[:itr]), lw=2)\n",
    "    else:\n",
    "        plt.plot(smooth_vector(train_curve), lw=2)\n",
    "    plt.xlabel('Meta (outer) iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0.0, 5.0])\n",
    "    plt.legend(['Train pre-adapt', 'Test pre-adapt', 'Train post-adapt', 'Test post-adapt'], loc=1)\n",
    "    plt.savefig(plot_file_name, dpi=100)\n",
    "\n",
    "    \n",
    "def measure_accuracy(task, model, train=False):\n",
    "    if train is True:\n",
    "        x, y = task['x_train'], task['y_train']\n",
    "    else:\n",
    "        x, y = task['x_val'], task['y_val']\n",
    "    y_hat = model.predict(x, return_numpy = True)\n",
    "    batch_size = y.shape[0]\n",
    "    predict_label = np.argmax(y_hat, axis=1)\n",
    "    try:\n",
    "        correct = np.sum(predict_label == y.cpu().data.numpy())\n",
    "    except:\n",
    "        correct = np.sum(predict_label == y.data.numpy())\n",
    "    return correct * 100.0 / batch_size\n",
    "\n",
    "    \n",
    "class DataLog:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.log = {}\n",
    "        self.max_len = 0\n",
    "        \n",
    "    def log_exp_args(self, parsed_args):\n",
    "        args = vars(parsed_args) # makes it a dictionary\n",
    "        for k in args.keys():\n",
    "            self.log_kv(k, args[k])\n",
    "\n",
    "    def log_kv(self, key, value):\n",
    "        # logs the (key, value) pair\n",
    "        if key not in self.log:\n",
    "            self.log[key] = []\n",
    "        self.log[key].append(value)\n",
    "        if len(self.log[key]) > self.max_len:\n",
    "            self.max_len = self.max_len + 1\n",
    "\n",
    "    def save_log(self, save_path=None):\n",
    "        save_path = self.log['save_dir'][-1] if save_path is None else save_path\n",
    "        pickle.dump(self.log, open(save_path+'/log.pickle', 'wb'))\n",
    "        with open(save_path+'/log.csv', 'w') as csv_file:\n",
    "            fieldnames = self.log.keys()\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in range(self.max_len):\n",
    "                row_dict = {}\n",
    "                for key in self.log.keys():\n",
    "                    if row < len(self.log[key]):\n",
    "                        row_dict[key] = self.log[key][row]\n",
    "                writer.writerow(row_dict)\n",
    "\n",
    "    def get_current_log(self):\n",
    "        row_dict = {}\n",
    "        for key in self.log.keys():\n",
    "            row_dict[key] = self.log[key][-1]\n",
    "        return row_dict\n",
    "\n",
    "    def read_log(self, log_path):\n",
    "        with open(log_path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            listr = list(reader)\n",
    "            keys = reader.fieldnames\n",
    "            data = {}\n",
    "            for key in keys:\n",
    "                data[key] = []\n",
    "            for row in listr:\n",
    "                for key in keys:\n",
    "                    try:\n",
    "                        data[key].append(eval(row[key]))\n",
    "                    except:\n",
    "                        None\n",
    "        self.log = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d44ea2-2dd8-49c4-a634-15802f42e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from   torch.nn import functional as F\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model, loss_function, inner_lr=1e-3, outer_lr=1e-2, GPU=False, inner_alg='gradient', outer_alg='adam'):\n",
    "        self.model = model\n",
    "        self.use_gpu = GPU\n",
    "        if GPU:\n",
    "            self.model.cuda()\n",
    "        assert outer_alg == 'sgd' or 'adam'\n",
    "        self.inner_opt = torch.optim.SGD(self.model.parameters(), lr=inner_lr)\n",
    "        if outer_alg == 'adam':\n",
    "            self.outer_opt = torch.optim.Adam(self.model.parameters(), lr=outer_lr, eps=1e-3)\n",
    "        else:\n",
    "            self.outer_opt = torch.optim.SGD(self.model.parameters(), lr=outer_lr)\n",
    "        self.loss_function = loss_function\n",
    "        assert inner_alg == 'gradient' # sqp unsupported in this version\n",
    "        self.inner_alg = inner_alg\n",
    "\n",
    "    def get_params(self):\n",
    "        return torch.cat([param.data.view(-1) for param in self.model.parameters()], 0).clone()\n",
    "\n",
    "    def set_params(self, param_vals):\n",
    "        offset = 0\n",
    "        for param in self.model.parameters():\n",
    "            param.data.copy_(param_vals[offset:offset + param.nelement()].view(param.size()))\n",
    "            offset += param.nelement()\n",
    "            \n",
    "    def set_outer_lr(self, lr):\n",
    "        for param_group in self.outer_opt.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "    def set_inner_lr(self, lr):\n",
    "        for param_group in self.inner_opt.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def regularization_loss(self, w_0, lam=0.0):\n",
    "        \"\"\"\n",
    "        Add a regularization loss onto the weights\n",
    "        The proximal term regularizes around the point w_0\n",
    "        Strength of regularization is lambda\n",
    "        lambda can either be scalar (type float) or ndarray (numpy.ndarray)\n",
    "        \"\"\"\n",
    "        regu_loss = 0.0\n",
    "        offset = 0\n",
    "        regu_lam = lam if type(lam) == float or np.float64 else to_tensor(lam)\n",
    "        if w_0.dtype == torch.float16:\n",
    "            try:\n",
    "                regu_lam = regu_lam.half()\n",
    "            except:\n",
    "                regu_lam = np.float16(regu_lam)\n",
    "        for param in self.model.parameters():\n",
    "            delta = param.view(-1) - w_0[offset:offset + param.nelement()].view(-1)\n",
    "            if type(regu_lam) == float or np.float64:\n",
    "                regu_loss += 0.5 * regu_lam * torch.sum(delta ** 2)\n",
    "            else:\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                param_lam = regu_lam[offset:offset + param.nelement()].view(-1)\n",
    "                param_delta = delta * param_lam\n",
    "                regu_loss += 0.5 * torch.sum(param_delta ** 2)\n",
    "            offset += param.nelement()\n",
    "        return regu_loss\n",
    "\n",
    "    def get_loss(self, x, y, return_numpy=False):\n",
    "        \"\"\"\n",
    "        Assume that x and y are torch tensors -- either in CPU or GPU (controlled externally)\n",
    "        \"\"\"\n",
    "        yhat = self.model.forward(x)\n",
    "        loss = self.loss_function(yhat, y)\n",
    "        if return_numpy:\n",
    "            loss = to_numpy(loss).ravel()[0]\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x, return_numpy=False):\n",
    "        yhat = self.model.forward(to_device(x, self.use_gpu))\n",
    "        if return_numpy:\n",
    "            yhat = to_numpy(yhat)\n",
    "        return yhat\n",
    "\n",
    "    def learn_on_data(self, x, y, num_steps=10,\n",
    "                      add_regularization=False,\n",
    "                      w_0=None, lam=0.0):\n",
    "        \n",
    "        assert self.inner_alg == 'gradient' # or 'sqp' or 'adam' # TODO(Aravind): support sqp and adam \n",
    "        train_loss = []\n",
    "        if self.inner_alg == 'gradient':\n",
    "            for i in range(num_steps):\n",
    "                self.inner_opt.zero_grad()\n",
    "                tloss = self.get_loss(x, y)\n",
    "                loss = tloss + self.regularization_loss(w_0, lam) if add_regularization else tloss\n",
    "                loss.backward()\n",
    "                self.inner_opt.step()\n",
    "                train_loss.append(to_numpy(tloss))\n",
    "\n",
    "        return train_loss\n",
    "\n",
    "    def learn_task(self, task, num_steps=10, add_regularization=False, w_0=None, lam=0.0):\n",
    "        xt, yt = task['x_train'], task['y_train']\n",
    "        return self.learn_on_data(xt, yt, num_steps, add_regularization, w_0, lam)\n",
    "\n",
    "    def move_toward_target(self, target, lam=2.0):\n",
    "        \"\"\"\n",
    "        Move slowly towards the target parameter value\n",
    "        Default value for lam assumes learning rate determined by optimizer\n",
    "        Useful for implementing Reptile\n",
    "        \"\"\"\n",
    "        # we can implement this with the regularization loss, but regularize around the target point\n",
    "        # and with specific choice of lam=2.0 to preserve the learning rate of inner_opt\n",
    "        self.outer_opt.zero_grad()\n",
    "        loss = self.regularization_loss(target, lam=lam)\n",
    "        loss.backward()\n",
    "        self.outer_opt.step()\n",
    "\n",
    "    def outer_step_with_grad(self, grad, flat_grad=False):\n",
    "        \"\"\"\n",
    "        Given the gradient, step with the outer optimizer using the gradient.\n",
    "        Assumed that the gradient is a tuple/list of size compatible with model.parameters()\n",
    "        If flat_grad, then the gradient is a flattened vector\n",
    "        \"\"\"\n",
    "        check = 0\n",
    "        for p in self.model.parameters():\n",
    "            check = check + 1 if type(p.grad) == type(None) else check\n",
    "        if check > 0:\n",
    "            # initialize the grad fields properly\n",
    "            dummy_loss = self.regularization_loss(self.get_params())\n",
    "            dummy_loss.backward()  # this would initialize required variables\n",
    "        if flat_grad:\n",
    "            offset = 0\n",
    "            grad = to_device(grad, self.use_gpu)\n",
    "            for p in self.model.parameters():\n",
    "                this_grad = grad[offset:offset + p.nelement()].view(p.size())\n",
    "                p.grad.copy_(this_grad)\n",
    "                offset += p.nelement()\n",
    "        else:\n",
    "            for i, p in enumerate(self.model.parameters()):\n",
    "                p.grad = grad[i]\n",
    "        self.outer_opt.step()\n",
    "\n",
    "    def matrix_evaluator(self, task, lam, regu_coef=1.0, lam_damping=10.0, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Constructor function that can be given to CG optimizer\n",
    "        Works for both type(lam) == float and type(lam) == np.ndarray\n",
    "        \"\"\"\n",
    "        if type(lam) == np.ndarray:\n",
    "            lam = to_device(lam, self.use_gpu)\n",
    "        def evaluator(v):\n",
    "            hvp = self.hessian_vector_product(task, v, x=x, y=y)\n",
    "            Av = (1.0 + regu_coef) * v + hvp / (lam + lam_damping)\n",
    "            return Av\n",
    "        return evaluator\n",
    "\n",
    "    def hessian_vector_product(self, task, vector, params=None, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Performs hessian vector product on the train set in task with the provided vector\n",
    "        \"\"\"\n",
    "        if x is not None and y is not None:\n",
    "            xt, yt = x, y\n",
    "        else:\n",
    "            xt, yt = task['x_train'], task['y_train']\n",
    "        if params is not None:\n",
    "            self.set_params(params)\n",
    "        tloss = self.get_loss(xt, yt)\n",
    "        grad_ft = torch.autograd.grad(tloss, self.model.parameters(), create_graph=True)\n",
    "        flat_grad = torch.cat([g.contiguous().view(-1) for g in grad_ft])\n",
    "        vec = to_device(vector, self.use_gpu)\n",
    "        h = torch.sum(flat_grad * vec)\n",
    "        hvp = torch.autograd.grad(h, self.model.parameters())\n",
    "        hvp_flat = torch.cat([g.contiguous().view(-1) for g in hvp])\n",
    "        return hvp_flat\n",
    "\n",
    "\n",
    "def make_fc_network(in_dim=1, out_dim=1, hidden_sizes=(40,40), float16=False):\n",
    "    non_linearity = nn.ReLU()\n",
    "    model = nn.Sequential()\n",
    "    model.add_module('fc_0', nn.Linear(in_dim, hidden_sizes[0]))\n",
    "    model.add_module('nl_0', non_linearity)\n",
    "    model.add_module('fc_1', nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "    model.add_module('nl_1', non_linearity)\n",
    "    model.add_module('fc_2', nn.Linear(hidden_sizes[1], out_dim))\n",
    "    if float16:\n",
    "        return model.half()\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "    \n",
    "def make_conv_network(in_channels, out_dim, task='Omniglot', filter_size=32):\n",
    "    assert task == 'Omniglot' or 'MiniImageNet'\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    if task == 'MiniImageNet':\n",
    "        model = model_imagenet_arch(in_channels, out_dim, filter_size)\n",
    "        \n",
    "    elif task == 'Omniglot':\n",
    "        num_filters = 64\n",
    "        conv_stride = 2\n",
    "        pool_stride = None\n",
    "    \n",
    "        model.add_module('conv1', nn.Conv2d(in_channels=in_channels, out_channels=num_filters,\n",
    "                                            kernel_size=3, stride=conv_stride, padding=1))\n",
    "        model.add_module('BN1', nn.BatchNorm2d(num_filters, track_running_stats=False))\n",
    "        model.add_module('relu1', nn.ReLU())\n",
    "        model.add_module('conv2', nn.Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                                            kernel_size=3, stride=conv_stride, padding=1))\n",
    "        model.add_module('BN2', nn.BatchNorm2d(num_filters, track_running_stats=False))\n",
    "        model.add_module('relu2', nn.ReLU())\n",
    "        model.add_module('pad2', nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "        model.add_module('conv3', nn.Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                                            kernel_size=3, stride=conv_stride, padding=1))\n",
    "        model.add_module('BN3', nn.BatchNorm2d(num_filters, track_running_stats=False))\n",
    "        model.add_module('relu3', nn.ReLU())\n",
    "        model.add_module('conv4', nn.Conv2d(in_channels=num_filters, out_channels=num_filters,\n",
    "                                        kernel_size=3, stride=conv_stride, padding=1))\n",
    "        model.add_module('BN4', nn.BatchNorm2d(num_filters, track_running_stats=False))\n",
    "        model.add_module('relu4', nn.ReLU())\n",
    "        model.add_module('flatten', Flatten())\n",
    "        model.add_module('fc1', nn.Linear(2*2*num_filters, out_dim))\n",
    "        \n",
    "    for layer in [model.conv1, model.conv2, model.conv3, model.conv4, model.fc1]:\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=1.73)\n",
    "        try:\n",
    "            torch.nn.init.uniform_(layer.bias, a=0.0, b=0.05)\n",
    "        except:\n",
    "            print(\"Bias layer not detected for layer:\", layer)\n",
    "            pass\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def model_imagenet_arch(in_channels, out_dim, num_filters=32, batch_norm=True, bias=True):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11dbe0f0-2259-422e-936a-1c80d9904545",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m backbone\u001b[38;5;241m.\u001b[39mBottleneckBlock\u001b[38;5;241m.\u001b[39mmaml \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m backbone\u001b[38;5;241m.\u001b[39mResNet\u001b[38;5;241m.\u001b[39mmaml \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m combined_network       \u001b[38;5;241m=\u001b[39m backbone\u001b[38;5;241m.\u001b[39mCombinedNetwork(\u001b[43mbb\u001b[49m, simple_net)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m MAML(combined_network, n_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, approx\u001b[38;5;241m=\u001b[39m(params\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaml_approx\u001b[39m\u001b[38;5;124m'\u001b[39m), problem \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: bb\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m}])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bb' is not defined"
     ]
    }
   ],
   "source": [
    "backbone.ConvBlock.maml = True\n",
    "backbone.SimpleBlock.maml = True\n",
    "backbone.BottleneckBlock.maml = True\n",
    "backbone.ResNet.maml = True\n",
    "combined_network       = backbone.CombinedNetwork(bb, simple_net).cuda()\n",
    "model = MAML(combined_network, n_support=9, approx=(params.method == 'maml_approx'), problem = \"regression\").cuda()\n",
    "optimizer = torch.optim.Adam([{'params': bb.parameters(), 'lr': 0.001}])\n",
    "for epoch in range(params.stop_epoch):\n",
    "    model.train_loop_regression(epoch, optimizer, nb_batch_of_batches = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13777161-e750-41ab-a417-fa269ebcba9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
