{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f72d7b7-6c46-4bff-bf69-0ab5a0784ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original packages\n",
    "import backbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "## Our packages\n",
    "import gpytorch\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "from statistics import mean\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "\n",
    "\n",
    "class UnLiMiTDI(nn.Module):\n",
    "    def __init__(self, conv_net, diff_net):\n",
    "        super(UnLiMiTDI, self).__init__()\n",
    "        ## GP parameters\n",
    "        self.feature_extractor = conv_net\n",
    "        self.diff_net = diff_net  #Differentiable network\n",
    "        self.get_model_likelihood_mll() #Init model, likelihood, and mll\n",
    "\n",
    "    def get_model_likelihood_mll(self, train_x=None, train_y=None):\n",
    "        if(train_x is None): train_x=torch.ones(19, 2916).cuda()\n",
    "        if(train_y is None): train_y=torch.ones(19).cuda()\n",
    "\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = ExactGPLayer(train_x=train_x, train_y=train_y, likelihood=likelihood, diff_net = self.diff_net, kernel='NTKcossim')\n",
    "\n",
    "        self.model      = model.cuda()\n",
    "        self.likelihood = likelihood.cuda()\n",
    "        self.mll        = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model).cuda()\n",
    "        self.mse        = nn.MSELoss()\n",
    "\n",
    "        return self.model, self.likelihood, self.mll\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def train_loop(self, epoch, optimizer):\n",
    "        batch, batch_labels = get_batch(train_people)\n",
    "        batch, batch_labels = batch.cuda(), batch_labels.cuda()\n",
    "        for inputs, labels in zip(batch, batch_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_conv = self.feature_extractor(inputs)\n",
    "            self.model.set_train_data(inputs=inputs_conv, targets=labels - self.diff_net(inputs_conv).reshape(-1))  \n",
    "            predictions = self.model(inputs_conv)\n",
    "            loss = -self.mll(predictions, self.model.train_targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = self.mse(predictions.mean, labels)\n",
    "\n",
    "            if (epoch%10==0):\n",
    "                print('[%d] - Loss: %.3f  MSE: %.3f noise: %.3f' % (\n",
    "                    epoch, loss.item(), mse.item(),\n",
    "                    self.model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "    def test_loop(self, n_support, optimizer=None): # no optimizer needed for GP\n",
    "        inputs, targets = get_batch(test_people)\n",
    "\n",
    "        support_ind = list(np.random.choice(list(range(19)), replace=False, size=n_support))\n",
    "        query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "        x_all = inputs.cuda()\n",
    "        y_all = targets.cuda()\n",
    "\n",
    "        x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "        y_support = targets[:,support_ind].cuda()\n",
    "\n",
    "        # choose a random test person\n",
    "        n = np.random.randint(0, len(test_people)-1)\n",
    "    \n",
    "        x_conv_support = self.feature_extractor(x_support[n]).detach()\n",
    "        self.model.set_train_data(inputs=x_conv_support, targets=y_support[n] - self.diff_net(x_conv_support).reshape(-1), strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_conv_query = self.feature_extractor(x_all[n]).detach()\n",
    "            pred    = self.likelihood(self.model(x_conv_query))\n",
    "            lower, upper = pred.confidence_region() #2 standard deviations above and below the mean\n",
    "            lower += self.diff_net(x_conv_query).reshape(-1)\n",
    "            upper += self.diff_net(x_conv_query).reshape(-1)\n",
    "        mse = self.mse(pred.mean + self.diff_net(self.feature_extractor(x_all[n])).reshape(-1), y_all[n])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def save_checkpoint(self, checkpoint):\n",
    "        # save state\n",
    "        gp_state_dict         = self.model.state_dict()\n",
    "        likelihood_state_dict = self.likelihood.state_dict()\n",
    "        conv_net_state_dict   = self.feature_extractor.state_dict()\n",
    "        diff_net_state_dict   = self.diff_net.state_dict()\n",
    "        torch.save({'gp': gp_state_dict, 'likelihood': likelihood_state_dict, 'conv_net':conv_net_state_dict, 'diff_net':diff_net_state_dict}, checkpoint)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        self.model.load_state_dict(ckpt['gp'])\n",
    "        self.likelihood.load_state_dict(ckpt['likelihood'])\n",
    "        self.feature_extractor.load_state_dict(ckpt['conv_net'])\n",
    "        self.diff_net.load_state_dict(ckpt['diff_net'])\n",
    "\n",
    "        \n",
    "###################\n",
    "#NTKernel\n",
    "###################\n",
    "        \n",
    "class NTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        result = jac1@jac2.T\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "    \n",
    "    \n",
    "    \n",
    "###################\n",
    "#NTKernel CosSim\n",
    "###################\n",
    "class CosSimNTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(CosSimNTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1T = self.compute_jacobian(x1).T\n",
    "        jac1T_norm = jac1T.norm(dim=0, keepdim=True)\n",
    "        jac1T_normalized = jac1T/jac1T_norm\n",
    "        #print(jac1.shape)\n",
    "        #print(jac1.norm(dim=0, keepdim=True).shape)\n",
    "        jac2T = self.compute_jacobian(x2).T if x1 is not x2 else jac1T\n",
    "        jac2T_norm = jac2T.norm(dim=0, keepdim=True)\n",
    "        jac2T_normalized = jac2T/jac2T_norm\n",
    "        \n",
    "        result = self.alpha * jac1T_normalized.T@jac2T_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "    \n",
    "###################\n",
    "#GP\n",
    "###################    \n",
    "class ExactGPLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, diff_net, kernel='NTK'):\n",
    "        super(ExactGPLayer, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gpytorch.means.ConstantMean()\n",
    "\n",
    "        ## NTKernel\n",
    "        if(kernel=='NTK'):\n",
    "            self.covar_module = NTKernel(diff_net)\n",
    "        elif(kernel=='NTKcossim'):\n",
    "            self.covar_module = CosSimNTKernel(diff_net)        \n",
    "        else:\n",
    "            raise ValueError(\"[ERROR] the kernel '\" + str(kernel) + \"' is not supported for regression, use 'rbf' or 'spectral'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e20314de-5757-4036-9133-b9dd4ccc9947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - Loss: 0.828  MSE: 0.000 noise: 0.693\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.815  MSE: 0.000 noise: 0.681\n",
      "[10] - Loss: 0.772  MSE: 0.219 noise: 0.580\n",
      "[10] - Loss: 0.799  MSE: 0.219 noise: 0.579\n",
      "[10] - Loss: 0.799  MSE: 0.219 noise: 0.579\n",
      "[10] - Loss: 0.799  MSE: 0.220 noise: 0.578\n",
      "[10] - Loss: 0.831  MSE: 0.220 noise: 0.578\n",
      "[10] - Loss: 0.819  MSE: 0.220 noise: 0.577\n",
      "[10] - Loss: 0.790  MSE: 0.220 noise: 0.577\n",
      "[10] - Loss: 0.785  MSE: 0.220 noise: 0.576\n",
      "[10] - Loss: 0.803  MSE: 0.221 noise: 0.576\n",
      "[10] - Loss: 0.795  MSE: 0.221 noise: 0.575\n",
      "[10] - Loss: 0.803  MSE: 0.221 noise: 0.575\n",
      "[10] - Loss: 0.804  MSE: 0.221 noise: 0.575\n",
      "[10] - Loss: 0.778  MSE: 0.221 noise: 0.574\n",
      "[10] - Loss: 0.798  MSE: 0.221 noise: 0.574\n",
      "[10] - Loss: 0.769  MSE: 0.222 noise: 0.573\n",
      "[10] - Loss: 0.765  MSE: 0.222 noise: 0.573\n",
      "[10] - Loss: 0.804  MSE: 0.222 noise: 0.572\n",
      "[10] - Loss: 0.804  MSE: 0.222 noise: 0.572\n",
      "[10] - Loss: 0.788  MSE: 0.222 noise: 0.571\n",
      "[10] - Loss: 0.845  MSE: 0.223 noise: 0.571\n",
      "[10] - Loss: 0.851  MSE: 0.223 noise: 0.570\n",
      "[10] - Loss: 0.782  MSE: 0.223 noise: 0.570\n",
      "[10] - Loss: 0.765  MSE: 0.223 noise: 0.570\n",
      "[10] - Loss: 0.767  MSE: 0.223 noise: 0.569\n",
      "[20] - Loss: 0.696  MSE: 0.575 noise: 0.476\n",
      "[20] - Loss: 0.708  MSE: 0.575 noise: 0.476\n",
      "[20] - Loss: 0.682  MSE: 0.575 noise: 0.476\n",
      "[20] - Loss: 0.693  MSE: 0.575 noise: 0.475\n",
      "[20] - Loss: 0.694  MSE: 0.575 noise: 0.475\n",
      "[20] - Loss: 0.781  MSE: 0.575 noise: 0.475\n",
      "[20] - Loss: 0.716  MSE: 0.575 noise: 0.474\n",
      "[20] - Loss: 0.708  MSE: 0.575 noise: 0.474\n",
      "[20] - Loss: 0.719  MSE: 0.575 noise: 0.473\n",
      "[20] - Loss: 0.698  MSE: 0.575 noise: 0.473\n",
      "[20] - Loss: 0.691  MSE: 0.575 noise: 0.473\n",
      "[20] - Loss: 0.692  MSE: 0.576 noise: 0.472\n",
      "[20] - Loss: 0.716  MSE: 0.576 noise: 0.472\n",
      "[20] - Loss: 0.760  MSE: 0.576 noise: 0.472\n",
      "[20] - Loss: 0.717  MSE: 0.576 noise: 0.471\n",
      "[20] - Loss: 0.670  MSE: 0.576 noise: 0.471\n",
      "[20] - Loss: 0.754  MSE: 0.577 noise: 0.470\n",
      "[20] - Loss: 0.822  MSE: 0.577 noise: 0.470\n",
      "[20] - Loss: 0.739  MSE: 0.577 noise: 0.470\n",
      "[20] - Loss: 0.842  MSE: 0.577 noise: 0.469\n",
      "[20] - Loss: 0.786  MSE: 0.577 noise: 0.469\n",
      "[20] - Loss: 0.685  MSE: 0.577 noise: 0.469\n",
      "[20] - Loss: 0.674  MSE: 0.577 noise: 0.468\n",
      "[20] - Loss: 0.674  MSE: 0.577 noise: 0.468\n",
      "[30] - Loss: 0.703  MSE: 0.330 noise: 0.387\n",
      "[30] - Loss: 0.670  MSE: 0.330 noise: 0.386\n",
      "[30] - Loss: 0.665  MSE: 0.330 noise: 0.386\n",
      "[30] - Loss: 0.659  MSE: 0.330 noise: 0.386\n",
      "[30] - Loss: 0.658  MSE: 0.330 noise: 0.385\n",
      "[30] - Loss: 0.709  MSE: 0.330 noise: 0.385\n",
      "[30] - Loss: 0.627  MSE: 0.330 noise: 0.385\n",
      "[30] - Loss: 0.703  MSE: 0.331 noise: 0.384\n",
      "[30] - Loss: 0.620  MSE: 0.331 noise: 0.384\n",
      "[30] - Loss: 0.587  MSE: 0.332 noise: 0.384\n",
      "[30] - Loss: 0.622  MSE: 0.332 noise: 0.383\n",
      "[30] - Loss: 0.638  MSE: 0.333 noise: 0.383\n",
      "[30] - Loss: 0.586  MSE: 0.333 noise: 0.383\n",
      "[30] - Loss: 0.615  MSE: 0.334 noise: 0.382\n",
      "[30] - Loss: 0.562  MSE: 0.334 noise: 0.382\n",
      "[30] - Loss: 0.557  MSE: 0.335 noise: 0.382\n",
      "[30] - Loss: 0.650  MSE: 0.335 noise: 0.381\n",
      "[30] - Loss: 0.620  MSE: 0.336 noise: 0.381\n",
      "[30] - Loss: 0.591  MSE: 0.336 noise: 0.381\n",
      "[30] - Loss: 0.707  MSE: 0.336 noise: 0.381\n",
      "[30] - Loss: 0.617  MSE: 0.337 noise: 0.380\n",
      "[30] - Loss: 0.546  MSE: 0.337 noise: 0.380\n",
      "[30] - Loss: 0.550  MSE: 0.337 noise: 0.380\n",
      "[30] - Loss: 0.565  MSE: 0.338 noise: 0.379\n",
      "[40] - Loss: 0.477  MSE: 0.335 noise: 0.313\n",
      "[40] - Loss: 0.464  MSE: 0.335 noise: 0.313\n",
      "[40] - Loss: 0.465  MSE: 0.335 noise: 0.313\n",
      "[40] - Loss: 0.480  MSE: 0.335 noise: 0.312\n",
      "[40] - Loss: 0.495  MSE: 0.335 noise: 0.312\n",
      "[40] - Loss: 0.549  MSE: 0.335 noise: 0.312\n",
      "[40] - Loss: 0.488  MSE: 0.335 noise: 0.311\n",
      "[40] - Loss: 0.530  MSE: 0.336 noise: 0.311\n",
      "[40] - Loss: 0.466  MSE: 0.336 noise: 0.311\n",
      "[40] - Loss: 0.474  MSE: 0.336 noise: 0.311\n",
      "[40] - Loss: 0.493  MSE: 0.337 noise: 0.310\n",
      "[40] - Loss: 0.499  MSE: 0.337 noise: 0.310\n",
      "[40] - Loss: 0.466  MSE: 0.337 noise: 0.310\n",
      "[40] - Loss: 0.481  MSE: 0.337 noise: 0.309\n",
      "[40] - Loss: 0.442  MSE: 0.337 noise: 0.309\n",
      "[40] - Loss: 0.449  MSE: 0.338 noise: 0.309\n",
      "[40] - Loss: 0.486  MSE: 0.338 noise: 0.309\n",
      "[40] - Loss: 0.478  MSE: 0.338 noise: 0.308\n",
      "[40] - Loss: 0.492  MSE: 0.338 noise: 0.308\n",
      "[40] - Loss: 0.557  MSE: 0.338 noise: 0.308\n",
      "[40] - Loss: 0.505  MSE: 0.338 noise: 0.307\n",
      "[40] - Loss: 0.437  MSE: 0.338 noise: 0.307\n",
      "[40] - Loss: 0.436  MSE: 0.337 noise: 0.307\n",
      "[40] - Loss: 0.442  MSE: 0.337 noise: 0.307\n",
      "[50] - Loss: 0.318  MSE: 0.131 noise: 0.251\n",
      "[50] - Loss: 0.361  MSE: 0.131 noise: 0.251\n",
      "[50] - Loss: 0.361  MSE: 0.132 noise: 0.250\n",
      "[50] - Loss: 0.336  MSE: 0.132 noise: 0.250\n",
      "[50] - Loss: 0.380  MSE: 0.131 noise: 0.250\n",
      "[50] - Loss: 0.371  MSE: 0.131 noise: 0.250\n",
      "[50] - Loss: 0.358  MSE: 0.131 noise: 0.250\n",
      "[50] - Loss: 0.355  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.383  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.392  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.376  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.387  MSE: 0.131 noise: 0.248\n",
      "[50] - Loss: 0.353  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.350  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.321  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.329  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.357  MSE: 0.130 noise: 0.247\n",
      "[50] - Loss: 0.342  MSE: 0.130 noise: 0.247\n",
      "[50] - Loss: 0.367  MSE: 0.130 noise: 0.247\n",
      "[50] - Loss: 0.408  MSE: 0.129 noise: 0.247\n",
      "[50] - Loss: 0.368  MSE: 0.129 noise: 0.246\n",
      "[50] - Loss: 0.337  MSE: 0.129 noise: 0.246\n",
      "[50] - Loss: 0.318  MSE: 0.129 noise: 0.246\n",
      "[50] - Loss: 0.324  MSE: 0.129 noise: 0.246\n",
      "[60] - Loss: 0.464  MSE: 0.569 noise: 0.201\n",
      "[60] - Loss: 0.312  MSE: 0.569 noise: 0.201\n",
      "[60] - Loss: 0.298  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.297  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.319  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.506  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.317  MSE: 0.570 noise: 0.200\n",
      "[60] - Loss: 0.364  MSE: 0.570 noise: 0.200\n",
      "[60] - Loss: 0.285  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.445  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.349  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.517  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.256  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.303  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.245  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.232  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.324  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.367  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.335  MSE: 0.570 noise: 0.198\n",
      "[60] - Loss: 0.484  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.362  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.268  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.250  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.270  MSE: 0.570 noise: 0.197\n",
      "[70] - Loss: 0.147  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.125  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.168  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.103  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.113  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.124  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.095  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.129  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.098  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.151  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.114  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.116  MSE: 0.059 noise: 0.159\n",
      "[70] - Loss: 0.092  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.098  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.122  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.090  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.098  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.133  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.167  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.090  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.109  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.080  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.084  MSE: 0.057 noise: 0.157\n",
      "[70] - Loss: 0.077  MSE: 0.057 noise: 0.157\n",
      "[80] - Loss: 0.051  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.054  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.061  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.031  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.119  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.079  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.038  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.077  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.008  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.097  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.096  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.087  MSE: 0.223 noise: 0.127\n",
      "[80] - Loss: 0.055  MSE: 0.223 noise: 0.127\n",
      "[80] - Loss: 0.067  MSE: 0.223 noise: 0.127\n",
      "[80] - Loss: 0.025  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.031  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.054  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.030  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.047  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.168  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.069  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.046  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: -0.004  MSE: 0.223 noise: 0.125\n",
      "[80] - Loss: 0.023  MSE: 0.223 noise: 0.125\n",
      "[90] - Loss: -0.061  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.084  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.027  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.104  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.029  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.069  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.058  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: 0.007  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.055  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.043  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.057  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.044  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.059  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.031  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.078  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.094  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.101  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.085  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.061  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.018  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.065  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.108  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.120  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.084  MSE: 0.114 noise: 0.099\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import configs\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "from io_utils import parse_args_regression, get_resume_file\n",
    "from methods.DKT_regression import DKT\n",
    "from methods.feature_transfer_regression import FeatureTransfer\n",
    "import backbone\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "bb           = backbone.Conv3().cuda()\n",
    "simple_net   = backbone.simple_net().cuda()\n",
    "\n",
    "model = UnLiMiTDI(bb, simple_net).cuda()\n",
    "optimizer = torch.optim.Adam([{'params': model.model.parameters(), 'lr': 0.001},\n",
    "                                {'params': model.feature_extractor.parameters(), 'lr': 0.001}])\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train_loop(epoch, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841204c4-4d98-4421-ac24-4541d5728551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Average MSE: 0.05402122889645398 +- 0.03329951535135017\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "mse_list = []\n",
    "for epoch in range(10):\n",
    "    mse = float(model.test_loop(5, optimizer).cpu().detach().numpy())\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f55a8f-3de1-454a-b802-8ccfd2c03dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original packages\n",
    "import backbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "## Our packages\n",
    "import gpytorch\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "from statistics import mean\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "\n",
    "class UnLiMiTDproj(nn.Module):\n",
    "    def __init__(self, conv_net, diff_net, P):\n",
    "        super(UnLiMiTDproj, self).__init__()\n",
    "        ## GP parameters\n",
    "        self.feature_extractor = conv_net\n",
    "        self.diff_net = diff_net  #Differentiable network\n",
    "        \n",
    "        input_dimension = sum(p.numel() for p in diff_net.parameters())\n",
    "        self.P = P\n",
    "        self.get_model_likelihood_mll() #Init model, likelihood, and mll\n",
    "\n",
    "    def get_model_likelihood_mll(self, train_x=None, train_y=None):\n",
    "        if(train_x is None): train_x=torch.ones(19, 2916).cuda()\n",
    "        if(train_y is None): train_y=torch.ones(19).cuda()\n",
    "\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = ExactGPLayer(train_x=train_x, train_y=train_y, likelihood=likelihood, diff_net = self.diff_net, P = self.P, kernel='NTKcossim')\n",
    "\n",
    "        self.model      = model.cuda()\n",
    "        self.likelihood = likelihood.cuda()\n",
    "        self.mll        = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model).cuda()\n",
    "        self.mse        = nn.MSELoss()\n",
    "\n",
    "        return self.model, self.likelihood, self.mll\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def train_loop(self, epoch, optimizer):\n",
    "        batch, batch_labels = get_batch(train_people)\n",
    "        batch, batch_labels = batch.cuda(), batch_labels.cuda()\n",
    "        for inputs, labels in zip(batch, batch_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_conv = self.feature_extractor(inputs)\n",
    "            self.model.set_train_data(inputs=inputs_conv, targets=labels - self.diff_net(inputs_conv).reshape(-1))  \n",
    "            predictions = self.model(inputs_conv)\n",
    "            loss = -self.mll(predictions, self.model.train_targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = self.mse(predictions.mean, labels)\n",
    "\n",
    "            if (epoch%10==0):\n",
    "                print('[%d] - Loss: %.3f  MSE: %.3f noise: %.3f' % (\n",
    "                    epoch, loss.item(), mse.item(),\n",
    "                    self.model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "    def test_loop(self, n_support, optimizer=None): # no optimizer needed for GP\n",
    "        inputs, targets = get_batch(test_people)\n",
    "\n",
    "        support_ind = list(np.random.choice(list(range(19)), replace=False, size=n_support))\n",
    "        query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "        x_all = inputs.cuda()\n",
    "        y_all = targets.cuda()\n",
    "\n",
    "        x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "        y_support = targets[:,support_ind].cuda()\n",
    "\n",
    "        # choose a random test person\n",
    "        n = np.random.randint(0, len(test_people)-1)\n",
    "    \n",
    "        x_conv_support = self.feature_extractor(x_support[n]).detach()\n",
    "        self.model.set_train_data(inputs=x_conv_support, targets=y_support[n] - self.diff_net(x_conv_support).reshape(-1), strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_conv_query = self.feature_extractor(x_all[n]).detach()\n",
    "            pred    = self.likelihood(self.model(x_conv_query))\n",
    "            lower, upper = pred.confidence_region() #2 standard deviations above and below the mean\n",
    "            lower += self.diff_net(x_conv_query).reshape(-1)\n",
    "            upper += self.diff_net(x_conv_query).reshape(-1)\n",
    "        mse = self.mse(pred.mean + self.diff_net(self.feature_extractor(x_all[n])).reshape(-1), y_all[n])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def save_checkpoint(self, checkpoint):\n",
    "        # save state\n",
    "        gp_state_dict         = self.model.state_dict()\n",
    "        likelihood_state_dict = self.likelihood.state_dict()\n",
    "        conv_net_state_dict   = self.feature_extractor.state_dict()\n",
    "        diff_net_state_dict   = self.diff_net.state_dict()\n",
    "        torch.save({\n",
    "            'gp': gp_state_dict,\n",
    "            'likelihood': likelihood_state_dict,\n",
    "            'conv_net': conv_net_state_dict,\n",
    "            'diff_net': diff_net_state_dict,\n",
    "            'proj_matrix': self.P  # Save the tensor directly\n",
    "        }, checkpoint)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        if 'covar_module.scaling_param' not in ckpt['gp'].keys():\n",
    "            ckpt['gp']['covar_module.scaling_param'] = torch.ones(self.P.shape[0]).cuda()\n",
    "        self.model.load_state_dict(ckpt['gp'])\n",
    "        self.likelihood.load_state_dict(ckpt['likelihood'])\n",
    "        self.feature_extractor.load_state_dict(ckpt['conv_net'])\n",
    "        self.diff_net.load_state_dict(ckpt['diff_net'])\n",
    "        if 'proj_matrix' in ckpt.keys():\n",
    "            self.P = ckpt['proj_matrix']\n",
    "        \n",
    "        print(f\"Total number of param that requires grad : {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "\n",
    "# ##################\n",
    "# NTKernel\n",
    "# ##################\n",
    "\n",
    "class NTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, P, **kwargs):\n",
    "        super(NTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        self.P = P # Projection matrix\n",
    "        \n",
    "        # Add subspace_dimension scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(P.shape[0]))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        D = torch.diag(torch.pow(self.scaling_param, 2))\n",
    "        \n",
    "        result = torch.chain_matmul(jac1, self.P.T, D, self.P, jac2.T)\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "\n",
    "    \n",
    "# ##################\n",
    "# NTKernel CosSim\n",
    "# ##################\n",
    "\n",
    "class CosSimNTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, P, **kwargs):\n",
    "        super(CosSimNTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        self.P = P # Projection matrix\n",
    "        \n",
    "        # Add subspace_dimension scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(P.shape[0]))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        D = torch.diag(self.scaling_param)\n",
    "        \n",
    "        result_1 = torch.chain_matmul(D, self.P, jac1.T)\n",
    "        result_2 = torch.chain_matmul(D, self.P, jac2.T)\n",
    "        \n",
    "        result_1_norm = result_1.norm(dim=0, keepdim=True)\n",
    "        result_1_normalized = result_1/result_1_norm\n",
    "        #print(result_1.shape)\n",
    "        #print(result_1.norm(dim=0, keepdim=True).shape)\n",
    "        result_2_norm = result_2.norm(dim=0, keepdim=True)\n",
    "        result_2_normalized = result_2/result_2_norm\n",
    "        \n",
    "        result = self.alpha * result_1_normalized.T@result_2_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "# ##################\n",
    "# GP layer\n",
    "# ##################\n",
    "class ExactGPLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, diff_net, P, kernel='NTK'):\n",
    "        super(ExactGPLayer, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gpytorch.means.ConstantMean()\n",
    "\n",
    "        ## NTKernel\n",
    "        if(kernel=='NTK'):\n",
    "            self.covar_module = NTKernel_proj(diff_net, P)\n",
    "        elif(kernel=='NTKcossim'):\n",
    "            self.covar_module = CosSimNTKernel_proj(diff_net, P)  \n",
    "        else:\n",
    "            raise ValueError(\"[ERROR] the kernel '\" + str(kernel) + \"' is not supported for regression, use 'rbf' or 'spectral'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1800d31-2dc1-4ef9-b1a0-8cc5b197cfca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118361\n",
      "U shape: torch.Size([118361, 402])\n",
      "Index tensor: tensor([401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388,\n",
      "        387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374,\n",
      "        373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360,\n",
      "        359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347, 346,\n",
      "        345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334, 333, 332,\n",
      "        331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321, 320, 319, 318,\n",
      "        317, 316, 315, 314, 313, 312, 311, 310, 309, 308, 307, 306, 305, 304,\n",
      "        303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292, 291, 290,\n",
      "        289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279, 278, 277, 276,\n",
      "        275, 274, 273, 272, 271, 270, 269, 268, 267, 266, 265, 264, 263, 262,\n",
      "        261, 260, 259, 258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248,\n",
      "        247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234,\n",
      "        233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220,\n",
      "        219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206,\n",
      "        205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192,\n",
      "        191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178,\n",
      "        177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164,\n",
      "        163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150,\n",
      "        149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136,\n",
      "        135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122,\n",
      "        121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108,\n",
      "        107, 106, 105, 104, 103, 102, 101, 100,  99,  98,  97,  96,  95,  94,\n",
      "         93,  92,  91,  90,  89,  88,  87,  86,  85,  84,  83,  82,  81,  80,\n",
      "         79,  78,  77,  76,  75,  74,  73,  72,  71,  70,  69,  68,  67,  66,\n",
      "         65,  64,  63,  62,  61,  60,  59,  58,  57,  56,  55,  54,  53,  52,\n",
      "         51,  50,  49,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,\n",
      "         37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,  24,\n",
      "         23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,  11,  10,\n",
      "          9,   8,   7,   6,   5,   4,   3,   2,   1,   0], device='cuda:0')\n",
      "Requested subspace dimension: 100\n",
      "Done sketching in 1.6125 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/functional.py:1626: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at ../aten/src/ATen/native/LinearAlgebra.cpp:1079.)\n",
      "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - Loss: 0.830  MSE: 0.000 noise: 0.693\n",
      "[0] - Loss: 0.828  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.830  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.825  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.827  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.825  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.681\n",
      "[10] - Loss: 0.743  MSE: 0.216 noise: 0.579\n",
      "[10] - Loss: 0.745  MSE: 0.216 noise: 0.579\n",
      "[10] - Loss: 0.745  MSE: 0.216 noise: 0.578\n",
      "[10] - Loss: 0.737  MSE: 0.216 noise: 0.578\n",
      "[10] - Loss: 0.757  MSE: 0.216 noise: 0.577\n",
      "[10] - Loss: 0.749  MSE: 0.216 noise: 0.577\n",
      "[10] - Loss: 0.737  MSE: 0.216 noise: 0.576\n",
      "[10] - Loss: 0.754  MSE: 0.216 noise: 0.576\n",
      "[10] - Loss: 0.737  MSE: 0.216 noise: 0.575\n",
      "[10] - Loss: 0.743  MSE: 0.216 noise: 0.575\n",
      "[10] - Loss: 0.742  MSE: 0.216 noise: 0.575\n",
      "[10] - Loss: 0.745  MSE: 0.216 noise: 0.574\n",
      "[10] - Loss: 0.741  MSE: 0.216 noise: 0.574\n",
      "[10] - Loss: 0.747  MSE: 0.216 noise: 0.573\n",
      "[10] - Loss: 0.738  MSE: 0.216 noise: 0.573\n",
      "[10] - Loss: 0.733  MSE: 0.216 noise: 0.572\n",
      "[10] - Loss: 0.735  MSE: 0.216 noise: 0.572\n",
      "[10] - Loss: 0.742  MSE: 0.216 noise: 0.571\n",
      "[10] - Loss: 0.743  MSE: 0.216 noise: 0.571\n",
      "[10] - Loss: 0.744  MSE: 0.216 noise: 0.571\n",
      "[10] - Loss: 0.739  MSE: 0.216 noise: 0.570\n",
      "[10] - Loss: 0.731  MSE: 0.216 noise: 0.570\n",
      "[10] - Loss: 0.733  MSE: 0.216 noise: 0.569\n",
      "[10] - Loss: 0.738  MSE: 0.216 noise: 0.569\n",
      "[20] - Loss: 0.638  MSE: 0.286 noise: 0.478\n",
      "[20] - Loss: 0.645  MSE: 0.286 noise: 0.477\n",
      "[20] - Loss: 0.658  MSE: 0.286 noise: 0.477\n",
      "[20] - Loss: 0.645  MSE: 0.286 noise: 0.477\n",
      "[20] - Loss: 0.644  MSE: 0.286 noise: 0.476\n",
      "[20] - Loss: 0.660  MSE: 0.286 noise: 0.476\n",
      "[20] - Loss: 0.638  MSE: 0.286 noise: 0.475\n",
      "[20] - Loss: 0.664  MSE: 0.286 noise: 0.475\n",
      "[20] - Loss: 0.639  MSE: 0.286 noise: 0.475\n",
      "[20] - Loss: 0.651  MSE: 0.286 noise: 0.474\n",
      "[20] - Loss: 0.640  MSE: 0.286 noise: 0.474\n",
      "[20] - Loss: 0.637  MSE: 0.286 noise: 0.473\n",
      "[20] - Loss: 0.636  MSE: 0.286 noise: 0.473\n",
      "[20] - Loss: 0.639  MSE: 0.286 noise: 0.473\n",
      "[20] - Loss: 0.639  MSE: 0.286 noise: 0.472\n",
      "[20] - Loss: 0.639  MSE: 0.286 noise: 0.472\n",
      "[20] - Loss: 0.645  MSE: 0.286 noise: 0.472\n",
      "[20] - Loss: 0.637  MSE: 0.286 noise: 0.471\n",
      "[20] - Loss: 0.641  MSE: 0.286 noise: 0.471\n",
      "[20] - Loss: 0.658  MSE: 0.286 noise: 0.470\n",
      "[20] - Loss: 0.646  MSE: 0.286 noise: 0.470\n",
      "[20] - Loss: 0.644  MSE: 0.286 noise: 0.470\n",
      "[20] - Loss: 0.630  MSE: 0.286 noise: 0.469\n",
      "[20] - Loss: 0.631  MSE: 0.286 noise: 0.469\n",
      "[30] - Loss: 0.563  MSE: 0.502 noise: 0.390\n",
      "[30] - Loss: 0.532  MSE: 0.502 noise: 0.390\n",
      "[30] - Loss: 0.524  MSE: 0.502 noise: 0.390\n",
      "[30] - Loss: 0.531  MSE: 0.502 noise: 0.389\n",
      "[30] - Loss: 0.544  MSE: 0.502 noise: 0.389\n",
      "[30] - Loss: 0.577  MSE: 0.502 noise: 0.389\n",
      "[30] - Loss: 0.519  MSE: 0.502 noise: 0.388\n",
      "[30] - Loss: 0.593  MSE: 0.502 noise: 0.388\n",
      "[30] - Loss: 0.525  MSE: 0.502 noise: 0.388\n",
      "[30] - Loss: 0.575  MSE: 0.502 noise: 0.387\n",
      "[30] - Loss: 0.538  MSE: 0.502 noise: 0.387\n",
      "[30] - Loss: 0.574  MSE: 0.502 noise: 0.387\n",
      "[30] - Loss: 0.526  MSE: 0.502 noise: 0.386\n",
      "[30] - Loss: 0.546  MSE: 0.502 noise: 0.386\n",
      "[30] - Loss: 0.519  MSE: 0.502 noise: 0.386\n",
      "[30] - Loss: 0.520  MSE: 0.502 noise: 0.385\n",
      "[30] - Loss: 0.545  MSE: 0.502 noise: 0.385\n",
      "[30] - Loss: 0.558  MSE: 0.502 noise: 0.385\n",
      "[30] - Loss: 0.531  MSE: 0.502 noise: 0.384\n",
      "[30] - Loss: 0.599  MSE: 0.502 noise: 0.384\n",
      "[30] - Loss: 0.544  MSE: 0.502 noise: 0.384\n",
      "[30] - Loss: 0.520  MSE: 0.502 noise: 0.383\n",
      "[30] - Loss: 0.523  MSE: 0.502 noise: 0.383\n",
      "[30] - Loss: 0.530  MSE: 0.502 noise: 0.383\n",
      "[40] - Loss: 0.399  MSE: 0.228 noise: 0.317\n",
      "[40] - Loss: 0.415  MSE: 0.228 noise: 0.317\n",
      "[40] - Loss: 0.408  MSE: 0.228 noise: 0.317\n",
      "[40] - Loss: 0.428  MSE: 0.228 noise: 0.316\n",
      "[40] - Loss: 0.405  MSE: 0.228 noise: 0.316\n",
      "[40] - Loss: 0.414  MSE: 0.228 noise: 0.316\n",
      "[40] - Loss: 0.415  MSE: 0.228 noise: 0.316\n",
      "[40] - Loss: 0.424  MSE: 0.228 noise: 0.315\n",
      "[40] - Loss: 0.393  MSE: 0.228 noise: 0.315\n",
      "[40] - Loss: 0.407  MSE: 0.228 noise: 0.315\n",
      "[40] - Loss: 0.406  MSE: 0.228 noise: 0.314\n",
      "[40] - Loss: 0.416  MSE: 0.228 noise: 0.314\n",
      "[40] - Loss: 0.508  MSE: 0.228 noise: 0.314\n",
      "[40] - Loss: 0.464  MSE: 0.228 noise: 0.314\n",
      "[40] - Loss: 0.401  MSE: 0.228 noise: 0.313\n",
      "[40] - Loss: 0.422  MSE: 0.228 noise: 0.313\n",
      "[40] - Loss: 0.426  MSE: 0.228 noise: 0.313\n",
      "[40] - Loss: 0.394  MSE: 0.228 noise: 0.313\n",
      "[40] - Loss: 0.388  MSE: 0.228 noise: 0.312\n",
      "[40] - Loss: 0.419  MSE: 0.228 noise: 0.312\n",
      "[40] - Loss: 0.415  MSE: 0.228 noise: 0.312\n",
      "[40] - Loss: 0.398  MSE: 0.228 noise: 0.311\n",
      "[40] - Loss: 0.409  MSE: 0.228 noise: 0.311\n",
      "[40] - Loss: 0.398  MSE: 0.228 noise: 0.311\n",
      "[50] - Loss: 0.441  MSE: 0.275 noise: 0.257\n",
      "[50] - Loss: 0.422  MSE: 0.275 noise: 0.257\n",
      "[50] - Loss: 0.453  MSE: 0.275 noise: 0.257\n",
      "[50] - Loss: 0.440  MSE: 0.275 noise: 0.257\n",
      "[50] - Loss: 0.417  MSE: 0.275 noise: 0.256\n",
      "[50] - Loss: 0.427  MSE: 0.275 noise: 0.256\n",
      "[50] - Loss: 0.376  MSE: 0.275 noise: 0.256\n",
      "[50] - Loss: 0.364  MSE: 0.275 noise: 0.256\n",
      "[50] - Loss: 0.310  MSE: 0.275 noise: 0.255\n",
      "[50] - Loss: 0.374  MSE: 0.275 noise: 0.255\n",
      "[50] - Loss: 0.318  MSE: 0.275 noise: 0.255\n",
      "[50] - Loss: 0.376  MSE: 0.275 noise: 0.255\n",
      "[50] - Loss: 0.501  MSE: 0.275 noise: 0.255\n",
      "[50] - Loss: 0.608  MSE: 0.275 noise: 0.254\n",
      "[50] - Loss: 0.361  MSE: 0.275 noise: 0.254\n",
      "[50] - Loss: 0.317  MSE: 0.275 noise: 0.254\n",
      "[50] - Loss: 0.429  MSE: 0.275 noise: 0.254\n",
      "[50] - Loss: 0.315  MSE: 0.275 noise: 0.254\n",
      "[50] - Loss: 0.322  MSE: 0.275 noise: 0.253\n",
      "[50] - Loss: 0.339  MSE: 0.275 noise: 0.253\n",
      "[50] - Loss: 0.334  MSE: 0.275 noise: 0.253\n",
      "[50] - Loss: 0.328  MSE: 0.275 noise: 0.253\n",
      "[50] - Loss: 0.332  MSE: 0.275 noise: 0.252\n",
      "[50] - Loss: 0.302  MSE: 0.275 noise: 0.252\n",
      "[60] - Loss: 0.227  MSE: 0.076 noise: 0.207\n",
      "[60] - Loss: 0.198  MSE: 0.076 noise: 0.207\n",
      "[60] - Loss: 0.275  MSE: 0.076 noise: 0.207\n",
      "[60] - Loss: 0.182  MSE: 0.076 noise: 0.206\n",
      "[60] - Loss: 0.235  MSE: 0.076 noise: 0.206\n",
      "[60] - Loss: 0.220  MSE: 0.076 noise: 0.206\n",
      "[60] - Loss: 0.246  MSE: 0.076 noise: 0.206\n",
      "[60] - Loss: 0.233  MSE: 0.076 noise: 0.206\n",
      "[60] - Loss: 0.193  MSE: 0.076 noise: 0.205\n",
      "[60] - Loss: 0.240  MSE: 0.076 noise: 0.205\n",
      "[60] - Loss: 0.176  MSE: 0.076 noise: 0.205\n",
      "[60] - Loss: 0.186  MSE: 0.076 noise: 0.205\n",
      "[60] - Loss: 0.196  MSE: 0.077 noise: 0.205\n",
      "[60] - Loss: 0.241  MSE: 0.077 noise: 0.204\n",
      "[60] - Loss: 0.188  MSE: 0.077 noise: 0.204\n",
      "[60] - Loss: 0.190  MSE: 0.077 noise: 0.204\n",
      "[60] - Loss: 0.186  MSE: 0.077 noise: 0.204\n",
      "[60] - Loss: 0.172  MSE: 0.077 noise: 0.204\n",
      "[60] - Loss: 0.183  MSE: 0.077 noise: 0.203\n",
      "[60] - Loss: 0.237  MSE: 0.077 noise: 0.203\n",
      "[60] - Loss: 0.163  MSE: 0.077 noise: 0.203\n",
      "[60] - Loss: 0.240  MSE: 0.077 noise: 0.203\n",
      "[60] - Loss: 0.196  MSE: 0.077 noise: 0.203\n",
      "[60] - Loss: 0.204  MSE: 0.076 noise: 0.202\n",
      "[70] - Loss: 0.050  MSE: 0.024 noise: 0.164\n",
      "[70] - Loss: 0.045  MSE: 0.024 noise: 0.164\n",
      "[70] - Loss: 0.059  MSE: 0.024 noise: 0.163\n",
      "[70] - Loss: 0.053  MSE: 0.024 noise: 0.163\n",
      "[70] - Loss: 0.085  MSE: 0.024 noise: 0.163\n",
      "[70] - Loss: 0.037  MSE: 0.024 noise: 0.163\n",
      "[70] - Loss: 0.078  MSE: 0.024 noise: 0.163\n",
      "[70] - Loss: 0.031  MSE: 0.024 noise: 0.163\n",
      "[70] - Loss: 0.031  MSE: 0.024 noise: 0.162\n",
      "[70] - Loss: 0.078  MSE: 0.024 noise: 0.162\n",
      "[70] - Loss: 0.026  MSE: 0.024 noise: 0.162\n",
      "[70] - Loss: 0.021  MSE: 0.024 noise: 0.162\n",
      "[70] - Loss: 0.037  MSE: 0.024 noise: 0.162\n",
      "[70] - Loss: 0.124  MSE: 0.024 noise: 0.162\n",
      "[70] - Loss: 0.034  MSE: 0.024 noise: 0.161\n",
      "[70] - Loss: 0.011  MSE: 0.024 noise: 0.161\n",
      "[70] - Loss: 0.012  MSE: 0.024 noise: 0.161\n",
      "[70] - Loss: 0.010  MSE: 0.024 noise: 0.161\n",
      "[70] - Loss: 0.027  MSE: 0.024 noise: 0.161\n",
      "[70] - Loss: 0.043  MSE: 0.024 noise: 0.161\n",
      "[70] - Loss: -0.007  MSE: 0.024 noise: 0.160\n",
      "[70] - Loss: 0.115  MSE: 0.024 noise: 0.160\n",
      "[70] - Loss: -0.010  MSE: 0.024 noise: 0.160\n",
      "[70] - Loss: -0.003  MSE: 0.024 noise: 0.160\n",
      "[80] - Loss: 0.016  MSE: 0.082 noise: 0.131\n",
      "[80] - Loss: -0.037  MSE: 0.082 noise: 0.131\n",
      "[80] - Loss: -0.038  MSE: 0.082 noise: 0.131\n",
      "[80] - Loss: -0.009  MSE: 0.082 noise: 0.131\n",
      "[80] - Loss: 0.007  MSE: 0.082 noise: 0.131\n",
      "[80] - Loss: 0.021  MSE: 0.082 noise: 0.131\n",
      "[80] - Loss: 0.046  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: 0.008  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: -0.011  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: 0.047  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: -0.016  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: -0.034  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: -0.036  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: 0.012  MSE: 0.082 noise: 0.130\n",
      "[80] - Loss: -0.056  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: -0.059  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: -0.043  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: -0.046  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: -0.025  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: 0.035  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: -0.041  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: 0.022  MSE: 0.082 noise: 0.129\n",
      "[80] - Loss: -0.029  MSE: 0.082 noise: 0.128\n",
      "[80] - Loss: -0.024  MSE: 0.082 noise: 0.128\n",
      "[90] - Loss: 0.008  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: -0.086  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: -0.074  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: 0.021  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: 0.030  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: -0.047  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: -0.071  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: 0.103  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: -0.089  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: 0.039  MSE: 0.260 noise: 0.104\n",
      "[90] - Loss: -0.089  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.091  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.088  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.101  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.069  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.147  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: 0.168  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.131  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.011  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.047  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.084  MSE: 0.260 noise: 0.103\n",
      "[90] - Loss: -0.032  MSE: 0.260 noise: 0.102\n",
      "[90] - Loss: -0.069  MSE: 0.260 noise: 0.102\n",
      "[90] - Loss: -0.068  MSE: 0.260 noise: 0.102\n"
     ]
    }
   ],
   "source": [
    "from projection import create_random_projection_matrix, proj_sketch\n",
    "\n",
    "# FIM proj search needs no gradient\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad_(False)\n",
    "optimizer = None\n",
    "# Batch preparation\n",
    "nb_batch_proj = 10\n",
    "    \n",
    "batches = []\n",
    "for _ in range(nb_batch_proj):\n",
    "    batch, batch_labels = get_batch(train_people)\n",
    "    for person_task in batch :\n",
    "        person_conv = model.feature_extractor(person_task.cuda()).detach()\n",
    "        batches.append(person_conv)  \n",
    "batches = torch.stack(batches)\n",
    "# FIM projection computation\n",
    "input_dimension = sum(p.numel() for p in simple_net.parameters())\n",
    "P = proj_sketch(model.diff_net, batches, 100).cuda()\n",
    "# Gradients back to training mode\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad_(True)\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    \n",
    "# Unlimitd-F training\n",
    "model = UnLiMiTDproj(bb, simple_net, P).cuda()\n",
    "optimizer = torch.optim.Adam([{'params': model.model.parameters(), 'lr': 0.001},\n",
    "                            {'params': model.feature_extractor.parameters(), 'lr': 0.001}])\n",
    "for epoch in range(100):\n",
    "    model.train_loop(epoch, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a052f1-c765-4ae4-9282-814b8b8192d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Average MSE: 0.0853831883519888 +- 0.06335430772426338\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "mse_list = []\n",
    "for epoch in range(10):\n",
    "    mse = float(model.test_loop(5, optimizer).cpu().detach().numpy())\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe9fafc6-c3c7-41eb-8c52-4de0f3d361e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original packages\n",
    "import backbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "## Our packages\n",
    "import gpytorch\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "from statistics import mean\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "\n",
    "\n",
    "class UnLiMiTDcov(nn.Module):\n",
    "    def __init__(self, conv_net, diff_net):\n",
    "        super(UnLiMiTDI, self).__init__()\n",
    "        ## GP parameters\n",
    "        self.feature_extractor = conv_net\n",
    "        self.diff_net = diff_net  #Differentiable network\n",
    "        self.get_model_likelihood_mll() #Init model, likelihood, and mll\n",
    "\n",
    "    def get_model_likelihood_mll(self, train_x=None, train_y=None):\n",
    "        if(train_x is None): train_x=torch.ones(19, 2916).cuda()\n",
    "        if(train_y is None): train_y=torch.ones(19).cuda()\n",
    "\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = ExactGPLayer(train_x=train_x, train_y=train_y, likelihood=likelihood, diff_net = self.diff_net, kernel='NTK')\n",
    "\n",
    "        self.model      = model.cuda()\n",
    "        self.likelihood = likelihood.cuda()\n",
    "        self.mll        = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model).cuda()\n",
    "        self.mse        = nn.MSELoss()\n",
    "\n",
    "        return self.model, self.likelihood, self.mll\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def train_loop(self, epoch, optimizer):\n",
    "        batch, batch_labels = get_batch(train_people)\n",
    "        batch, batch_labels = batch.cuda(), batch_labels.cuda()\n",
    "        for inputs, labels in zip(batch, batch_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_conv = self.feature_extractor(inputs)\n",
    "            self.model.set_train_data(inputs=inputs_conv, targets=labels - self.diff_net(inputs_conv).reshape(-1))  \n",
    "            predictions = self.model(inputs_conv)\n",
    "            loss = -self.mll(predictions, self.model.train_targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = self.mse(predictions.mean, labels)\n",
    "\n",
    "            if (epoch%10==0):\n",
    "                print('[%d] - Loss: %.3f  MSE: %.3f noise: %.3f' % (\n",
    "                    epoch, loss.item(), mse.item(),\n",
    "                    self.model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "    def test_loop(self, n_support, optimizer=None): # no optimizer needed for GP\n",
    "        inputs, targets = get_batch(test_people)\n",
    "\n",
    "        support_ind = list(np.random.choice(list(range(19)), replace=False, size=n_support))\n",
    "        query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "        x_all = inputs.cuda()\n",
    "        y_all = targets.cuda()\n",
    "\n",
    "        x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "        y_support = targets[:,support_ind].cuda()\n",
    "\n",
    "        # choose a random test person\n",
    "        n = np.random.randint(0, len(test_people)-1)\n",
    "    \n",
    "        x_conv_support = self.feature_extractor(x_support[n]).detach()\n",
    "        self.model.set_train_data(inputs=x_conv_support, targets=y_support[n] - self.diff_net(x_conv_support).reshape(-1), strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_conv_query = self.feature_extractor(x_all[n]).detach()\n",
    "            pred    = self.likelihood(self.model(x_conv_query))\n",
    "            lower, upper = pred.confidence_region() #2 standard deviations above and below the mean\n",
    "            lower += self.diff_net(x_conv_query).reshape(-1)\n",
    "            upper += self.diff_net(x_conv_query).reshape(-1)\n",
    "        mse = self.mse(pred.mean + self.diff_net(self.feature_extractor(x_all[n])).reshape(-1), y_all[n])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def save_checkpoint(self, checkpoint):\n",
    "        # save state\n",
    "        gp_state_dict         = self.model.state_dict()\n",
    "        likelihood_state_dict = self.likelihood.state_dict()\n",
    "        conv_net_state_dict   = self.feature_extractor.state_dict()\n",
    "        diff_net_state_dict   = self.diff_net.state_dict()\n",
    "        torch.save({'gp': gp_state_dict, 'likelihood': likelihood_state_dict, 'conv_net':conv_net_state_dict, 'diff_net':diff_net_state_dict}, checkpoint)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        self.model.load_state_dict(ckpt['gp'])\n",
    "        self.likelihood.load_state_dict(ckpt['likelihood'])\n",
    "        self.feature_extractor.load_state_dict(ckpt['conv_net'])\n",
    "        self.diff_net.load_state_dict(ckpt['diff_net'])\n",
    "\n",
    "        \n",
    "# ##################\n",
    "# NTKernel\n",
    "# ##################\n",
    "\n",
    "class NTKernelcov(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernelcov, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        # Add number of params scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(sum(p.numel() for p in simple_net.parameters())))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        D = torch.diag(torch.pow(self.scaling_param, 2))\n",
    "        \n",
    "        result = torch.chain_matmul(jac1, D, jac2.T)\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "\n",
    "    \n",
    "# ##################\n",
    "# NTKernel CosSim\n",
    "# ##################\n",
    "\n",
    "class CosSimNTKernelcov(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(CosSimNTKernelcov, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # Add subspace_dimension scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(sum(p.numel() for p in simple_net.parameters())))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        D = torch.diag(self.scaling_param)\n",
    "        \n",
    "        result_1 = torch.chain_matmul(D, jac1.T)\n",
    "        result_2 = torch.chain_matmul(D, jac2.T)\n",
    "        \n",
    "        result_1_norm = result_1.norm(dim=0, keepdim=True)\n",
    "        result_1_normalized = result_1/result_1_norm\n",
    "        #print(result_1.shape)\n",
    "        #print(result_1.norm(dim=0, keepdim=True).shape)\n",
    "        result_2_norm = result_2.norm(dim=0, keepdim=True)\n",
    "        result_2_normalized = result_2/result_2_norm\n",
    "        \n",
    "        result = self.alpha * result_1_normalized.T@result_2_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "    \n",
    "###################\n",
    "#GP\n",
    "###################    \n",
    "class ExactGPLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, diff_net, kernel='NTK'):\n",
    "        super(ExactGPLayer, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gpytorch.means.ConstantMean()\n",
    "\n",
    "        ## NTKernel\n",
    "        if(kernel=='NTK'):\n",
    "            self.covar_module = NTKernelcov(diff_net)\n",
    "        elif(kernel=='NTKcossim'):\n",
    "            self.covar_module = CosSimNTKernelcov(diff_net)        \n",
    "        else:\n",
    "            raise ValueError(\"[ERROR] the kernel '\" + str(kernel) + \"' is not supported for regression, use 'rbf' or 'spectral'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d41a6-6573-4604-8f45-4db33299b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "bb           = backbone.Conv3().cuda()\n",
    "simple_net   = backbone.simple_net().cuda()\n",
    "\n",
    "model = UnLiMiTDI(bb, simple_net).cuda()\n",
    "optimizer = torch.optim.Adam([{'params': model.model.parameters(), 'lr': 0.001},\n",
    "                                {'params': model.feature_extractor.parameters(), 'lr': 0.001}])\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train_loop(epoch, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
