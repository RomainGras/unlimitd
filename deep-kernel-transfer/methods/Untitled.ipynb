{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9547ce45-2d43-4b47-bcaa-5d918c3de6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "class MyNetworkTester:\n",
    "    def __init__(self):\n",
    "        # A simple CNN for testing purposes\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 96 * 96, 1)  # Assuming input size is 100x100\n",
    "        )\n",
    "        \n",
    "        # Initialize some dummy scaling parameters\n",
    "        net_params = dict(self.net.named_parameters())\n",
    "        self.scaling_params = {k: torch.ones_like(v) for k, v in net_params.items()}\n",
    "        def create_jac_func(net):\n",
    "            \"\"\"\n",
    "            Computes the functional call of a single input, to be differentiated in parallel using vmap\n",
    "            \"\"\"\n",
    "            def fnet_single(params, x):\n",
    "                return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "            jac_func = vmap(jacrev(fnet_single), (None, 0))\n",
    "            return jac_func\n",
    "        self.jac_func = create_jac_func(self.net)\n",
    "        \n",
    "\n",
    "    def test_function(self, x1, x2, diag=False):\n",
    "        x1 = x1.reshape(x1.size(0), 3, 100, 100)\n",
    "        x2 = x2.reshape(x2.size(0), 3, 100, 100)\n",
    "        \n",
    "        params = dict(self.net.named_parameters())\n",
    "        \n",
    "        # Calculate Jacobians for x1\n",
    "        jac1 = self.jac_func(params, x1)\n",
    "        sp_jac1 = [(self.scaling_params[k]*j).flatten(2) for (k, j) in jac1.items()]  \n",
    "\n",
    "        if torch.equal(x1, x2):\n",
    "            jac2=jac1\n",
    "            sp_jac2=sp_jac1\n",
    "        else:\n",
    "            jac2 = self.jac_func(params, x2)\n",
    "            sp_jac2 = [(self.scaling_params[k]*j).flatten(2) for (k, j) in jac2.items()]  \n",
    "        ntk_list = []\n",
    "        for j1, j2 in zip(sp_jac1, sp_jac2):\n",
    "            ntk_list.append(torch.einsum('Naf,Maf->aNM', j1, j2))\n",
    "        \n",
    "        ntk = torch.sum(torch.stack(ntk_list), dim=0).squeeze(0)\n",
    "        \n",
    "        if diag:\n",
    "            return ntk.diag()\n",
    "        return ntk\n",
    "\n",
    "\n",
    "# Now, we will instantiate the class and test the function\n",
    "\n",
    "# Create an instance of the network tester\n",
    "tester = MyNetworkTester()\n",
    "\n",
    "# Create random inputs x1 and x2 (batch_size=4 for testing)\n",
    "x1 = torch.randn(19, 3, 100, 100)\n",
    "x2 = torch.randn(19, 3, 100, 100)\n",
    "\n",
    "# Test the function (without diagonal)\n",
    "ntk = tester.test_function(x1, x1, diag=False)\n",
    "print(ntk.shape)\n",
    "# Test the function (without diagonal)\n",
    "ntk = tester.test_function(x1, x2, diag=False)\n",
    "print(ntk.shape)\n",
    "# Test the function (with diagonal)\n",
    "ntk_diag = tester.test_function(x1, x2, diag=True)\n",
    "print(ntk_diag.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363666f5-3022-4df5-a265-bfca2379d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Initialize your params\n",
    "        self.params = {'param1': torch.randn(3, 3), 'param2': torch.randn(3, 3)}\n",
    "        \n",
    "        # Make scaling_params a Parameter to allow optimization\n",
    "        self.scaling_params = {k: nn.Parameter(torch.ones_like(v, device='cuda:0')) for k, v in self.params.items()}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Example forward pass, using scaling_params\n",
    "        # Do something with scaling_params here\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70891442-9cc7-4685-813c-82d3c70b62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MyModel()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
