{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9652f9-7164-40b5-bf5e-c85d4195a031",
   "metadata": {},
   "source": [
    "## This is KERNEL UNLIMITD no MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6373e1-b6a2-4334-8afd-432ef43d6779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.3\n",
      "2.0.0+cu117\n",
      "1.11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(gpytorch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9e954-b4fc-4471-9d37-f328d2e9a85c",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65146f80-9533-4b28-8400-746674ce3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine_Task():\n",
    "    \"\"\"\n",
    "    A sine wave data distribution object with interfaces designed for MAML.\n",
    "    \"\"\"\n",
    "    def __init__(self, amplitude, phase, xmin, xmax):\n",
    "        self.amplitude = amplitude\n",
    "        self.phase = phase\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "\n",
    "    def true_function(self, x):\n",
    "        \"\"\"\n",
    "        Compute the true function on the given x.\n",
    "        \"\"\"\n",
    "        return self.amplitude * np.sin(self.phase + x)\n",
    "\n",
    "    def sample_data(self, size=1, noise=0.0, sort=False):\n",
    "        \"\"\"\n",
    "        Sample data from this task.\n",
    "\n",
    "        returns:\n",
    "            x: the feature vector of length size\n",
    "            y: the target vector of length size\n",
    "        \"\"\"\n",
    "        x = np.random.uniform(self.xmin, self.xmax, size)\n",
    "        if(sort): x = np.sort(x)\n",
    "        y = self.true_function(x)\n",
    "        if(noise>0): y += np.random.normal(loc=0.0, scale=noise, size=y.shape)\n",
    "        x = torch.tensor(x, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d5d9de5-fd39-4a2c-8871-2610c7db49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_Distribution():\n",
    "    \"\"\"\n",
    "    The task distribution for sine regression tasks for MAML\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, amplitude_min, amplitude_max, phase_min, phase_max, x_min, x_max, family=\"sine\"):\n",
    "        self.amplitude_min = amplitude_min\n",
    "        self.amplitude_max = amplitude_max\n",
    "        self.phase_min = phase_min\n",
    "        self.phase_max = phase_max\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.family = family\n",
    "\n",
    "    def sample_task(self):\n",
    "        \"\"\"\n",
    "        Sample from the task distribution.\n",
    "\n",
    "        returns:\n",
    "            Sine_Task object\n",
    "        \"\"\"\n",
    "        amplitude = np.random.uniform(self.amplitude_min, self.amplitude_max)\n",
    "        phase = np.random.uniform(self.phase_min, self.phase_max)\n",
    "        if(self.family==\"sine\"):\n",
    "            return Sine_Task(amplitude, phase, self.x_min, self.x_max)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79671c89-dd12-4d80-a235-07d93b72995e",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164157a4-969b-4845-9739-18c0960110bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Feature, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, 40)\n",
    "        self.layer2 = nn.Linear(40,40)\n",
    "        self.layer3 = nn.Linear(40,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b90ca0-95e2-4720-924f-63a34b51683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosSimKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CosSimKernel, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Normalize input vectors\n",
    "        x1_norm = x1 / x1.norm(dim=0, keepdim=True)\n",
    "        x2_norm = x2 / x2.norm(dim=0, keepdim=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cos_sim = x1_norm.T @ x2_norm\n",
    "        \n",
    "        if diag:\n",
    "            return cos_sim.diag()\n",
    "        else:\n",
    "            return cos_sim\n",
    "\n",
    "\n",
    "class CustomLinearKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLinearKernel, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Normalize input vectors\n",
    "        x1_norm = x1.T\n",
    "        x2_norm = x2.T\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        custlin = x1_norm.T @ x2_norm\n",
    "        \n",
    "        if diag:\n",
    "            return custlin.diag()\n",
    "        else:\n",
    "            return custlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d2889b-1dbe-4305-8216-02a4f29fa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, net):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "        self.covar_module = CustomLinearKernel()\n",
    "        #self.covar_module.base_kernel.variance = 1.0\n",
    "        #self.covar_module.base_kernel.raw_variance.requires_grad = False\n",
    "        #self.covar_module.raw_outputscale.requires_grad = False\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=40)\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = self.feature_extractor(x)\n",
    "        #z_normalized = z - z.min(0)[0]\n",
    "        #z_normalized = 2 * (z_normalized / z_normalized.max(0)[0]) - 1\n",
    "        #x_normalized = x - x.min(0)[0]\n",
    "        #x_normalized = 2 * (x_normalized / x_normalized.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af44617d-b990-44f7-837e-f0b5663ce4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(net, batch):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian for the batch. This needs to be adapted based on the actual function.\n",
    "    \"\"\"\n",
    "    net.zero_grad()\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "    def fnet_single(params, x):\n",
    "        return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, batch)\n",
    "    jac = jac.values()\n",
    "    # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    reshaped_tensors = [\n",
    "        j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "            .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "            .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "        for j in jac\n",
    "    ]\n",
    "    return torch.cat(reshaped_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4f355e-716e-429d-8495-2db8483d6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1761])\n",
      "Trainable parameters of GP : 1\n",
      "Parameter name: likelihood.noise_covar.raw_noise, Size: torch.Size([1]), Number of elements: 1\n",
      "Trainable parameters of NN : 1761\n"
     ]
    }
   ],
   "source": [
    "n_shot_train = 10\n",
    "n_shot_test = 5\n",
    "train_range=(-5.0, 5.0)\n",
    "test_range=(-5.0, 5.0) # This must be (-5, +10) for the out-of-range condition\n",
    "criterion = nn.MSELoss()\n",
    "tasks     = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                  phase_min=0.0, phase_max=np.pi, \n",
    "                                  x_min=train_range[0], x_max=train_range[1], \n",
    "                                  family=\"sine\")\n",
    "net       = Feature()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-4))\n",
    "# likelihood.noise = 1e-4\n",
    "dummy_inputs = torch.zeros([n_shot_train,1])\n",
    "dummy_z = jacobian(net, dummy_inputs).T\n",
    "print(dummy_z.shape)\n",
    "dummy_labels = torch.zeros([n_shot_train])\n",
    "gp = ExactGPModel(dummy_z, dummy_labels, likelihood, net)\n",
    "trainable_params_gp = sum(p.numel() for p in gp.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters of GP : {trainable_params_gp}\")\n",
    "trainable_params_net = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "for name, param in gp.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter name: {name}, Size: {param.size()}, Number of elements: {param.numel()}\")\n",
    "\n",
    "print(f\"Trainable parameters of NN : {trainable_params_net}\")\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "optimizer = torch.optim.Adam([{'params': gp.parameters(), 'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a936d9-90c6-4491-bfeb-cab3da9287ba",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5656afc-3fdd-40b2-9ee3-84a81a5d29bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[0] - Loss: 2.703  MSE: 4.087  lengthscale: 0.000   noise: 0.694\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1000] - Loss: 1.799  MSE: 4.961  lengthscale: 0.000   noise: 0.775\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2000] - Loss: 1.578  MSE: 0.093  lengthscale: 0.000   noise: 0.845\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3000] - Loss: 3.535  MSE: 11.762  lengthscale: 0.000   noise: 0.908\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[4000] - Loss: 1.445  MSE: 0.055  lengthscale: 0.000   noise: 0.961\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[5000] - Loss: 1.953  MSE: 1.975  lengthscale: 0.000   noise: 1.012\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[6000] - Loss: 2.672  MSE: 8.096  lengthscale: 0.000   noise: 1.074\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[7000] - Loss: 2.168  MSE: 5.463  lengthscale: 0.000   noise: 1.119\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[8000] - Loss: 2.667  MSE: 6.383  lengthscale: 0.000   noise: 1.151\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[9000] - Loss: 2.853  MSE: 15.094  lengthscale: 0.000   noise: 1.180\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[10000] - Loss: 2.960  MSE: 8.153  lengthscale: 0.000   noise: 1.165\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[11000] - Loss: 1.857  MSE: 3.397  lengthscale: 0.000   noise: 1.152\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[12000] - Loss: 1.711  MSE: 1.253  lengthscale: 0.000   noise: 1.169\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[13000] - Loss: 1.714  MSE: 1.631  lengthscale: 0.000   noise: 1.184\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[14000] - Loss: 1.670  MSE: 0.155  lengthscale: 0.000   noise: 1.179\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[15000] - Loss: 1.556  MSE: 0.022  lengthscale: 0.000   noise: 1.140\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[16000] - Loss: 1.665  MSE: 0.166  lengthscale: 0.000   noise: 1.127\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[17000] - Loss: 2.013  MSE: 1.474  lengthscale: 0.000   noise: 1.120\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[18000] - Loss: 2.366  MSE: 3.633  lengthscale: 0.000   noise: 1.154\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[19000] - Loss: 2.526  MSE: 5.455  lengthscale: 0.000   noise: 1.159\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[20000] - Loss: 2.497  MSE: 8.378  lengthscale: 0.000   noise: 1.154\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[21000] - Loss: 2.743  MSE: 10.496  lengthscale: 0.000   noise: 1.143\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[22000] - Loss: 1.827  MSE: 1.499  lengthscale: 0.000   noise: 1.126\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[23000] - Loss: 1.801  MSE: 1.445  lengthscale: 0.000   noise: 1.184\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[24000] - Loss: 2.137  MSE: 6.597  lengthscale: 0.000   noise: 1.159\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[25000] - Loss: 1.683  MSE: 0.770  lengthscale: 0.000   noise: 1.173\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[26000] - Loss: 1.986  MSE: 3.324  lengthscale: 0.000   noise: 1.145\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[27000] - Loss: 1.799  MSE: 2.293  lengthscale: 0.000   noise: 1.154\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[28000] - Loss: 2.682  MSE: 5.434  lengthscale: 0.000   noise: 1.197\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[29000] - Loss: 1.552  MSE: 0.231  lengthscale: 0.000   noise: 1.184\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[30000] - Loss: 1.806  MSE: 1.441  lengthscale: 0.000   noise: 1.205\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[31000] - Loss: 1.722  MSE: 0.689  lengthscale: 0.000   noise: 1.209\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[32000] - Loss: 2.748  MSE: 8.689  lengthscale: 0.000   noise: 1.196\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[33000] - Loss: 2.852  MSE: 7.982  lengthscale: 0.000   noise: 1.163\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[34000] - Loss: 1.655  MSE: 3.670  lengthscale: 0.000   noise: 1.163\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[35000] - Loss: 1.859  MSE: 2.162  lengthscale: 0.000   noise: 1.152\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[36000] - Loss: 3.075  MSE: 9.649  lengthscale: 0.000   noise: 1.128\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[37000] - Loss: 2.117  MSE: 4.065  lengthscale: 0.000   noise: 1.137\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[38000] - Loss: 1.797  MSE: 1.522  lengthscale: 0.000   noise: 1.166\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[39000] - Loss: 2.539  MSE: 7.628  lengthscale: 0.000   noise: 1.156\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[40000] - Loss: 1.821  MSE: 2.471  lengthscale: 0.000   noise: 1.127\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[41000] - Loss: 1.955  MSE: 5.011  lengthscale: 0.000   noise: 1.112\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[42000] - Loss: 1.500  MSE: 0.008  lengthscale: 0.000   noise: 1.158\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[43000] - Loss: 2.455  MSE: 4.181  lengthscale: 0.000   noise: 1.181\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[44000] - Loss: 1.553  MSE: 0.159  lengthscale: 0.000   noise: 1.180\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[45000] - Loss: 1.831  MSE: 4.054  lengthscale: 0.000   noise: 1.182\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[46000] - Loss: 1.600  MSE: 0.738  lengthscale: 0.000   noise: 1.182\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[47000] - Loss: 1.818  MSE: 1.584  lengthscale: 0.000   noise: 1.170\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[48000] - Loss: 1.752  MSE: 1.394  lengthscale: 0.000   noise: 1.189\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[49000] - Loss: 1.601  MSE: 0.195  lengthscale: 0.000   noise: 1.172\n",
      "Total time : 263.968924392 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "likelihood.train()\n",
    "gp.train()\n",
    "net.train()\n",
    "\n",
    "tot_iterations=50000\n",
    "\n",
    "t = t = time.time_ns()\n",
    "for epoch in range(tot_iterations):\n",
    "    # gp.likelihood.noise = 1e-2\n",
    "    optimizer.zero_grad()\n",
    "    inputs, labels = tasks.sample_task().sample_data(n_shot_train, noise=0.05)\n",
    "    z = jacobian(net, inputs).T\n",
    "    gp.set_train_data(inputs=z, targets=labels)  \n",
    "    predictions = gp(z)\n",
    "    loss = -mll(predictions, gp.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #---- print some stuff ----\n",
    "    if(epoch%1000==0):\n",
    "        mse = criterion(predictions.mean, labels)\n",
    "        print(predictions.mean)\n",
    "        print('[%d] - Loss: %.3f  MSE: %.3f  lengthscale: %.3f   noise: %.3f' % (\n",
    "            epoch, loss.item(), mse.item(),\n",
    "            0.0, #gp.covar_module.base_kernel.lengthscale.item(),\n",
    "            gp.likelihood.noise.item()\n",
    "        ))\n",
    "print(f\"Total time : {(time.time_ns()-t)/1e9} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9898f0-8fb6-4f06-9f2f-febfdfdf2a54",
   "metadata": {},
   "source": [
    "## Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c73ff8e-705c-4e89-a477-43119a1dddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_test = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                phase_min=0.0, phase_max=np.pi, \n",
    "                                x_min=test_range[0], x_max=test_range[1], \n",
    "                                family=\"sine\")\n",
    "\n",
    "sample_task = tasks_test.sample_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d39f387-17a4-446c-879a-aa11b6d7d810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes torch.Size([200]) and (1761,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m     ax\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mlinspace(train_range[\u001b[38;5;241m1\u001b[39m], test_range[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1000\u001b[39m), dotted_curve, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#query points (ground-truth)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#ax.scatter(x_query, y_query, color='blue')\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#query points (predicted)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m ax\u001b[38;5;241m.\u001b[39mfill_between(np\u001b[38;5;241m.\u001b[39msqueeze(x_all),\n\u001b[1;32m     48\u001b[0m                 lower\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), upper\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m     49\u001b[0m                 alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.1\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#support points\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes torch.Size([200]) and (1761,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGhCAYAAAByPf5TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUhElEQVR4nO3dd3xUVdoH8N+dOzNpEEgMRXpRETR0EBAIIFUDUsQXsaDyurviKy4rq+i6uior7i666tqxIeqCIn0BQcDQmxWkaRKKqNSQnszMvff942zmTmhpM3Pb7/v57GeTMQOHmzvnPuc5zzlH0jRNAxEREZENuIxuABEREVG4MLAhIiIi22BgQ0RERLbBwIaIiIhsg4ENERER2QYDGyIiIrINBjZERERkGwxsiIiIyDbcRjcg2jRNg6pyT8IyLpfE6xEFvM7RwescPbzW0cHrLLhcEiRJqtTPOi6wUVUNp08XGt0MU3C7XUhKSkBeXhECAdXo5tgWr3N08DpHD691dPA665KTEyDLlQtsOBVFREREtsHAhoiIiGyDgQ0RERHZBgMbIiIisg0GNkRERGQbDGyIiIjINhjYEBERkW0wsCEiIiLbYGBDREREtsHAhoiIiGyDgQ0RERHZBgMbIiIisg0GNkRERGQbjjvdm4iq5uRJCXv2uHDmjITiYiA5WUPjxhratFEhy0a3joioPAY2RHSO/ftd+PhjN5Yu9eDgwfMndmvX1nDttQGMGxfAoEEBeDxRbiQR0XkwsCGioD17XPj7371YvrziKCU/X8LKlR6sXOlBkyYqHnqoFGPHBpjFISJDMbAhIpSUADNnevHKK14oihR8XZY1dO2qoGNHFZdeqiImBjh1SsL+/S5s2SLj5EmRzfnpJxcmT47D228rePnlErRpoxr1TyEih2NgQ+Rw2dkS7rwzDnv36qmWBg1UTJrkw003BVCvnnbe9ykKkJEh4803vVi7VnQl334rY+DAeDz9dCnuvNMflfYTEYXiqigiB8vIkDF4cEIwqPF4NDz0UCm2by/Evff6LxjUAIAsAwMGKJg7txiLFhXh8ssVAEBpqYSHHorFQw/FwM/YhoiijIENkUMtW+bG+PFxyM0VU0+XX67g88+LMHWqD3FxVfuzevUS7/3Nb3zB1957z4uJE2NRWhrOVhMRXRwDGyIHWrzYjXvuiYXfL4KaIUMCWLmyCG3bVr82Ji4OmD69FC+9VAyvV2R6Vq704K674hjcEFHUMLAhcpiMDBmTJsUGi4RvvtmPd98tRu3a4fnzx40L4MMPixEXJ4Kbzz934/77Y6GynpiIooCBDZGDfPutC3feGRfM1Iwf78NLL5XAHeZlBGlpovYmPl4EN4sWefDUUzHh/UuIiM4j4oHNoUOH8Pjjj+PGG29Eu3btkJ6eXun3Lly4EEOHDkVqairS09OxYsWKCLaUyN6OH5dw++1xKCwUQc3QoX7MnFkKV4R6gZ49Fbz5ZjFcLhHcvPqqF++9x138iCiyIh7Y/PDDD8jIyEDz5s3RunXrSr9v5cqVmDZtGgYNGoRZs2ahR48emDJlCjZu3BjB1hLZUyAA/Pa3sfj1V/GR7949gDfeCH+m5myDByv429/0ApvHHovB118zUUxEkSNpmnbh9ZxhoKoqXP8dEk6bNg27d+/GsmXLKnzfsGHDcMUVV+DFF18MvjZx4kTk5+fj448/rnZ7FEXF6dOF1X6/nbjdLiQlJSAnpxCBAAsgIsUM1/nJJ2PwyiteAEDDhio+/7wI9etH9KNfzp//HIM33hB/f9OmKj7/vBBJSeH9O8xwnZ2C1zo6eJ11yckJkOXKDYoiPnRyVSPPfeTIEWRlZZ0zbZWeno7vvvsOp0+fDlfziGxv7Vo5GNS43Rreeqs4qkENADz+eCm6dhX73Bw54sIDD8QiskMqIoo2nw+YPduD7duNzcqacufhrKwsAECrVq3Kvd66dWtomoasrCwkJydX+893u5kKBxCMfisbBVP1GHmdz5wBpkyJDX7/1FM+9OoFRHvdgNsNvPNOKfr3j8OpU+KMqfnzVdxySyBsfwfv5+jhtY4OK11nTQPuuy8Gixe7kZCg4fvvi5CYaExbTBnY5ObmAgASz7oqderUKfffq8PlkpCUlFD9xtlQYmIVd2OjajHiOv/f/wG//CK+HjQImDYtBpJkzOqkpCTgnXeAG28U3z/6aAxGjIhBkybh/Xt4P0cPr3V0WOE6z50LLF4svo6Pl9C0aULEFiZUxJSBTRlJksp9X1YOdPbrVaGqGvLyimrULruQZRcSE+OQl1cMRXH2/G0kGXWdly2T8dFHIltTp46G558vxpkzxs7/9OkDjBvnxdy5HuTmAnfcEcD8+aWowUc6iPdz9PBaR4dVrvOpU8B998UDEB/kZ58tQW6uEta/IzExrtKZK1MGNqGZmZSUlODreXl5AM7N5FSV04uwzqYoKq9JFETzOhcUANOm6aO8GTNK0KCBgkD4Zn6q7emnS/DFFzJ+/dWFdevc+PhjP8aMCV/DeD9HD691dJj9Oj/xRAxOnxZBzYgRfgwf7je0rzHlxF1ZbU1ZrU2ZzMxMSJJ0Tu0NEZX3j3/E4Oefxcd7wIBAWAOHmqpTB5g5syT4/RNPxOC/YxYispitW2V89JFYnFC7toa//tX481NMGdg0bdoUrVq1wvLly8u9vmzZMrRv375GhcNEdvf99y68+abYCC82VsOMGSVhmeoJp8GDFQwdKo7+Pn7chb//nbsSE1mN3w/88Y/6Z/dPfypFgwbGL3eM+FRUcXExMjIyAABHjx5FQUEBVq5cCQDo3r07kpOT8eijj2LRokXYs2dP8H2TJ0/GlClT0KxZM/Tq1Qtr1qzBpk2b8NZbb0W6yUSWpWliE7yyc6AeeMCHli2N72jO569/LUVGhhvFxRLeesuDW27x46qrzJtuJ6LyZs/2YP9+GQDQqZOCCRP8BrdIiHhgc+rUKTzwwAPlXiv7/v3338c111wDVVWhKOULjYYNG4aSkhK8/vrrePvtt9G8eXP885//RO/evSPdZCLLWrVKxqZN4mPdooWK//s/n8EturCmTTVMmeLDM8/EQFUlPPVUDObNKza6WURUCbm5wMyZ3uD3zz5bAlk2sEEhIr7zsNlw52Edd7WMjmhd50AASEuLxw8/iN7l7beLMXy4eWprzqe0FLj22gQcPixmxT/+uAj9+lVvNQXv5+jhtY4OM1/n0N3Mx4zx47XXSip4R82YaudhIoqODz7wBIOabt0UpKebO6gBgJgY4JFH9GLDp56KgWqu/puIznLokIRZs0QdX0yMhkcfNb5gOBQDGyIbKCgA/v53PS38l7+Yr2D4QkaNCqB9e5Gl2b1bxqefmnIXCiL6r2eeiYHPJzqY3/7Wh6ZNzTXxw8CGyAbefNOLkyfFx3nECD+6dbNO2sPlEmdJlZkxIwal5hoAEtF/7dnjwsKFIltzySUqJk82Xx0fAxsii8vLA15/XWRrZNl8aeHK6NtXwYABYursp59c+Pe/PQa3iIjO57nn9Mzw5Mk+w86DuhgGNkQW99ZbXpw5I9LCY8cG0KqVudLClTVtmh6QvfSSFz7zDQSJHO37711YulQMOurVU02zvPtsDGyILOzsbM3vf2+9bE2Zjh1VDByoZ23mzWPWhshMzs7WxMcb2JiLYGBDZGF2ydaUefBBPTB78UUv/OYcEBI5zvffu7BsmRhs1K+v4o47zPvhZGBDZFEFBfbJ1pTp0kVF//4ia3P4sAuffMIVUkRm8MIL5bM1cXEX+WGDMbAhsqgPPvAEszVjxlg/W1Nm6tTQrE0MlOrt10dEYZKdLWHpUjHISElRcfvt5s3WAAxsiCzJ7wfeeEMfQd1/v30qbbt1U9Gnj8jaZGe7sHIlszZERnr9dS9UVQyifvMbv6mzNQADGyJLWrzYjaNHxcd30KAA2rSxzr41lXHffXqg9uqr3ov8JBFF0smTUnD7hfh4DXfeaf5BFAMbIovRNATPaAHKBwF20b+/grZtxRzUjh0ytm9nV0VkhHfe8aCkRGRrbr/dj7p1jW1PZbC3ILKYjAwZ338vzoTq3FlBz572K0KRJODee/WA7bXXmLUhirbCQhHYAGKBwm9/a41BFAMbIosJnZq57z6fZc6EqqpRowJo0EBMsS1f7kZWlk3/oUQm9cknHpw+LcKEUaMCaNLEGgsUGNgQWciPP0r44gtRTNusmYrrrzf/Cd7VFRMD3HOPWH2haRLeeotZG6Jo0TQ9WwOUz6CaHQMbIgt591394X733T7IsoGNiYI77vAhLk6MEufN86CgwOAGETnE5s0y9u0THUz37gGkplpngQIDGyKLKCgA5s4VI6i4OA233GLuvSTCoW5dYPRo8e/Mz5cwfz6PWSCKhrff1j9rEydaq69hYENkEfPne5CfX7Yhnx9JSQY3KEruvlvvVN991wPNGtP8RJZ19KiEFSvElHf9+ipuuMFaU94MbIgs4Oz57rvustYIqiZSU1V07SpWfu3dK2PbNpvPvxEZ7P33PVAUMYi64w4/vBYrb2NgQ2QBVp7vDoe779YLF0MDPCIKr9JSYM4c8RlzuzVTH3Z5IQxsiCzAyvPd4TB8eAApKSKYW7bMjWPHuPSbKBKWLHHj5EkRGqSnB9CwofXmfhnYEJncsWPWnu8Oh5gY4NZbRUAXCEjBESURhdcHH+ifrdD6NithYENkcvPm6fPd48dbb747XCZM8MPlEqPHjz7y8NRvojDLzJSwZYsYRF1+uYJrrrHmh4yBDZGJaZp4iJdxwhLvC2nSREP//qKj/eknFzZsYBExUTh9+KHe19x6q9+yu5ozsCEysa1bZWRliY9p794BtGxpvfnucBo/Xg/sQgM+IqoZv1/fJ8vj0TB2rHWnvBnYEJnY2SMopxsyRC8iXr7cjdOnDW4QkU2sWqUXDQ8dGkC9etYdRDGwITKpvDxg6VIx312njmbrc6Eqy+sFbrpJXAefT8KnnzJrQxQOdhpEMbAhMqkFCzwoLhaT3Dfd5EdcnMENMonQ6agPP+ROxEQ1dfSohLVrRc1akyYq0tKsWTRchoENkUmFjqBCH+ZOd+WVKrp0ER3vnj0yvvuO3RhRTcyd64GqikHULbf4LX+4LnsEIhPau9eFb78VvUuHDorjdhquyNlZGyKqHk0DPv5YfIYkyR6H6zKwITKhTz5xB78eN876HU24jRzpR3y8mINauNCD0lKDG0RkUV9+6UJ2dtnKSwVNmlh/bpeBDZHJKAqCRbFut4aRI1k0fLbatREsps7NlfD55+4K3kFE5/PJJ3rGc+xYewyiGNgQmczmzTJ++UV8NK+7TsEll1h/BBUJN92kd8Lz5zOwIaoqnw9YtEgENnFxmm2Oa2FgQ2QyoSOo0Ic3lde3r4J69UTt0erVbpw5Y2x7iKxmzRo3cnJE0fDQoQHUrm1wg8KEgQ2RiRQVidOrAaB2bQ2DB9tjBBUJbjcwerS+p83ixczaEFVFaKbTLtNQAAMbIlP57DM3CgrECGr4cO5dU5HQjFZowTURXVxurthtGABSUlT062ftvWtCMbAhMpH580ML+ZitqUj79iouv1x0yJs3yzh0yOAGEVnEkiUelJaKQdTo0QG4bTQuYGBDZBInTui7fzZurKJnT/uMoCJFkvQjFgDgo48MbAyRhYRmOO00DQUwsCEyjcWL3VAUMYIaM8YPFz+dlTJ6tN4pz5kDHrFAVIHDhyVs3SoCm8svV9C+vb02AGXXSWQSoQc6chqq8po319C9u7hee/cCu3axWyO6mLIl3oDIeEqSgY2JAPYARCZw+LCEL78U01Dt2ilo08ZeI6hIC52Omj/f4gfdEEXYkiX6NNSoUfaahgIY2BCZQmhHw52Gq27ECD9kWcxBLV7s5nQU0QVkZUn47jsR/HfsqKBFC/t9WBjYEJnA4sV6anjECPuNoCItORlISxPF1keOuPD11+zaiM7HCX0NP/1EBsvOloInebdvr6BVK/uNoKJh5Eh9FVlo501EukWL9OzwjTfaMzvMwIbIYEuW6A9hu3Y00XDDDfpeHEuWcDqK6GwHDriwd68YRHXpoqBpU3t+SBjYEBks9CgAu6aGoyEpCRg0SHx99KgLX37J7o0oVGhfM3KkffsafvKJDJSZKWH3bjGC6txZQfPm9hxBRcvNN+tfczqKSKdp5QOb4cPtmx1mYENkoND9JJitqbkbbwQ8HhEcLl3qhspV80QAgL17XThwQAyirrkmgEaN7DuIYmBDZKDQZd4jRth3BBUtSUlA//6iiPjnn13YuZNdHBHgrC0l+KknMsj+/XohX9euCpo0se8IKppCV0eFFmYTOZWYhhKfBUnSkJ7OwIaIImDZstBll5yGCpdhwwLwekWQuGQJp6OI9u93ITNTPO579lTQoIG9B1EMbIgMsny5HtjYfQQVTXXq6NNRv/7K6Sii//xH72tuuMH+fQ0/8UQGOHRIwq5d+rbmjRvbewQVbTfcoGfAli/ndBQ5W2hgc/31DGyIKAJWrHDWCCraBg8OBM+OWr6cm/WRcx06pG8p0amTMwZRDGyIDOC0EVS0JScDvXqJ6aiDB13Yu5ddHTlT6JS3U/oaftqJouz4cQnbt4sR1BVXKLj8cla3RkJoJx7auRM5Sfn6GmcsUmBgQxRlK1e6oWkSAOeMoIwwbJh+bUM7dyKnOHZMwo4dYhDVpo2Cyy6z/zQUwMCGKOqctkLBKI0aaejUSUxHff+9jEOHJINbRBRdoYMoJ/U1DGyIoig3F9i4UYygmjRR0b49p6EiKbQz53QUOY1Ta/kY2BBF0erVbvj9+jSUxCRCRF1/feiybwY25BxnzuiDqKZNVaSmOmcQxcCGKIqcuELBSJddpuGKK8R01PbtMo4fZyRJzrB6tRuBgDMHUQxsiKKkuBhYu1YENikpKq65RqngHRQOZQGkpklYuZJZG3IGJ9fyMbAhipJ162QUFYlh09ChAciywQ1yCNbZkNMUFwPr1umDqG7dnDWIYmBDFCUrVuiRTOhSZIqs9u1VNGki6gs2bJCRl2dwg4gibMMGGcXFYhA1ZIjzBlFRCWyys7MxceJEdOzYET179sT06dNRUlJS4ftuv/12tGnT5pz/ZWZmRqHVROGjqsCqVWIEFR+voU8fZ42gjCRJeiDp90vBkSyRXX32mX6PDxnivEFUxD/heXl5mDBhAho1aoSXXnoJp0+fxowZM3DmzBnMnDmzwvd37twZDz/8cLnXmjRpEqnmEkXE9u3AiRNiBNW3bwCxsQY3yGEGDw5g1iwvANHp33ij8zp7cobQQVRsrIa+fZ03iIp4YDN37lzk5eVh0aJFSE5OBgDIsoypU6fi3nvvRevWrS/6/sTERHTs2DHSzSSKqKVL9a+HDHFeR2O0nj0V1K6tIT9fwpo1bgQCgJuJG7Kh775z4dgxMRnTt6+C+HiDG2SAiE9FrV+/Hj179gwGNQAwZMgQeL1eZGRkRPqvJzKFssBGkjQMGsRsQbR5vUD//uK65+RI2LnTYUUH5Bih01CDBzuzr4l4YJOZmXlOVsbr9aJZs2aVqpXZvn07OnbsiNTUVNx2223YsWNHpJpKFBFHjkjYtUt83bmzivr1nXFei9mEdvKhnT+RnTCwiVKNTWJi4jmvJyYmIjc396Lv7datG2688Ua0aNECx48fx9tvv4277roLc+bMQadOnardJrebi8EAQJZd5f6fIqNsvhsAhg5VeP9FSEX389ChKlwuDaoqYdUqN55+2hknHUcC+47oqOp1/uknCbt3i2xkp04KmjSRADhoZ77/MmzYomkapAq2Qpw8eXK57/v164f09HS8+uqrmDVrVrX+XpdLQlJSQrXea1eJiXFGN8HWVq3Sv775Zi+SkrzGNcYBLnQ/JyUBvXoBGzcCP/zgwqlTCbjssig3zmbYd0RHZa/z3Ln61yNHyo591kU8sElMTETeeTaOyM/Pr7Bw+Gzx8fFIS0vDZ599Vu32qKqGvLyiar/fTmTZhcTEOOTlFUNRnHOOSDTl5QFffBEPQEKzZiqaNClGTo7RrbKnytzPAwd6sHGjCCznzSvFpEnOTNXXFPuO6Kjqdf700xiUPdbT0oqRk2Of301iYlylM1cRD2xat259Ti2Nz+fD4cOHMWbMmCr/eZpW8/qEQMA+v+xwUBSV1yRC1qzRD70cMkThQyAKLnY/Dxzox1/+IgKblStl/OY3vmg2zXbYd0RHZa5zQYHYmA8AGjVS0bZtAAGHxu0RnyDt27cvtm7dipyQYerq1avh8/mQlpZWpT+rqKgIGRkZSE1NDXcziSKi/EZZXOZttMsvV9GihXhAbN0qo4IyPyLLyMhww+cTg6jBg5116OXZIh7YjBs3DrVr18akSZOwYcMGLFq0CE8//TSGDx9ebirq0UcfRbt27YLf79y5E/feey8WLFiArVu3YsmSJbj11ltx4sQJ3HfffZFuNlGNKQrw+ediBFWrFnDttQxsjCZJ+kqRQIC7EJN9OH234VBRqbGZPXs2pk+fjvvvvx+xsbFIT0/H1KlTy/2cqqpQFL3jr1evHnw+H55//nmcOXMGcXFx6NSpE5588km0b98+0s0mqrEdO2ScPi3GDkOGADExcGxq2EwGDw7gzTf1XYhHjuQvhawtdBAVH685fhAlaeEoWrEQRVFx+nSh0c0wBbfbhaSkBOTkFHKePAKeesqLl1+OAQDMng0MH87rHEmVvZ99PqBt21rIz5dQt66GPXsKuAtxFbHviI7KXucdO1y44QaxAur66/14772Kz2K0muTkhEoXD3MTAqIIKdu/xuXScP31BjeGgrxeYMAAkaU5c0bCjh3chZisbfVqTkOFYmBDFAGHDkk4cEA8MLt2VZGSYnCDqBzuQkx28vnn+j183XXOnoYCGNgQRcSaNXpHM2gQOxqzue66AFwuMQu/Zg0zNmRdv/6q7zbcoYPCI1vAwIYoItauZWBjZsnJQJcuomZh/34ZR444eG0sWdratXpgft11nIYCGNgQhV1Jib5RVoMGKlJTWVxpRqEPgdAMG5GVhE5DDRzIwAZgYEMUdlu2yCguFhmAAQMUR2+UZWahDwEGNmRFfr/YmA8AkpNVdOrEQRTAwIYo7EIfkhxBmdfVV6uoV088CDZskFFaanCDiKpoxw4Z+fli5NSvnwKZ5WIAGNgQhV1ZYCPLGvr2ZWBjVi6XvoKkqEjCli18KpC1lG3KB3AQFYqBDVEYZWVJyMwUH6vu3RXUqWNwg+iiWGdDVlZ2z0qShv79uUihDAMbojAKXQ3F/STMLy0tAFnmsm+ynqNHJezdK+7Zzp1VXHIJl3mXYWBDFEaho34uvTS/unWBrl1FAPrjjzIOHmSlN1lDaF9TtpM2CQxsiMKkuBjYtEmMoC69VEW7dlyhYAUDB+qZNU5HkVWEZhhZX1MeAxuiMNm8WUZJiRjxX3ddgMu8LSJ0tBs6lUhkVqWlwPr14l5NSVHRoQMHUaEY2BCFSehGWQMGsL7GKq6+WkWDBuLBsHGjjBL7HYxMNrNtm4zCQjFy6t9fgYtP8nJ4OYjCQNP0wMbt1pCWxtSwVUiSXg9VXCxh82YWEZO5ca+si2NgQxQGWVkSDh0SH6cePRTUrm1wg6hKQlewcTqKzK6svsbl0tCvHwObszGwIQoDrlCwtrS0ANxusVw2dEqRyGwOH5Zw4IAIbLp0UZGUZHCDTIiBDVEYlD+IjvU1VpOYCHTrJn5vWVkuZGWx8pvMidNQFWNgQ1RDhYUIbsffuLGKNm24QsGKOB1FVsDApmIMbIhqaPNmGaWlXOZtdaEbKjKwITMqLRUr9wCgfn0VV1/NQdT5MLAhqqEvvtAfgjyvxbratVPRsKF4UIhg1eAGEZ1l504ZRUX6ad4cRJ0fAxuiGvriCzGCkmUNffowNWxVkiQeFoA47XvbNi77JnPJyNDvSW4pcWEMbIhq4OhRCT/8oB9El5hocIOoRkKXzpYFrERmEZod7tuX2eELYWBDVAOhIyjuJ2F9aWkKJEks+163jnU2ZB6nTwPffise2e3aKWjQgKd5XwgDG6IaCB1BMTVsfZdcogXP3fn+exnHjrGIgcxhwwY3NE2vr6ELY2BDVE2qCqxfLzI2tWtr6NyZKxTsgNNRZEah9yIHURfHwIaomnbtcuH0afER6t07ADdnLmwhdGVbaEaOyCiaBmRkiHsxJkZDjx7M2FwMAxuiairraACmhu2ka1cFCQmifiEjQ4bKRBwZ7McfJfz0k3hcX3ONgrg4gxtkcgxsiKqJqWF78ngQXLZ/8qQLu3ezmyRjhfY1XKRQMX5iiaqhsBDYvl10Ns2aqWjZkisU7CQ0A8fVUWS0detCAxtmhyvCwIaoGrZuleHziRUKaWk8RsFu+vdnATGZg9+vH6OQkqKiXTvOjVaEgQ1RNYQWlXIEZT8tW2po3lw8QLZvl1FQYHCDyLG2bgUKCsoGUQpcfGpXiJeIqBrKNuZzuXiMgl2VZW38fgmbNjFrQ8ZYvVr/mrV8lcPAhqiKfv1Vwr594kHXqZOKunWNbQ9FRuiyb9bZkFFWrdK/Zna4chjYEFURV0M5g9ibSBSFcz8bMsKZM8COHeLrK69U0LAhFylUBgMboiri/jXOULu22NMGALKyXDh0iBXiFF3r1+v7KKWlsa+pLAY2RFWgqnp9TUKChi5d2NnYGaejyEjll3kzO1xZDGyIqmDPHhdOniw7RkGBx2NwgyiiQh8moQ8ZokjTNP2e83p5jEJVMLAhqgLuAOos7durSE4WcwEbN7rh9xvcIHKM7GwJhw+XHaOgIiHB4AZZCAMboioIra9h4bD9ybJe25CfL+HLL5m1oegI7WtCp0SpYgxsiCqpuFjsOAwATZqoaN2aKxScIDQzx12IKVpC7zUGNlXDwIaokrZtk1FaymMUnIbnRlG0BQJi6hMAUlKA1FQeo1AVDGyIKonHKDjTpZdqaNtW/L6/+caF06cNbhDZ3ldfuZCfL0ZOAweCxyhUES8XUSWVpYYliccoOE3fviKw0TQJmzYxa0ORFVpfM2iQgQ2xKAY2RJVw7JiEPXtEYNOhg4rkZIMbRFHFOhuKptDsMAObqmNgQ1QJ69fzGAUn69FDgdcrisVDR9NE4ZaXJ6aiAODyy1U0bWpwgyyIgQ1RJfAYBWdLSAC6dRO/98OHXcjOZuU4RcbGjW4oiri/uBqqehjYEFVA0/RjFOLjteD5QeQsoWf1MGtDkcJl3jXHwIaoAvv2uXDsmPio9OqlICbG4AaRIUKnIMsCXaJwKwuaPR4N117LwKY6GNgQVSB0BMX6Gudq315F3bqizkZMFxjcILKdQ4ckZGeLx3LXrgpq1TK4QRbFwIaoAqyvIUAcr1C2zD83V8I337D7pPBiXxMe/GQSXURJCbBli8jYNGyo4ooruAOok7HOhiKJh+yGBwMboovYsUNGcbFYodCvn8JjFByOdTYUKYoCbNggguW6dTW0b89BVHUxsCG6CNbXUKjmzTW0aCEeODt3yigoMLhBZBvffONCbq4YOfXtG4DMuLnaGNgQXUTodEPZtvrkbGUBrt8vBacpiWoqtK8JnfKkqmNgQ3QBJ09K+O478eBKTVVQr55mcIvIDFhnQ5HA7HD4MLAhuoANG9jR0Ln69AnA5So7XoEZG6q5ggIxtQkArVqpaNaMg6iaYGBDdAGhB9Fx6SWVqVMH6NRJ1Nns3y/jl19YUU41s2mTjECgbJECB1E1xcCG6DxCj1GIjdXQvTsDG9L17as/fEIPSCWqjtBBFOtrao6BDdF5/PCDCz//LD4ePXsqiI01uEFkKqyzoXAqG0TJsobevZmxqSkGNkTnEVo7wfoaOlvXrgri40UdxPr1MjSWRFA1/fSThB9/FP1Nly4Katc2uEE2wMCG6DxYX0MX4/WKA1EB4PhxF/buZVdK1cNjFMKPn0ais/h8opgPAOrXV9G2LXcApXNxF2IKBx6jEH4MbIjOsnOnjKIisUIhLY3HKND5sc6GakpRgPXrxb2TmKihY0cOosKBgQ3RWbhRFlVGmzYqGjYUD6ItW2SUlhrcILKcXbtcyMkRI6c+fQJwMz4Oi6gENtnZ2Zg4cSI6duyInj17Yvr06SgpKanUexcuXIihQ4ciNTUV6enpWLFiRYRbS07Hrc2pMiRJP2ajuFjCjh2cjqKqYV8TGREPbPLy8jBhwgQUFhbipZdewsMPP4ylS5fiscceq/C9K1euxLRp0zBo0CDMmjULPXr0wJQpU7Bx48ZIN5sc6vRpcRgdALRtq6BBAy53oQtjnQ3VBOtrIiPiia+5c+ciLy8PixYtQnJyMgBAlmVMnToV9957L1q3bn3B97744osYOnQoHnzwQQBAjx49kJ2djZdeegm9e/eOdNPJgTZudEPTynYA5QiKLi70YNSMDDf+9Cefga0hKykoALZvF4FN8+YqWrTgICpcIp6xWb9+PXr27BkMagBgyJAh8Hq9yMjIuOD7jhw5gqysLKSnp5d7PT09Hd999x1Onz4dsTaTc7G+hqqiQQMNbduK4Obbb11gt0SVtXWrDL+fxyhEQsQzNpmZmRgzZky517xeL5o1a4bMzMwLvi8rKwsA0KpVq3Kvt27dGpqmISsrq1ywVBVuN2umAUCWXeX+3+nEMQriIxETo6F3by0s9wqvc3QYdZ3791exd68MTZOwebMHI0faP9PHe7rmMjI8wa8HDFDP29fwOldPxAObvLw8JCYmnvN6YmIicnNzL/i+sv929nvr1KlT7r9XlcslISkpoVrvtavExDijm2AKBw4AR46Ir3v3ltC4cXjvE17n6Ij2dR4+HHj1VfH1li2xuOuuqP71huI9XX0bNoj/d7mAESNiUbfuhX+W17lqDFtcpmkapEpsEHL2z2j/3bu8Mu89H1XVkJdXVK332o0su5CYGIe8vGIoCvdPWLzYDSAGANCnjw85Of6w/Lm8ztFh1HVOTQW83nj4fBI++0zF6dPFtt/7iPd0zRw9KmHPnngAQOfOCjStBDk55/4cr7MuMTGu0pmriAc2iYmJyMvLO+f1/Pz8ixYOh2ZmUlJSgq+X/VnnywJVViDg7BvkbIqi8poAWLtW/9D06eMP+zXhdY6OaF9nrxfo3l3Bxo1uHD7swg8/aGjVyhmFoLynq2fdutBl3oEKryGvc9VEfOKudevW59TS+Hw+HD58+KKBTVltTVmtTZnMzExIknRO7Q1RTfj9YkUUAKSkqLjqKnYiVHmhe5CU7SRLdCE8iy6yIh7Y9O3bF1u3bkVOSJ5t9erV8Pl8SEtLu+D7mjZtilatWmH58uXlXl+2bBnat29f7cJhovP58ksZBQVi/qBvXwUu1upRFXA/G6osVRUnwgNArVoaOndmYBNuEe++x40bh9q1a2PSpEnYsGEDFi1ahKeffhrDhw8vl7F59NFH0a5du3LvnTx5MlasWIF//vOf2LZtG5555hls2rQJkydPjnSzyWFCH0ZceklVlZqqIilJTD9t3OiGwmcVXcD337tw8qR49PbuHYDHU8EbqMqiUmMze/ZsTJ8+Hffffz9iY2ORnp6OqVOnlvs5VVWhnNUbDBs2DCUlJXj99dfx9ttvo3nz5vjnP//Jzfko7EJTw9zanKpKlsVZP0uWeJCbK+Gbb1zo0oXTmXQu9jWRJ2lly4wcQlFUnD5daHQzTMHtdiEpKQE5OYWOLkzLzQXatKkFVZXQpo2CDRvCu2qO1zk6jL7Oc+Z48OCDsQCAadNK8Yc/2HcXYqOvtZWNGROHDRtEcLN1a8FFC815nXXJyQmVXhXFSgJyvA0b3FBVHqNANcM6G6pIURGwbZu4N5o2VdGypaPyClHDwIYcL/QhxGMUqLqaNdPQsqUYVe/cKaOgwOAGkels3SrD59OPUbD7fkdGYWBDjlc25+3xaOjZkxkbqr6ywNjvl7BlC7M2VB7ra6KDgQ05Wna2hEOHxMege3cFCTxtg2og9GFVdu4YUZmy7LAkaejTh9nhSGFgQ44W+vBhfQ3VVO/eAbhcom6CdTYU6tgxCXv3inuiY0cVSUkGN8jGGNiQo33xBetrKHzq1AE6dRJ1Nvv3y/jlFxZRkMC9sqKHgQ05ViCgH6OQnKwiNdXZyykpPLg6is6HxyhEDwMbcqyvv3YhL0+MqPv0USDzGURhEPrQYp0NAYCm6UFufLyGLl0Y2EQSAxtyLNbXUCR06aIgIUHU2axfL0NlItDx9uxx4cSJsmMUFHi9BjfI5hjYkGOxvoYiweMRDy8AOHHChT172M06HffKii5+4siR8vPFid4AcNllCpo04Q6gFD6hxaGhATQ5E+troouBDTmSOIFZ1NdwoywKt/KBDetsnKy4WOw4DACNGqm47DLOTUYaAxtyJE5DUSS1aqWhaVPxANu2TUZReM9VJQvZtk1GSQmPUYgmBjbkSGWjaLdbC9ZDEIWLJOlZm9JSKThiJ+fhNFT0MbAhxzl4UEJ2tn6MQq1aBjeIbCn0IcbpKOcqyw6LYxQY2EQDAxtyHC7zpmjg8Qp07JiEPXvE775DBxWXXMJFCtHAwIYcJ7S+hlubU6QkJenHK+zdK+PXX1lc4TQ8RsEYDGzIUQIBYMMGHqNA0RFamM5l387D+hpjMLAhRwk9RqFvXx6jQJHFOhvnUtXyxyh07crAJloY2JCjlB9BMTVMkdWli4JatXi8ghPxGAXjMLAhRwkNbLgxH0WaOF5BBNAnT7rw/ffscp2CtXzG4aeMHCM3F/jqK3HLX3GFgsaNuUKBIi90OmrdOk5HOQWzw8ZhYEOOEXqMAgv5KFpCH2pc9u0MRUVix2EAaNJERevWHERFEwMbcgymhskILVtqaNZMP16hsNDgBlHEbd0qo7SUxygYhYENOYKm6dMAHo+Gnj2ZsaHoCD1ewefj8QpOwGXexmJgQ46QnS3h8GFxu19zjYKEBIMbRI7CZd/OUjblKI5RYHY42hjYkCNwBEVG6tNHP16BG/XZ27FjEvbuFb/jTp1UJCUZ3CAHYmBDjsD6GjJSnTpA586izmb/fhk//8yiC7tiX2M8BjZke36/WBEFACkpKq6+mrukUfRxdZQzMDtsPAY2ZHtffimjoEA/RsHFu54MEBrYsM7GnkKPUUhI0NClCwMbI7CLJ9tjapjMoHNnFbVrizqbjAwer2BH33/vwsmT4rHap08AHo/BDXIoBjZkexkZTA2T8dxuBFfInD7twq5d7H7thke2mAM/WWRrOTniRG8AaNtWQcOG3AGUjMNl3/YWmh3u35/ZYaMwsCFb27jRDVUV9TUcQZHRytfZsIDYTkKPUWjWTEXLlhxEGYWBDdka62vITFq00NCihSiu2b5dRkGBwQ2isNm6VYbPVzaI4jEKRmJgQ7alaXq6PyZGQ48ezNiQ8coCbL9fwpYtzNrYRejJ7azlMxYDG7KtH3904cgRcYt3764gPt7gBhGBdTZ2tXatCFJdLh6jYDQGNmRba9boo+HrrmNHQ+bQu3cAsizqL9auZWBjB4cPS/jhB9HfdO2qoG5dY9vjdAxsyLZCHxoDBjA1TOaQmCgefgCQmenCoUMsxrC60L7muuvY1xiNgQ3ZUlERgvULjRuraNOGu6GReYQ+/NasYdbG6sqmoQBgwABmh43GwIZsafNmGaWlYiQ8YABXKJC5hE6NcjrK2nw+YP16/Sy61FQOoozGwIZsKXQUzGkoMpurrlJRr554AG7cKKO01OAGUbVt2yajqKhsEMWz6MyAvwKypbJRsNutoW9fpobJXFwuPeAuKpKwdSuXfVtV+UEU+xozYGBDtpOVJSE7W1/mXbu2wQ0iOo/Q6SjW2VjXunUiKJUkjZuAmgQDG7IdroYiK0hLC8DlKlv2zYyNFR09KmHvXvG769xZRXKywQ0iAAxsyIbKBzYcQZE5JSWJhyEAHDgg48gRVrhbTehuw+xrzIOBDdlKSQmwaZMYQTVsqOKqq7hCgcyL01HWFroJKAMb82BgQ7ayZYuM4mIu8yZrKL/sm9NRVuL368u8k5NVdOzIQZRZMLAhW2F9DVlJ+/YqUlLEA3HDBjd8PoMbRJW2c6eM/HwxcurXT4HMuNQ0GNiQrZSlhmVZQ1oaU8Nkbi4X0L+/CMALCyVs28ano1Vwt2HzYmBDtnHokIQffxSdTZcuCurUMbhBRJUQ+lBknY11hP6uQk9sJ+MxsCHb4EF0ZEX9+imQJC77tpJjxyTs3i1+Vx06KKhfXzO4RRSKgQ3ZRtlGWUD5okwiM7vkEi247HvfPhlHj7Li3ezY15gbAxuyhdLS8gfRXX01VyiQdYROR/FQTPMLnYYqq5Ei82BgQ7awfTsPoiPrKr+fDaejzCwQADIyRGBTp46GLl0Y2JgNu3+yhc8/5w6gZF0dOqhIThZZxvXruezbzHbulHHmjBhEpaUF4GaCzXQY2JAtrF4tRrkul4b+/RnYkLXIsr6ypqBAwvbtzNqYVVlfAwCDBrGvMSMGNmR5WVn6Mu/u3RUkJRncIKJqCH1IrlrFNIBZrV4tfjeSpHH1pUkxsCHLK+toAGDQIHY0ZE0DBgQgy2LZcOg9TeZx6JCEffvK9spSkZLCZd5mxMCGLC90dDt4MFPDZE1JSSLjCACZmS5kZnLZt9mE1vKxrzEvBjZkafn5wNatYgTVrJmKK67gMm+yLk5HmVv57DADG7NiYEOW9sUXbvj9YmQ7eDBP8yZrGzxYn0rldJS5FBQAGzeKQVSjRirateMgyqwY2JClcQRFdnL55SpatBAPzK1bZeTmGtwgChKnr4uR06BBHESZGQMbsixVBT7/XIyg4uM19OrFwmGyNknSazcCAQnr1jFrYxahy7xZX2NuUQlsMjIyMHLkSKSmpmLQoEH48MMPK/W+Nm3anPO/a6+9NsKtJav4+msXTp4Ut3BaWgAxMQY3iCgMWGdjPpqmZ4fj4jT07s1BlJlF/FPz9ddfY9KkSbjxxhsxbdo0fPXVV5g+fTq8Xi/Gjh1b4ftvv/12pKenB7/3eDyRbC5ZSOg0VGhtApGV9eypoFYtDQUFEtascUNRxAZ+ZJxdu1w4dkwMovr0URAXZ3CD6KIiHti88soraNeuHZ555hkAQI8ePfDLL7/gxRdfxJgxY+Cq4FCfSy+9FB07dox0M8mCQkezAwcyNUz24PUC/fsHsHSpBzk5EnbulHHNNQzcjcS+xloiOhXl8/mwdetW3HDDDeVeHz58OE6cOIE9e/ZE8q8nG/vlFwm7d4thbMeOCho04EZZZB/lp6OYrjEaFylYS0QzNocPH4bf70erVq3KvX7ZZZcBADIzM3H11Vdf9M9488038fzzzyMuLg69e/fGQw89hEaNGtWoXW43a6YBQJZd5f7fStas0W/dIUMUU/9OrXydrcRO13nIEBWSpEHTJKxe7cGTT5rrYWqna12RY8ckfP21CC6vvlpB8+YSgOgsiXLSdQ6niAY2uf9dq5iYmFju9bLvcytYyzhy5Ej069cPKSkpOHDgAF577TWMHz8eixcvRp06darVJpdLQlJSQrXea1eJidabMF63Tv967FgvkpK8xjWmkqx4na3IDtc5KQno0QPYsgXYt8+FM2cS0LKl0a06lx2udUUWLdK/vvFG2ZDnhxOuczhVObDJz8/H8ePHK/y5pk2bBr+WLrDg/0Kvl/nb3/4W/Lpbt27o0qULRo8ejY8//hj33HNPJVtcnqpqyMsrqtZ77UaWXUhMjENeXjEUxTqbTRUVAZ9/Hg9AQsOGKlq0KEZOjtGtujCrXmersdt1vu46D7ZsEQH7xx+X4je/MU/Wxm7X+mLmz49B2aOyT59i5ORE79/rpOtckcTEuEpnrqoc2KxevRqPPPJIhT+3aNGiYFbl7MxMXl4egHMzORW58sor0bJlS3z//fdVet/ZAgFn3yBnUxTVUtdkzRo3iotFUDxwYACqqkK1QPOtdp2tyi7X+brr/Jg+XQQ2K1fKuPtun8EtOpddrvWFFBUB69aJaah69VR07BhAwID40u7XOdyqHNiMHj0ao0ePrtTP+nw+eDweZGVloW/fvsHXf/zxRwBA69atq/rXQ9NYJOp0K1bot+3115tnFEsUTu3aqWjSRMVPP7mwebOMggKgVi2jW+UsGRn6IGro0AAqWMRLJhHRX5PX60WPHj2wYsWKcq8vW7YM9erVQ7t27ar05+3duxcHDx5EampqOJtJFhII6KtEEhK4URbZlyTpK3B8Pu5CbISVK/VrPmwYB1FWEfFPyn333YfbbrsNjz32GIYPH46vvvoKn3zyCZ566qlye9gMGjQIjRo1wuzZswEAb7/9No4cOYLu3bsjOTkZP/zwA15//XU0bNiwUhv7kT3t2CHj9Glx3wwYEEBsrMENIoqgYcMCePddMR31n/+4MXw4H67REggAn32mH9nCQZR1RDyw6dSpE1599VU8//zzWLRoERo2bIjHHnvsnOBEURSoIYUSLVu2xKpVq7B8+XIUFhYiKSkJaWlp+P3vf1/l2hyyj+XLOYIi5+jVS0Fiooa8PAmff+6Gzyc28KPICx1EXXcdB1FWEpXcZlpaGtLS0i76M2vXri33/YABAzBgwIBINossRtP0+hq3W+MOoGR7Xq+Yjvr0Uw/y8iRs2iSjf39mDqIhtJZv6FD2NVbCUiiyjD17XDh8WNyyvXopqFvX2PYQRUNogXxoxpIiJ3QQJcsadxu2GAY2ZBks5CMn6t8/gJgYsRp05Uq3JbY2sLp9+1w4dIiDKKtiYEOWwdQwOVGtWkC/fmL66dgxF776it12pIX2NRxEWQ8/IWQJP/0k4bvvxAqFDh0UNG7M/YzIOa6/3h/8mtNRkcdBlLUxsCFL4DQUOdmgQQpcLhHML1/uAfcpjZyjRyV8+60YRKWmKmjShBfbahjYkCUwNUxOlpKioUcPMR2VleXCgQPsuiOFgyjr46eDTC8nB9i8WYygWrRQceWVrJ4k5wldHRUa6FN4hU71cRrKmhjYkOmtWOGGoojzWoYNC6CCQ+GJbCn0Ics6m8g4eVLsFQQALVuquOoqDqKsiIFNmBw9KuHMGaNbYU9Ll3qCX48Y4b/ITxLZV7NmGlJTxXTUN9/IOHqUEX64LV/uhqqK6zp8uJ+DKItiYBMGGzfK6NYtAX36JODYMX4SwunMGWD9ejGCatxYRefOHEGRc4VORy1bxqxNuC1dql9TnstlXQxswuDAARcCAQnHjrkwfz47m3BaudINv18Ei+npnIYiZwt92C5e7LnIT1JVnT4tBqkA0KyZivbtOYiyKgY2YZCWpnc2odMmVHPLlunXc/hwTkORs11xhYq2bcV01M6dnI4Kp5Ur9Vo+DqKsjYFNGLRureGqq0Rn89VXMg4f5iciHPLygC++ECOoSy9V0bUrR1BEI0aEDqSYIQ6X0EEpB1HWxsAmTG68Ue9slixhZxMOq1a54fPpIygX71aicoENp6PCg7V89sJHRZiErtZZsoSdTTiEBogs5CMSLr9cRbt2IkP85ZcyfvqJGeKaYi2fvTCwCZNWrcovxTx4kJ+MmigoANatE4FN/foqunVTDG4RkXlwOiq8WMtnLwxswih0OopFxDWzerUbpaX6CEqWDW4QkYkwQxw+obV8DRuyls8OGNiEUWikzzqbmlm8mNNQRBdy2WX6goUvv5Rx5AgzxNW1fDlr+eyGv8IwatlSQ4cOorP59lsZ2dnsbKojNxdYs0YENvXqqcHD/4hIx+mo8Fi4UM94jRzJaSg7YGATZqHZBU5HVc/y5fo01KhRnIYiOh9OR9XciRNScDVUs2YqunXjNJQdMLAJs9DOZtEijqKq49NP9U569GiOoIjOp3VrDVdfzf2zamLJEn1TvpEjeTaUXTCwCbMWLTR06iQ6m927Zezfz0tcFceOScFtzVu0UNGpE0dQRBcSumBh0SJmbapqwYLQQRRr+eyCT90IGDNGzzJ8+imzNlWxZIl+uu7o0RxBEV1MaE3IJ5+4oWkGNsZijhyRsGOHGERdeaWCdu04iLILBjYRMHJkALIsephPP/VA5eel0jiCIqq85s01dO8uPif798vYvZtdemWFFg2PGsW+xk74KYiA+vU1pKWJ6agjR1zYvp3Vr5WRnS3hyy/FtbrqKgVXXMGIkKgiN92kP5Tnz+d0VGUtWKBn00eNYi2fnTCwiZDQ6aj58zkdVRmhNQLM1hBVzogRfng8IkO8YIEbCndHqNC+fS7s2SMGUV26KGjRgnN4dsLAJkKGDQsgPl58WJYs8cDnM7hBJqdp5euROIIiqpzkZOC668RA4NgxV7D4ni5s4UK9r+HKS/thYBMhtWoBQ4eKzubMGSm44Ryd3+7dLhw4IDrka64JoEkTjqCIKmvsWE5HVZaq6ltKuFxauY0OyR4Y2ETQ2LGcjqqsjz/mNBRRdQ0aFEDt2mIwsGyZG0VFBjfIxLZulXH4sHj09e2roEEDDqLshoFNBKWlKUhJEQWwq1a5kZdncINMyu/Xp6G8Xo3TUERVFBurn1VXWChh1SoOpC5k7lx9EDVuHPsaO2JgE0Fut1j6DQClpRKPWLiANWtknDwpbsVhwwKoW9fY9hBZUejqqE8+YV9zPgUF+gHFtWtrGDaM2WE7YmATYTfdpI8I/v1vjqLOhyMooprr1UtBo0YiQ7x2rYxjx7i75dn+8x83ior0IxTi4gxuEEUEA5sI69RJxZVXivWX27e78eOP7GxCnTypp80bNFCD+/8QUdW4XMDNN4uBgaJI5erWSJg3T78m//M/HETZFQObCJMkYPx4/QP00UfsbEItXOhGICCCvbFj/XAzqUVUbaEZz48+8vCIhRCHD0vYuFF0MK1a8SRvO2NgEwU33RSA2y16mHnzPPBzoBAUOg31P//D+W6immjVSkOvXuJzlJnJXc9DhdYdjRvHc+jsjIFNFKSkaBgyRHQ2J064sHYtOxtA7F2za5e4Fp07K2jThiMooppihvhcqqoPoiRJK7cVB9kPA5soufVWdjZn+/BD/TqU1QYQUc2kp+t72ixe7EZBgcENMoENG2QcOiQed336KGjcmHN0dsbAJkr69VPQsKHISKxe7cbx487OgxYV6anhuDit3NlaRFR98fH6kSRFRRIWL+ZA6v339Wtwxx3sa+yOgU2UuN16FX4gIOHjj51dJbt4sRt5eWXLLgOoU8fgBhHZSGiGODQz6kTHjklYsUL0tykpavCoG7IvBjZRdMstemczZ44XqoNLSmbP9ga/vuMOnhBKFE4dO6po21ZsnbBzp4w9e5zb1c+b5wmuvBw/3g+vt4I3kOU59243QKtWGvr2FaOF7GwXvvjCmUXEu3a58NVX4t9+9dUKOnd2cIRHFAGSVH7K5b33nJm1UVVgzhz9337bbZyGcgIGNlF2113sbM6e7+ayS6LwGzvWj/h4UST7ySce5Ocb3CADZGToRcP9+gXQogWLhp2AgU2UDRkSwKWX6gdjHjnirKd6QQEwf74IbBIStHJHThBR+CQmIrisubDQmTsRs2jYmRjYRJnbrX/AVFUqlyZ1ggULPCgsFMHc6NF+1KplcIOIbCw0Q/zuu87aifjXXyV89pkoGq5fXw3uJUb2x8DGALfd5g/uRPzBBx6UlhrcoCjRNODtt/VAbsIEjqCIIqldOxU9eogH+oEDMjZtck5d37vv6kXDt97qh8dZY0hHY2BjgAYNNKSni87m5EkXli1zxtLvDRtk7N0rOtZu3RS0b8+iYaJIu/tufQDxzjvOeLqXlOjTUG63Vi5zRfbHwMYgoR+0t95yxvrDN9/U/52//S2XeBNFw/XXB1C/vhhErFjhxs8/27+ub8ECN06dEo+3ESMCaNjQQXNwxMDGKD16KMF9Jr78Usb27fb+VWRlSVi1SmSmGjdWcf31nO8migavF7j9djGQUhQJb71l76yNpgFvvKEPon7zGw6inMbeT1MTkyTg3nv1D9xrr9k7axOalbr7bj/czph9IzKFO+/0w+sVWYvZs722Xvq9aZM+5d2lC/fJciIGNgYaNUpPES9f7kZ2tj1TxHl5wL//LUaJ8fEabruNIyiiaGrQQD/ROj9fsvUxC2++qf/bmK1xJgY2BoqJAe65R3Q2miaVS5/ayXvveYNLvMeO9SMpyeAGETnQ736n1/W9+aYXARvOBu/f78LKlSKwufRSNbhIg5yFgY3B7rjDF9wddO5cD3JyDG5QmBUXA6+/LjoaSdLwu99xBEVkhDZtVAwaJB70P/3kwtKl9psP/te/9MHh737n4xJvh2JgY7CkJP1wzKIiCe+8Y6+szb//7cHJk/rqhNatuTqByCiTJukDi5df9tpqw74jRyQsWCCCtbp1tWDBNDkPAxsT+O1vfZBl0cO88YYXBQUGNyhM/H7glVf0QG3yZGZriIzUq5eCDh3Easxdu2SsWWOfDftee80b3JBv4kQfdzV3MAY2JtCihYYxY0SK+MwZ+2RtFi5048gRcYsNGBBAaipXJxAZSZKA3/9eH2A891yMLbI2J0/qBdHx8RruuYeDKCdjYGMSv/99KVwu0cO89poHhYUGN6iGVLX8fPcDD7CjITKDYcMC5fbQ+uIL62dtXn/dg+Jika25/XY/kpMNbhAZioGNSVx2mYaRI0XW5tQpF2bPtnbV28KFbuzfrx+f0KOHYnCLiAgAXC7gwQf1gcbMmdbO2pw4IQX3yfJ6uUCBGNiYypQpPkiS6GFeecWLoiKDG1RNfj/w97/HBL9/+OFSSPbcoofIktLTA2jTRgw2duyQsWGDdbM2L73kRVGR6GDuuMOPxo0tHKVRWDCwMZE2bVQMHy6yNidOuDBrljVrbebN8yA7W9xavXsH0LcvszVEZuJyAX/4g57ZmDHDmlmbn3+W8N57IrsdF6dxypsAMLAxnYce8gVrbV56yYtTp6yV6igpAWbO1AOyRx4pNbA1RHQhI0YEcOWVeq3NsmXW29fmn//0orRU9JF33+1HgwYWjM4o7BjYmMwVV6gYP17f+vyFF6yVtXn/fQ9+/lncVoMGBdCtG1dCEZmRLAOPPaYPPP761xj4LbT1S3a2vhKqVi0N99/PQRQJDGxM6I9/9CEuTow83n3Xg8OHrZG1yckRy0fLTJvGjobIzAYNUtCrl5j+zspyYc4c6yxaePLJmOC+Nb/9rY8roSiIgY0JXXqpFjy8zeeT8MwzMRW8wxyeey4GOTmioxk92s99a4hMTpKAxx/XByAzZ1pjg9BNm2QsXy6CsPr1Vdx3H2trSMfAxqTuv9+H5GQRGCxY4MHWreZetfDDDy68845exPfnPzNbQ2QFnTuruPFGMQd18qSrXNbVjBQF+POf9Tb+6U+l3GWYymFgY1KJicDDD+ujkGnTYkx7Gq+mAY8/rqeF/+//fFxySWQhf/pTKWJiyo518WDfPvM+Gj780IPdu8VALzVVwf/8j0k7RjJMxO/eTZs24cEHH8TAgQPRpk0bPPXUU5V+r9/vx3PPPYfevXujQ4cOuP3227Fv374IttZc7rjDj/btxaqFPXtkvPuuOee/ly51Y80asaKicWOmhYmspkULDfffLz63gYCEadPMufz72DEJTz+tZ2umTy+Fy7wxGBkk4rfE+vXrsXfvXnTr1g2JiYlVeu+MGTPw4YcfYvLkyXj11Vfhdrtx55134sSJExFqrbnIMvDssyXB7599Nga//GKuQuIzZ4BHHtE7mqeeKkV8vHHtIaLquf9+H5o3F9Pfmze78ckn5lv+/fjjMcjNFX3g2LF+9OzJPbLoXBEPbB5++GEsX74cM2bMQO3atSv9vmPHjmHu3Ll48MEHcfPNN+Paa6/Fv/71L2iahtmzZ0ewxebStauKW28VI6n8fAkPPhhrqpHU00/H4MQJcRsNHepHejrTwkRWFBcHPPOMPpB67LFYHDtmnoHUmjUyFi4UWevkZBVPPsk6Pjq/iAc2rmrmCTdu3AhFUXDDDTcEX6tVqxYGDBiAjIyMcDXPEh5/vBT164uR1OefuzFvnjlGUhkZMubMEfvsJCRoePZZHp1AZGWDBinBQuIzZyRMnWqOgVRODvCHP8QGv//LX0qRkmKChpEpmeMJeR6ZmZlISUlB3bp1y73eunVrLF26FKqqVjtocrutNSlbrx7wwgs+jB8vPtiPPRaLfv2K0aRJzT7Ysuwq9/9Vcfo0cP/9ekfz+OM+NGsmAWBkc7aaXGeqPF7n8Jg504fNm904cULCZ5+58emnXowbVz4TG81rrWnAQw/F4JdfxN+Vlqbg1ltVSJL9f8+8p6vHtIFNXl7eeaeu6tSpA7/fj6KiItSqxho/l0tCUlJCOJoYVbfcAqxYAcyZA+TlSbj33nisWwd4wlBPnJgYV6Wf1zRg4kTg11/F94MHA1OnxsDlMvcyUaNV9TpT9fA610xSEvDGG8Do0eL7hx+OwXXXxeCKK8792Whc6/ffBxYv1tv2wQcykpOt14fXBO/pqqlyYJOfn4/jx49X+HNNmzaF11uz4wCk88xraDXMi6qqhrw8ax6b/eSTwLp1cfjpJxc2bQIefNCHJ5+s/h7osuxCYmIc8vKKoSiV30zvrbfcWLhQBDHJyRpeeKEYublMC19Ida8zVQ2vc/j06weMG+fF3Lke5OcDo0apWLWqOLgwIFrXet8+CffdF4eyTPBzz5UgIUFBTk7E/kpT4T2tS0yMq3TmqsqBzerVq/HII49U+HOLFi1C27Ztq/rHByUmJiIvL++c1/Py8uDxeBBfg6U3gYA1b5BatYBZs4oxYkQ8/H4J//qXF127Khg2rGYFu4qiVvqabNok49FH9YD1+edLkJKimHaPHTOpynWm6uN1Do9nninBV1+5cOCAjD17XJg61YsXXywpV0cXyWudmwvcemsCCgrEX3jzzX6kp/sd2dfwnq6aKgc2o0ePxuiyHGUEtW7dGqdOncKZM2fK1dlkZmaiZcuW1a6vsbouXVQ88UQpHntM1Lfce28sFi8uQocOkb/pjxyR8L//GxvciO+++3y4/noH9jJEDlCrFvD22yUYMiQeRUUS5s714LLLVEyeHPl9qhQF+N3v4pCdLfr5q69W8Pe/l1TwLiLBtNFB79694XK5sGLFiuBrhYWFWLt2LdLS0gxsmfHuucePUaPEFFRRkYRbb43DkSORLdo9dUrCuHFxOHVK3DL9+wfKnQxMRPbTpo2K55/XA4rp02Pw6aeRLc3UNOCPf4wJbvqZnKzivfeKuT8WVVrEi4ePHj2KXbt2AQCKi4tx+PBhrFy5EgAwdOjQ4M8NGjQIjRo1Cu5R06BBA4wbNw4zZ86E2+1Go0aN8M477wAAJkyYEOlmm5okAS++WIKff5awbZsbx4+7cPPN8ViwoAiXXhr+Wpf8fOCWW+Lwww9iG/NWrVS88UYxZHMfX0VEYTB6dACHDpVixgxRVzd5ciySk0tx883h/7s0DZg+3YsPPhDT3R6PhlmzStCsGWv4qPIiHths27atXE3Ohg0bsGHDBgDA/v37g68rigJVLT+dMm3aNMTHx+OFF15Afn4+OnTogNmzZ6NevXqRbrbpxcYCs2cX44YbEpCZ6UJmpgsjR4rgJpznNJ08KeGWW+Lw7bciimnYUMW8eUU4axU+EdnY73/vw08/SZgzxwu/X8Jtt8UgJgbo2zd8f0fZmXNvvCGCGknS8PLLJejTh7sLU9VIWk2XGVmMoqg4fbrQ6GaEzZEjEkaNisfhw2KKqGFDFXPmFFeq5sbtdiEpKQE5OYXnLUzLypJw++16piY5WcWiRcW48koWsVVFRdeZwoPXObICAWDSpFgsWiT2mJBlYMaMUkyY4KvxxpzFxcDUqbH45BN9/4q//a0Ed91V/VWfdsB7WpecnFDpVVGmrbGhymnaVMPixUVo2VLc9L/+6sKIEfH48ENPjXYMXbHCjcGDE4JBzaWXqli8mEENkVO53cCrr5bgpptEsKEoYuO8KVNiUFBQ/T/34EEJI0bEB4Mal0tsIeH0oIaqj4GNDTRurOE//ylCt24iZVtcLGHKlFjccUccDh6s2lDq+HEJv/tdLCZMiENennjvFVcoWLq0CG3aMKghcjK3G3j55RI88IC+Muqjj7zo3z8Ba9fKVRpM+f3AG2940K9fQnCqOz5e1NSMH8/VllR9DGxsIiVFw6efFuG22/QO57PP3Lj22gQ89FAM9uy5+K86K0vCE0/EoHv3BCxYoKeDhw/3Y+XKIhbvEREAwOUCnnjCj/ffF2fEAcChQy6MGxePMWPi8Nln8kX3msnNBWbP9qBnzwT8+c+xKCoSA6jmzVUsX16E4cMZ1FDNsMbGhlascOPBB2Nw8mT5YKZVKxW9egXQvLmGxEQNJSUSjh6NwebNCnbvLr/EqW5dDU8+WYJx4wI82LKGOE8eHbzO0VN2rb/6qgiTJnmxbVv5dSiXXKKiRw8FbduqSEnR4PcDx45J+PZbGdu2yfD5yncqEyf68Kc/laIap+TYGu9pXVVqbBjY2FR+PvDKK168/ro3OCKqjNhYDbff7scDD/hQv76jbo2IYecUHbzO0RN6rX0+FUuWuPHMMzE4eLBqkwD9+wfw0EOl6NKFv6/z4T2tY2BzEU4JbMrk5gLz53uwYIEHX3/tCu4aHEqSNLRvr2LkSD9uuimABg0cdUtEHDun6OB1jp7zXWtFAdatk/HRRx5s2OBGbu75B1SXXqoiPT2Am2/2R2XHdCvjPa1jYHMRTgtsQhUWAnv2uPDrry4UFgK1a0u47LJYNGtWiLg4Z39oIomdU3TwOkdPRddaVYH9+104elTCyZMSvF4xvd2unYqGDR31yKkR3tO6qgQ2Ed+gj8wjIQHo1k0FID4g4kMD5OTAkQfLEVFkuFxA27YqanAOMlG1cVUUERER2QYDGyIiIrINBjZERERkGwxsiIiIyDYY2BAREZFtMLAhIiIi22BgQ0RERLbBwIaIiIhsg4ENERER2QYDGyIiIrINBjZERERkGwxsiIiIyDYY2BAREZFtSJqmOeoMeU3ToKqO+idflCy7oCiq0c2wPV7n6OB1jh5e6+jgdRZcLgmSJFXqZx0X2BAREZF9cSqKiIiIbIOBDREREdkGAxsiIiKyDQY2REREZBsMbIiIiMg2GNgQERGRbTCwISIiIttgYENERES2wcCGiIiIbIOBDREREdkGAxsiIiKyDQY2REREZBsMbIiIiMg2GNjQOXbv3o22bduiU6dORjfFdhRFwaxZs3DbbbehR48e6NatG2699VZs2bLF6KZZVnZ2NiZOnIiOHTuiZ8+emD59OkpKSoxulu2sWLECkyZNQlpaGjp27Ijhw4fjo48+gqqqRjfN1goLC9G3b1+0adMGu3btMro5lsDAhsrRNA1PP/00kpOTjW6KLZWUlOCNN97AlVdeiRkzZuD5559HgwYNcNddd2HdunVGN89y8vLyMGHCBBQWFuKll17Cww8/jKVLl+Kxxx4zumm28+6778Lr9eKhhx7C66+/joEDB+Kvf/0r/vGPfxjdNFt79dVXoSiK0c2wFLfRDSBz+fTTT5GTk4MxY8Zgzpw5RjfHdmJjY7FmzRrUqVMn+Frv3r1x8OBBvPPOO+jfv7+BrbOeuXPnIi8vD4sWLQoG47IsY+rUqbj33nvRunVrg1toH6+//nq5AU+PHj1QVFSEDz/8EFOmTIHX6zWwdfaUmZmJjz76CA8//DCeeOIJo5tjGczYUFBeXh6ee+45PPLII/B4PEY3x5ZkWS4X1ACAJEm48sorcfz4cYNaZV3r169Hz549yz1whwwZAq/Xi4yMDANbZj/ny+K2bdsWpaWlOHPmTPQb5AB//etfMW7cOLRs2dLoplgKAxsKeuGFF3DVVVcxaxBlqqri66+/ZnahGjIzM8+5bl6vF82aNUNmZqZBrXKOL7/8EnXr1sUll1xidFNsZ+XKldi3bx/uu+8+o5tiOQxsCACwd+9ezJ8/H4888ojRTXGcOXPmIDs7G3fddZfRTbGcvLw8JCYmnvN6YmIicnNzDWiRc+zatQsLFizAhAkTIMuy0c2xleLiYjz77LP4wx/+gFq1ahndHMthjY1N5efnV2pqo2nTpvB4PHjqqacwfvx4Zg2qoSrX+uw6hO3bt+Mf//gH7r77bnTr1i1STXQcTdMgSZLRzbCtEydOYPLkyUhNTcU999xjdHNs57XXXsMll1yC0aNHG90US2JgY1OrV6+uVPZl0aJFyMrKQmZmJmbOnIm8vDwAQGlpKQAxIo6JiUFMTExE22tlVbnWbdu2DX6/b98+TJo0CQMHDsQf//jHSDbRthITE4P3bKj8/HwG6RGSn5+Pe+65B7GxsXjttddYjxdmR48exTvvvINXXnkFBQUFAICioqLg/xcWFiIhIcHIJpqepGmaZnQjyFj/+te/8PLLL1/wv99zzz2YOnVqFFtkf4cPH8b48ePRqlUrvPXWW1xRUk233XYbateujddeey34ms/nQ5cuXTBlyhTcfffdBrbOfkpLSzFx4kQcPHgQ8+bNQ+PGjY1uku1s27YNd9xxxwX/e4cOHfDxxx9HsUXWw4wNYdSoUejevXu51xYuXIjly5dj1qxZaNSokUEts6cTJ07g7rvvRkpKCl599VUGNTXQt29fvPbaa8jJyUFSUhIAkUHz+XxIS0szuHX2EggE8MADD2Dfvn344IMPGNRESNu2bfH++++Xe23v3r2YMWMGnnzySaSmphrUMutgYENo0qQJmjRpUu617du3Q5ZlXHPNNQa1yp5KSkrwv//7vzh16hSmTZuGH3/8sdx/79ixozENs6hx48bhgw8+wKRJkzBp0iScOnUKzz77LIYPH86pqDB76qmnsG7dOvzxj39ESUkJvvnmm+B/u+yyy1jkGiaJiYkX7HevuuoqXHXVVVFukfUwsCGKopMnT2Lfvn0AcN5lnPv37492kywtMTERs2fPxvTp03H//fcjNjYW6enpnDqNgI0bNwLAeXcafv/99zkIItNgjQ0RERHZBvexISIiIttgYENERES2wcCGiIiIbIOBDREREdkGAxsiIiKyDQY2REREZBsMbIiIiMg2GNgQERGRbTCwISIiIttgYENERES2wcCGiIiIbOP/AQI7a5d2LUQrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    z_all = jacobian(net, x_all)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    z_support = z_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    z_query = z_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=z_support, targets=y_support, strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(z_all)).mean\n",
    "    lower, upper = likelihood(gp(z_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "\n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    #plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48930c65-2172-43a9-a3e0-24e095e76206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test, please wait...\n",
      "-------------------\n",
      "Average MSE: 6.417262971799821 +- 10.081227661399424\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "tot_iterations=500\n",
    "mse_list = list()\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    z_all = jacobian(net, x_all)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    z_support = z_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    z_query = z_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=z_support, targets=y_support, strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(z_query)).mean\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc363e85-24b8-4d32-839d-90f8d957377e",
   "metadata": {},
   "source": [
    "## Sketching FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8235e575-f82f-4e83-8a8d-c35144afb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1430333-1852-44f2-964f-0472624477b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(Y, W, Psi):\n",
    "    \"\"\"\n",
    "    given Y = A @ Om, (N, k)\n",
    "    and W = Psi @ A, (l, M)\n",
    "    and Psi(X) = Psi @ X, (N,...) -> (l,...)\n",
    "    where Om and Psi and random sketching operators\n",
    "    returns Q (N x k), X (k x M) such that A ~= QX\n",
    "    \"\"\"\n",
    "    # Perform QR decomposition on Y to get orthonormal basis Q\n",
    "    Q, _ = torch.linalg.qr(Y, mode='reduced')\n",
    "    \n",
    "    # Apply Psi to Q and then perform QR decomposition\n",
    "    U, T = torch.linalg.qr(torch.matmul(Psi, Q), mode='reduced')\n",
    "    \n",
    "    # Solve the triangular system T @ X = U^T @ W for X\n",
    "    # PyTorch does not have a direct equivalent to scipy.linalg.solve_triangular,\n",
    "    # so we use torch.linalg.solve which can handle triangular matrices if specified.\n",
    "    X = torch.linalg.solve(T, torch.matmul(U.T, W))\n",
    "    \n",
    "    return Q, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a743d193-5aea-4fa0-9749-420b58d3e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_low_rank_approx(Y, W, Psi):\n",
    "    \"\"\"\n",
    "    Perform a symmetric low-rank approximation of the matrix A.\n",
    "    \"\"\"\n",
    "    Q, X = low_rank_approx(Y, W, Psi)  # Assuming Psi is now correctly handled\n",
    "    k = Q.shape[-1]  # Dimension of the sketches\n",
    "    \n",
    "    # Concatenate Q and X.T along columns to form a larger matrix\n",
    "    tmp = torch.cat((Q, X.T), dim=1)  # Correctly access the transpose\n",
    "    \n",
    "    # Perform QR decomposition on the concatenated matrix\n",
    "    U, T = torch.linalg.qr(tmp, mode='reduced')\n",
    "    \n",
    "    # Extract T1 and T2 from T\n",
    "    T1 = T[:, :k]\n",
    "    T2 = T[:, k:2*k]\n",
    "    \n",
    "    # Compute symmetric matrix S\n",
    "    S = (T1 @ T2.T + T2 @ T1.T) / 2\n",
    "    \n",
    "    return U, S\n",
    "\n",
    "# Example usage\n",
    "N, k, l = 100, 10, 50  # Example dimensions\n",
    "Y = torch.randn(N, k)  # Random Y matrix\n",
    "W = torch.randn(l, N)  # Random W matrix\n",
    "Psi = torch.randn(l, N)  # Random Psi matrix\n",
    "\n",
    "# Call the function\n",
    "U, S = sym_low_rank_approx(Y, W, Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82aaa719-6caf-4b9e-9786-7afea76dce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_rank_eig_approx(Y, W, psi, r):\n",
    "    \"\"\"\n",
    "    Returns U (N x r), D (r) such that A ~= U diag(D) U^T using PyTorch.\n",
    "    \"\"\"\n",
    "    # Obtain symmetric low-rank approximation\n",
    "    U, S = sym_low_rank_approx(Y, W, psi)\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    D, V = torch.linalg.eigh(S)\n",
    "    \n",
    "    # Truncate to keep the top-r eigenvalues and corresponding eigenvectors\n",
    "    D = D[-r:]  # Top r eigenvalues\n",
    "    V = V[:, -r:]  # Corresponding eigenvectors\n",
    "    \n",
    "    # Update U to be U @ V\n",
    "    U = U @ V\n",
    "    \n",
    "    return U, D\n",
    "\n",
    "# Example usage\n",
    "N, k, l, r = 100, 10, 50, 5  # Example dimensions\n",
    "Y = torch.randn(N, k)  # Random Y matrix\n",
    "W = torch.randn(l, N)  # Random W matrix\n",
    "Psi = torch.randn(l, N)  # Random Psi matrix\n",
    "\n",
    "# Call the function\n",
    "U, D = fixed_rank_eig_approx(Y, W, Psi, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b40e27c-ca24-4155-9f8e-35c5f8ebd3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch(net, batches, k, l):\n",
    "    \"\"\"\n",
    "    Returns a good rank 2k approximation of the FIM using PyTorch.\n",
    "    \"\"\"\n",
    "    M = batches.size(0)\n",
    "    N_params = sum(p.numel() for p in net.parameters())\n",
    "    print(N_params)\n",
    "\n",
    "    om = torch.randn(k, N_params)\n",
    "    psi = torch.randn(l, N_params)\n",
    "\n",
    "    Y = torch.zeros(N_params, k)\n",
    "    W = torch.zeros(l, N_params)\n",
    "\n",
    "    for batch in batches:\n",
    "        JT = jacobian(net, batch).T\n",
    "        Y += (om @ JT @ JT.T).T / M\n",
    "        W += (psi @ JT @ JT.T) / M\n",
    "\n",
    "    # Compute the rank-2k approximation\n",
    "    U, D = fixed_rank_eig_approx(Y, W, psi, 2 * k)\n",
    "\n",
    "    return U, D\n",
    "\n",
    "def jacobian(net, batch):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian for the batch. This needs to be adapted based on the actual function.\n",
    "    \"\"\"\n",
    "    net.zero_grad()\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "    def fnet_single(params, x):\n",
    "        return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, batch)\n",
    "    jac = jac.values()\n",
    "    # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    reshaped_tensors = [\n",
    "        j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "            .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "            .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "        for j in jac\n",
    "    ]\n",
    "    return torch.cat(reshaped_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3298db9f-e3d4-4ed7-b2f9-29884eb56bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def proj_sketch(net, batches, subspace_dimension):\n",
    "    t = time.time_ns()\n",
    "\n",
    "    T = 6 * subspace_dimension + 4 \n",
    "    k = (T - 1) // 3                    # k = 2 * subspace_dimension + 1\n",
    "    l = T - k                           # l = 4 * subspace_dimension + 3\n",
    "\n",
    "    U, D = sketch(net, batches, k, l)\n",
    "    idx = D.argsort(descending=True)\n",
    "    print(\"U shape:\", U.shape)\n",
    "    print(\"Index tensor:\", idx)\n",
    "    print(\"Requested subspace dimension:\", subspace_dimension)\n",
    "    \n",
    "    # Ensure idx is of type long for indexing\n",
    "    # idx = idx.long()\n",
    "\n",
    "    P1 = U[:, idx[:subspace_dimension]].T\n",
    "\n",
    "    print(f\"Done sketching in {(time.time_ns() - t) / 1e9:.4f} s\")\n",
    "\n",
    "    return P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b18d459-8181-432b-8ad4-20eafca9012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding projection matrix\n",
      "1761\n",
      "U shape: torch.Size([1761, 42])\n",
      "Index tensor: tensor([41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24,\n",
      "        23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,\n",
      "         5,  4,  3,  2,  1,  0])\n",
      "Requested subspace dimension: 10\n",
      "Done sketching in 0.6235 s\n",
      "PROJECTION MATRIX : tensor([[-0.0238, -0.0243, -0.0242,  ..., -0.0234, -0.0223, -0.0233],\n",
      "        [-0.0170,  0.0200, -0.0352,  ...,  0.0088, -0.0191,  0.0180],\n",
      "        [-0.0408,  0.0204,  0.0122,  ...,  0.0273,  0.0099,  0.0302],\n",
      "        ...,\n",
      "        [-0.0187, -0.0602,  0.0416,  ...,  0.0261,  0.0084, -0.0162],\n",
      "        [-0.0175,  0.0070,  0.0398,  ..., -0.0175, -0.0384,  0.0173],\n",
      "        [ 0.0095,  0.0220, -0.0336,  ..., -0.0484,  0.0292,  0.0272]])\n",
      "Found projection matrix\n"
     ]
    }
   ],
   "source": [
    "subspace_dimension = 10\n",
    "\n",
    "print(\"Finding projection matrix\")\n",
    "# here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "# P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "\n",
    "\n",
    "# Generate batches in the range [-5, 5]\n",
    "batch_size = 100\n",
    "input_dimensions = sum(p.numel() for p in net.parameters())\n",
    "batches = 10 * torch.rand(batch_size, input_dimensions, 1) - 5  # Scaled from [0, 1] to [-5, 5]\n",
    "\n",
    "# Call the projection sketch function\n",
    "P1 = proj_sketch(net=net, batches=batches, subspace_dimension=subspace_dimension)\n",
    "\n",
    "# Still part of the computation graph ; detach it :\n",
    "P1 = P1.detach()\n",
    "print(f\"PROJECTION MATRIX : {P1}\")\n",
    "print(\"Found projection matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2591ace-5325-4ae3-b1e6-4ab71cfc60c9",
   "metadata": {},
   "source": [
    "## Unlimitd F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "976d9ab7-d85b-4b01-9493-a4c9f7380d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosSimKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CosSimKernel, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # Normalize input vectors\n",
    "        x1_norm = x1 / x1.norm(dim=-1, keepdim=True)\n",
    "        x2_norm = x2 / x2.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cos_sim = x1_norm.matmul(x2_norm.transpose(-2, -1))\n",
    "        \n",
    "        # Ensure the output is between -1 and 1 for numerical stability\n",
    "        cos_sim = cos_sim.clamp(-1, 1)\n",
    "        \n",
    "        if diag:\n",
    "            return cos_sim.diag()\n",
    "        else:\n",
    "            return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8ada96e1-d211-40f1-99dd-0ce5948a9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, net):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = CosSimKernel()\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=40)\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = self.feature_extractor(x)\n",
    "        #z_normalized = z - z.min(0)[0]\n",
    "        #z_normalized = 2 * (z_normalized / z_normalized.max(0)[0]) - 1\n",
    "        #x_normalized = x - x.min(0)[0]\n",
    "        #x_normalized = 2 * (x_normalized / x_normalized.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "47dbab25-9d6b-49f0-a71c-635f317251bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "n_shot_train = 10\n",
    "n_shot_test = 5\n",
    "train_range=(-5.0, 5.0)\n",
    "test_range=(-5.0, 5.0) # This must be (-5, +10) for the out-of-range condition\n",
    "criterion = nn.MSELoss()\n",
    "tasks     = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                  phase_min=0.0, phase_max=np.pi, \n",
    "                                  x_min=train_range[0], x_max=train_range[1], \n",
    "                                  family=\"sine\")\n",
    "net.train()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-4))\n",
    "# likelihood.noise = 1e-4\n",
    "dummy_inputs = torch.zeros([n_shot_train,1])\n",
    "dummy_z = P1@jacobian(net, dummy_inputs)\n",
    "print(dummy_z.shape)\n",
    "dummy_labels = torch.zeros([n_shot_train])\n",
    "gp = ExactGPModel(dummy_z, dummy_labels, likelihood, net)\n",
    "trainable_params = sum(p.numel() for p in gp.parameters() if p.requires_grad)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "optimizer = torch.optim.Adam([{'params': gp.parameters(), 'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4b0461df-9e69-43b0-b16b-0e0940b5bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[0] - Loss: 35.389  MSE: 2.147  lengthscale: 0.000   noise: 0.694\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[200] - Loss: 25.586  MSE: 0.029  lengthscale: 0.000   noise: 0.786\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[400] - Loss: 23.271  MSE: 4.377  lengthscale: 0.000   noise: 0.880\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[600] - Loss: 25.059  MSE: 0.251  lengthscale: 0.000   noise: 0.971\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[800] - Loss: 26.911  MSE: 11.638  lengthscale: 0.000   noise: 1.063\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1000] - Loss: 34.527  MSE: 1.284  lengthscale: 0.000   noise: 1.151\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1200] - Loss: 18.835  MSE: 1.045  lengthscale: 0.000   noise: 1.242\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1400] - Loss: 24.447  MSE: 1.633  lengthscale: 0.000   noise: 1.326\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1600] - Loss: 22.578  MSE: 6.260  lengthscale: 0.000   noise: 1.409\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1800] - Loss: 21.687  MSE: 0.013  lengthscale: 0.000   noise: 1.494\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2000] - Loss: 28.395  MSE: 6.812  lengthscale: 0.000   noise: 1.572\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2200] - Loss: 24.021  MSE: 0.006  lengthscale: 0.000   noise: 1.655\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2400] - Loss: 30.315  MSE: 4.162  lengthscale: 0.000   noise: 1.736\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2600] - Loss: 25.844  MSE: 4.961  lengthscale: 0.000   noise: 1.812\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2800] - Loss: 26.360  MSE: 3.876  lengthscale: 0.000   noise: 1.889\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3000] - Loss: 34.313  MSE: 10.725  lengthscale: 0.000   noise: 1.965\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3200] - Loss: 20.788  MSE: 5.662  lengthscale: 0.000   noise: 2.040\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3400] - Loss: 25.878  MSE: 13.120  lengthscale: 0.000   noise: 2.122\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3600] - Loss: 19.957  MSE: 1.010  lengthscale: 0.000   noise: 2.199\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3800] - Loss: 24.838  MSE: 0.020  lengthscale: 0.000   noise: 2.274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     z \u001b[38;5;241m=\u001b[39m P1\u001b[38;5;129m@jacobian\u001b[39m(net, inputs)\n\u001b[1;32m     14\u001b[0m     gp\u001b[38;5;241m.\u001b[39mset_train_data(inputs\u001b[38;5;241m=\u001b[39mz, targets\u001b[38;5;241m=\u001b[39mlabels)  \n\u001b[0;32m---> 15\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m-\u001b[39mmll(predictions, gp\u001b[38;5;241m.\u001b[39mtrain_targets)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "likelihood.train()\n",
    "gp.train()\n",
    "net.train()\n",
    "\n",
    "tot_iterations=10000 #50000\n",
    "tasks_per_iter=10\n",
    "for epoch in range(tot_iterations):\n",
    "    loss = 0\n",
    "    for _ in range(tasks_per_iter):\n",
    "        # gp.likelihood.noise = 1e-2\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = tasks.sample_task().sample_data(n_shot_train, noise=0.05)\n",
    "        z = P1@jacobian(net, inputs)\n",
    "        gp.set_train_data(inputs=z, targets=labels)  \n",
    "        predictions = gp(z)\n",
    "        loss = loss -mll(predictions, gp.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #---- print some stuff ----\n",
    "    if(epoch%200==0):\n",
    "        mse = criterion(predictions.mean, labels)\n",
    "        print(predictions.mean)\n",
    "        print('[%d] - Loss: %.3f  MSE: %.3f  lengthscale: %.3f   noise: %.3f' % (\n",
    "            epoch, loss.item(), mse.item(),\n",
    "            0.0, #gp.covar_module.base_kernel.lengthscale.item(),\n",
    "            gp.likelihood.noise.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62539e07-f4b8-4323-b6c3-ef11ed3eb8f1",
   "metadata": {},
   "source": [
    "## Second Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863fc82-1755-41d7-869e-11f298f257fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_test = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                phase_min=0.0, phase_max=np.pi, \n",
    "                                x_min=test_range[0], x_max=test_range[1], \n",
    "                                family=\"sine\")\n",
    "\n",
    "sample_task = tasks_test.sample_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5668c88-650b-4c91-8851-5f94bac6644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support, strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(x_all)).mean\n",
    "    lower, upper = likelihood(gp(x_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "\n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c57298-5913-4383-8b4e-585e4c66a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "tot_iterations=500\n",
    "mse_list = list()\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support, strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(x_query)).mean\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06bbe4-d4f0-423f-833c-3d14c2f7384b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea226aa2-7776-48c3-b7b4-0e5543cdccc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f2c06c3f-650f-4e14-a91d-15e975b8432f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNwithGP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[291], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m data_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Covariance noise already leant via gpytorch.likelihoods.GaussianLikelihood()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define model, likelihood, and optimizer\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNNwithGP\u001b[49m(\u001b[38;5;241m40\u001b[39m, K, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mparameters()},\n\u001b[1;32m     13\u001b[0m ], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training_loop\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NNwithGP' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "num_epochs = 50\n",
    "n_tasks_per_epoch = 10\n",
    "input_dim = 1\n",
    "K = 10 # Number of points per task\n",
    "data_noise = 0.05\n",
    "# Covariance noise already leant via gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# Define model, likelihood, and optimizer\n",
    "model = NNwithGP(40, K, 'relu', 10)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# Training_loop\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get a new batch of data\n",
    "    train_x, train_y = get_raw_batch(n_tasks_per_epoch, K, 0, data_noise)\n",
    "    \n",
    "    # Initialize loss accumulation\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over each task\n",
    "    for i in range(train_x.size(0)):  # Assuming train_x is of shape (nb_task, K, 1)\n",
    "        # Process each task individually\n",
    "        nn_output = model.forward_nn(train_x[i])  # Forward pass through the NN for task i\n",
    "        gp_pred = model.forward_gp(nn_output)  # GP forward using NN output for task i\n",
    "        loss = -model.likelihood(gp_pred, train_y[i]).log_prob(train_y[i])  # Compute NLL for task i\n",
    "        total_loss += loss  # Accumulate loss\n",
    "\n",
    "    # Calculate mean loss across all tasks\n",
    "    mean_loss = total_loss / n_tasks_per_epoch\n",
    "    \n",
    "    # Backpropagate the mean loss\n",
    "    mean_loss.backward()\n",
    "    \n",
    "    # Optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch {epoch+1}, Mean Loss: {mean_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec71192-4fe2-4603-b63a-10872abd7933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61a4a26-5e39-4f17-8976-db1b6ce36e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNwithGP(nn.Module):\n",
    "    def __init__(self, n_neurons, K, activation, reg_dim):\n",
    "        super(NNwithGP, self).__init__()\n",
    "        # Neural network layers\n",
    "        self.dense1 = nn.Linear(K, n_neurons)\n",
    "        if activation == \"relu\":\n",
    "            self.act_fn = nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.act_fn = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.dense3 = nn.Linear(n_neurons, reg_dim)\n",
    "\n",
    "        # Additional parameter used in the GP kernel\n",
    "        self.extra_param = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        # GP Model Components\n",
    "        self.gp_mean = gpytorch.means.ConstantMean()\n",
    "        self.gp_covar = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    def forward_nn(self, x):\n",
    "        x = self.act_fn(self.dense1(x))\n",
    "        x = self.act_fn(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "    def forward_gp(self, x):\n",
    "        mean_x = self.gp_mean(x)\n",
    "        # Use extra_param in the kernel\n",
    "        self.gp_covar.base_kernel.lengthscale = torch.abs(self.extra_param)\n",
    "        covar_x = self.gp_covar(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382b2391-26c1-4d7c-a60d-a2fd22453a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1 and 2x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize the Jacobian matrix\u001b[39;00m\n\u001b[1;32m     28\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mflatten()), \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()))\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out))\n\u001b[1;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x40)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 40)\n",
    "        self.layer2 = nn.Linear(40,40)\n",
    "        self.layer3 = nn.Linear(40,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define an input tensor\n",
    "input_tensor = torch.randn(2, 1, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_tensor)\n",
    "\n",
    "# Initialize the Jacobian matrix\n",
    "jacobian = torch.zeros(len(outputs.flatten()), sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# Compute Jacobian\n",
    "for i, output in enumerate(outputs.flatten()):\n",
    "    # Zero all parameter gradients\n",
    "    model.zero_grad()\n",
    "    # Compute gradient of output[i] with respect to all parameters\n",
    "    output.backward(retain_graph=True)\n",
    "    # Store gradients in the Jacobian row\n",
    "    j = 0\n",
    "    for param in model.parameters():\n",
    "        jacobian[i, j:j+param.numel()] = param.grad.flatten()\n",
    "        j += param.numel()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "print(\"Non-trainable parameters:\", non_trainable_params)\n",
    "print(\"Jacobian matrix:\")\n",
    "print(jacobian.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd22d0d-32ea-4311-bf26-2b6c320fd121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1801\n",
      "Non-trainable parameters: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The inputs given to jacobian must be either a Tensor or a tuple of Tensors but the given inputs has type <class 'builtin_function_or_method'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainable parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable_params)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-trainable parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, non_trainable_params)\n\u001b[0;32m---> 37\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mempirical_ntk_jacobian_contraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfnet_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mempirical_ntk_jacobian_contraction\u001b[0;34m(fnet_single, params, x1, x2)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempirical_ntk_jacobian_contraction\u001b[39m(fnet_single, params, x1, x2):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Compute J(x1)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     jac1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(jac1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Compute J(x2)\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/autograd/functional.py:588\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _jacfwd(func, inputs, strict, vectorize)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 588\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_as_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjacobian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/autograd/functional.py:37\u001b[0m, in \u001b[0;36m_as_tuple\u001b[0;34m(inp, arg_name, fn_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m given to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must be either a Tensor or a tuple of Tensors but the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m value at index \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg_name, fn_name, i, \u001b[38;5;28mtype\u001b[39m(el)))\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m given to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must be either a Tensor or a tuple of Tensors but the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg_name, fn_name, arg_name, \u001b[38;5;28mtype\u001b[39m(el)))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_inp_tuple, inp\n",
      "\u001b[0;31mTypeError\u001b[0m: The inputs given to jacobian must be either a Tensor or a tuple of Tensors but the given inputs has type <class 'builtin_function_or_method'>."
     ]
    }
   ],
   "source": [
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "x_train = torch.randn(20, 5, device=device)\n",
    "x_test = torch.randn(20, 5, device=device)\n",
    "\n",
    "\n",
    "net = SimpleNN().to(device)\n",
    "\n",
    "# Detaching the parameters because we won't be calling Tensor.backward().\n",
    "params = {k: v.detach() for k, v in net.named_parameters()}\n",
    "\n",
    "def apply_net(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "def empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2):\n",
    "    # Compute J(x1)\n",
    "    jac1 = torch.autograd.functional.jacobian(lambda params: apply_net(params, x1))\n",
    "    print(jac1.shape)\n",
    "\n",
    "    # Compute J(x2)\n",
    "    jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)\n",
    "    jac2 = jac2.values()\n",
    "    jac2 = [j.flatten(2) for j in jac2]\n",
    "    print(jac1[1].shape)\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    result = torch.stack([torch.einsum('Nf,Mf->NM', j1, j2) for j1, j2 in zip(jac1, jac2)])\n",
    "    result = result.sum(0)\n",
    "    return result\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "print(\"Non-trainable parameters:\", non_trainable_params)\n",
    "result = empirical_ntk_jacobian_contraction(fnet_single, params, x_train, x_train)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b635a-bbe6-4a86-9119-974874ed3cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9d3c5-6bd2-438c-8e69-47dcafde8b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4503570-ebd8-4dd0-9680-fdd244ddec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67a0ce8f-88c3-434e-89ba-34bd0b7e4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "\n",
    "    def compute_gradients(self, inputs):\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        grads_x1 = self.compute_gradients(x1)\n",
    "        grads_x2 = self.compute_gradients(x2) if x1 is not x2 else grads_x1\n",
    "        result = torch.matmul(grads_x1, grads_x2.transpose(-1, -2))\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "\n",
    "# Make sure the network and the kernel are correctly initialized and used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6017998-6777-4694-b8d7-fa4acb4c5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPWithNTK(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, neural_net):\n",
    "        super(GPWithNTK, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = NTKernel(neural_net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ab25391-58c4-4024-8557-8ad7a64fc434",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mset_train_data(inputs\u001b[38;5;241m=\u001b[39mtrain_x, targets\u001b[38;5;241m=\u001b[39mtrain_y)  \n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py:192\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    185\u001b[0m         covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;241m*\u001b[39m(diff_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m covar_size \u001b[38;5;28;01mfor\u001b[39;00m diff_size, covar_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(diff\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], padded_batch_shape)),\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    189\u001b[0m         )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m covar \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39minv_quad_logdet(inv_quad_rhs\u001b[38;5;241m=\u001b[39mdiff\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), logdet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/linear_operator/operators/added_diag_linear_operator.py:209\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     added_diag_linear_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_linear_op \u001b[38;5;241m+\u001b[39m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_diag_tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2064\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentation_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m    Returns a\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;124;03m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLinearOperatorRepresentationTree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/linear_operator/operators/linear_operator_representation_tree.py:15\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[0;34m(self, linear_op)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(linear_op\u001b[38;5;241m.\u001b[39m_args, linear_op\u001b[38;5;241m.\u001b[39m_differentiable_kwargs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(arg\u001b[38;5;241m.\u001b[39mrepresentation):  \u001b[38;5;66;03m# Is it a lazy tensor?\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         representation_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mslice\u001b[39m(counter, counter \u001b[38;5;241m+\u001b[39m representation_size, \u001b[38;5;28;01mNone\u001b[39;00m), arg\u001b[38;5;241m.\u001b[39mrepresentation_tree()))\n\u001b[1;32m     17\u001b[0m         counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m representation_size\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:397\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrepresentation()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# representation\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepresentation()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[0;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mNTKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 23\u001b[0m     grads_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     grads_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(x2) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x2 \u001b[38;5;28;01melse\u001b[39;00m grads_x1\n\u001b[1;32m     25\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(grads_x1, grads_x2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mNTKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 23\u001b[0m     grads_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     grads_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(x2) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x2 \u001b[38;5;28;01melse\u001b[39;00m grads_x1\n\u001b[1;32m     25\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(grads_x1, grads_x2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "    \u001b[0;31m[... skipping similar frames: Kernel.__call__ at line 530 (588 times), Module.__call__ at line 31 (588 times), NTKernel.forward at line 23 (588 times), NTKernel.compute_gradients at line 8 (587 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mNTKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 23\u001b[0m     grads_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     grads_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(x2) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x2 \u001b[38;5;28;01melse\u001b[39;00m grads_x1\n\u001b[1;32m     25\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(grads_x1, grads_x2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[77], line 7\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(inputs)\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2348\u001b[0m, in \u001b[0;36mModule.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_replica\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2342\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling .zero_grad() from a module created with nn.DataParallel() has no effect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe parameters are copied (in a differentiable manner) from the original module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means they are not leaf nodes in autograd and so don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt accumulate gradients. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you need gradients in your forward method, consider using autograd.grad instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m   2349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2081\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \n\u001b[1;32m   2062\u001b[0m \u001b[38;5;124;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2079\u001b[0m \n\u001b[1;32m   2080\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2081\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters(recurse\u001b[38;5;241m=\u001b[39mrecurse):\n\u001b[1;32m   2082\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m param\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2115\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;124;03mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \n\u001b[1;32m   2111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   2114\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix, recurse\u001b[38;5;241m=\u001b[39mrecurse, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate)\n\u001b[0;32m-> 2115\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2049\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2047\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   2048\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules(prefix\u001b[38;5;241m=\u001b[39mprefix, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate) \u001b[38;5;28;01mif\u001b[39;00m recurse \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[0;32m-> 2049\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_prefix, module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   2050\u001b[0m     members \u001b[38;5;241m=\u001b[39m get_members_fn(module)\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m members:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2260\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n\u001b[0;32m-> 2260\u001b[0m         \u001b[43mmemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "train_x = torch.randn(50, 10)  # Example data\n",
    "train_y = torch.randn(50)  # Example targets\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "net = SimpleNN()\n",
    "model = GPWithNTK(train_x, train_y, likelihood, net)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 0.01}\n",
    "], lr=0.1)\n",
    "\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    model.set_train_data(inputs=train_x, targets=train_y)  \n",
    "    predictions = model(train_x)\n",
    "    loss = -mll(predictions, model.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, 50, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f1d3c-23b0-4af3-a23b-1bdfa5f7d491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d7d60-2445-4e49-8dc1-cfff8eeca5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517d9a4-dd15-40dd-b72c-a6638c4d1898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "253c2c25-a592-4038-8302-b22d7bce1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(2, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "52313dfb-3d0f-4f9f-a686-db5aa14d8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.randn(20, 1, device=device)\n",
    "x_test = torch.randn(5, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "2269b86a-ef81-49d3-8d41-a88356eaebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 31\n",
      "Non-trainable parameters: 0\n",
      "['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNN().to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in net.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "print(\"Non-trainable parameters:\", non_trainable_params)\n",
    "\n",
    "params = {k: v for k, v in net.named_parameters()}\n",
    "print(list(params.keys()))\n",
    "print(params['fc1.weight'].shape)\n",
    "print(params['fc1.bias'].shape)\n",
    "print(params['fc2.weight'].shape)\n",
    "print(params['fc2.bias'].shape)\n",
    "print(params['fc3.weight'].shape)\n",
    "print(params['fc3.bias'].shape)\n",
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "bc50a023-569c-4989-8bf6-0493d37877b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "torch.Size([31, 60])\n",
      "torch.Size([31, 15])\n",
      "torch.Size([60, 15])\n"
     ]
    }
   ],
   "source": [
    "def empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2):\n",
    "    # Compute J(x1)\n",
    "    jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x1)\n",
    "    print(jac1.keys())\n",
    "    jac1 = jac1.values()\n",
    "    # jac1 = [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    def reshape_jac(jacobians):\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "              .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "              .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jacobians\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0)\n",
    "    \n",
    "    jac1 = reshape_jac(jac1)\n",
    "    print(jac1.shape)\n",
    "\n",
    "    # Compute J(x2)\n",
    "    jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)\n",
    "    jac2 = jac2.values()\n",
    "    jac2 = reshape_jac(jac2)\n",
    "    print(jac2.shape)\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    result = torch.einsum('fN,fM->NM', jac1, jac2)\n",
    "    return result\n",
    "\n",
    "result = empirical_ntk_jacobian_contraction(fnet_single, params, x_train, x_test)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd48c6-c475-4f8c-9746-6fbe7bf5dc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
