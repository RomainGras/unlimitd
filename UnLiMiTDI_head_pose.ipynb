{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f72d7b7-6c46-4bff-bf69-0ab5a0784ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original packages\n",
    "import backbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "## Our packages\n",
    "import gpytorch\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "from statistics import mean\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "\n",
    "\n",
    "class UnLiMiTDI(nn.Module):\n",
    "    def __init__(self, conv_net, diff_net):\n",
    "        super(UnLiMiTDI, self).__init__()\n",
    "        ## GP parameters\n",
    "        self.feature_extractor = conv_net\n",
    "        self.diff_net = diff_net  #Differentiable network\n",
    "        self.get_model_likelihood_mll() #Init model, likelihood, and mll\n",
    "\n",
    "    def get_model_likelihood_mll(self, train_x=None, train_y=None):\n",
    "        if(train_x is None): train_x=torch.ones(19, 2916).cuda()\n",
    "        if(train_y is None): train_y=torch.ones(19).cuda()\n",
    "\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = ExactGPLayer(train_x=train_x, train_y=train_y, likelihood=likelihood, diff_net = self.diff_net, kernel='NTKcossim')\n",
    "\n",
    "        self.model      = model.cuda()\n",
    "        self.likelihood = likelihood.cuda()\n",
    "        self.mll        = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model).cuda()\n",
    "        self.mse        = nn.MSELoss()\n",
    "\n",
    "        return self.model, self.likelihood, self.mll\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def train_loop(self, epoch, optimizer):\n",
    "        batch, batch_labels = get_batch(train_people)\n",
    "        batch, batch_labels = batch.cuda(), batch_labels.cuda()\n",
    "        for inputs, labels in zip(batch, batch_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_conv = self.feature_extractor(inputs)\n",
    "            self.model.set_train_data(inputs=inputs_conv, targets=labels - self.diff_net(inputs_conv).reshape(-1))  \n",
    "            predictions = self.model(inputs_conv)\n",
    "            loss = -self.mll(predictions, self.model.train_targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = self.mse(predictions.mean, labels)\n",
    "\n",
    "            if (epoch%10==0):\n",
    "                print('[%d] - Loss: %.3f  MSE: %.3f noise: %.3f' % (\n",
    "                    epoch, loss.item(), mse.item(),\n",
    "                    self.model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "    def test_loop(self, n_support, optimizer=None): # no optimizer needed for GP\n",
    "        inputs, targets = get_batch(test_people)\n",
    "\n",
    "        support_ind = list(np.random.choice(list(range(19)), replace=False, size=n_support))\n",
    "        query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "        x_all = inputs.cuda()\n",
    "        y_all = targets.cuda()\n",
    "\n",
    "        x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "        y_support = targets[:,support_ind].cuda()\n",
    "\n",
    "        # choose a random test person\n",
    "        n = np.random.randint(0, len(test_people)-1)\n",
    "    \n",
    "        x_conv_support = self.feature_extractor(x_support[n]).detach()\n",
    "        self.model.set_train_data(inputs=x_conv_support, targets=y_support[n] - self.diff_net(x_conv_support).reshape(-1), strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_conv_query = self.feature_extractor(x_all[n]).detach()\n",
    "            pred    = self.likelihood(self.model(x_conv_query))\n",
    "            lower, upper = pred.confidence_region() #2 standard deviations above and below the mean\n",
    "            lower += self.diff_net(x_conv_query).reshape(-1)\n",
    "            upper += self.diff_net(x_conv_query).reshape(-1)\n",
    "        mse = self.mse(pred.mean + self.diff_net(self.feature_extractor(x_all[n])).reshape(-1), y_all[n])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def save_checkpoint(self, checkpoint):\n",
    "        # save state\n",
    "        gp_state_dict         = self.model.state_dict()\n",
    "        likelihood_state_dict = self.likelihood.state_dict()\n",
    "        conv_net_state_dict   = self.feature_extractor.state_dict()\n",
    "        diff_net_state_dict   = self.diff_net.state_dict()\n",
    "        torch.save({'gp': gp_state_dict, 'likelihood': likelihood_state_dict, 'conv_net':conv_net_state_dict, 'diff_net':diff_net_state_dict}, checkpoint)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        self.model.load_state_dict(ckpt['gp'])\n",
    "        self.likelihood.load_state_dict(ckpt['likelihood'])\n",
    "        self.feature_extractor.load_state_dict(ckpt['conv_net'])\n",
    "        self.diff_net.load_state_dict(ckpt['diff_net'])\n",
    "\n",
    "        \n",
    "###################\n",
    "#NTKernel\n",
    "###################\n",
    "        \n",
    "class NTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        result = jac1@jac2.T\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "    \n",
    "    \n",
    "    \n",
    "###################\n",
    "#NTKernel CosSim\n",
    "###################\n",
    "class CosSimNTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(CosSimNTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1T = self.compute_jacobian(x1).T\n",
    "        jac1T_norm = jac1T.norm(dim=0, keepdim=True)\n",
    "        jac1T_normalized = jac1T/jac1T_norm\n",
    "        #print(jac1.shape)\n",
    "        #print(jac1.norm(dim=0, keepdim=True).shape)\n",
    "        jac2T = self.compute_jacobian(x2).T if x1 is not x2 else jac1T\n",
    "        jac2T_norm = jac2T.norm(dim=0, keepdim=True)\n",
    "        jac2T_normalized = jac2T/jac2T_norm\n",
    "        \n",
    "        result = self.alpha * jac1T_normalized.T@jac2T_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "    \n",
    "###################\n",
    "#GP\n",
    "###################    \n",
    "class ExactGPLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, diff_net, kernel='NTK'):\n",
    "        super(ExactGPLayer, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gpytorch.means.ConstantMean()\n",
    "\n",
    "        ## NTKernel\n",
    "        if(kernel=='NTK'):\n",
    "            self.covar_module = NTKernel(diff_net)\n",
    "        elif(kernel=='NTKcossim'):\n",
    "            self.covar_module = CosSimNTKernel(diff_net)        \n",
    "        else:\n",
    "            raise ValueError(\"[ERROR] the kernel '\" + str(kernel) + \"' is not supported for regression, use 'rbf' or 'spectral'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e20314de-5757-4036-9133-b9dd4ccc9947",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - Loss: 0.828  MSE: 0.000 noise: 0.693\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.815  MSE: 0.000 noise: 0.681\n",
      "[10] - Loss: 0.772  MSE: 0.219 noise: 0.580\n",
      "[10] - Loss: 0.799  MSE: 0.219 noise: 0.579\n",
      "[10] - Loss: 0.799  MSE: 0.219 noise: 0.579\n",
      "[10] - Loss: 0.799  MSE: 0.220 noise: 0.578\n",
      "[10] - Loss: 0.831  MSE: 0.220 noise: 0.578\n",
      "[10] - Loss: 0.819  MSE: 0.220 noise: 0.577\n",
      "[10] - Loss: 0.790  MSE: 0.220 noise: 0.577\n",
      "[10] - Loss: 0.785  MSE: 0.220 noise: 0.576\n",
      "[10] - Loss: 0.803  MSE: 0.221 noise: 0.576\n",
      "[10] - Loss: 0.795  MSE: 0.221 noise: 0.575\n",
      "[10] - Loss: 0.803  MSE: 0.221 noise: 0.575\n",
      "[10] - Loss: 0.804  MSE: 0.221 noise: 0.575\n",
      "[10] - Loss: 0.778  MSE: 0.221 noise: 0.574\n",
      "[10] - Loss: 0.798  MSE: 0.221 noise: 0.574\n",
      "[10] - Loss: 0.769  MSE: 0.222 noise: 0.573\n",
      "[10] - Loss: 0.765  MSE: 0.222 noise: 0.573\n",
      "[10] - Loss: 0.804  MSE: 0.222 noise: 0.572\n",
      "[10] - Loss: 0.804  MSE: 0.222 noise: 0.572\n",
      "[10] - Loss: 0.788  MSE: 0.222 noise: 0.571\n",
      "[10] - Loss: 0.845  MSE: 0.223 noise: 0.571\n",
      "[10] - Loss: 0.851  MSE: 0.223 noise: 0.570\n",
      "[10] - Loss: 0.782  MSE: 0.223 noise: 0.570\n",
      "[10] - Loss: 0.765  MSE: 0.223 noise: 0.570\n",
      "[10] - Loss: 0.767  MSE: 0.223 noise: 0.569\n",
      "[20] - Loss: 0.696  MSE: 0.575 noise: 0.476\n",
      "[20] - Loss: 0.708  MSE: 0.575 noise: 0.476\n",
      "[20] - Loss: 0.682  MSE: 0.575 noise: 0.476\n",
      "[20] - Loss: 0.693  MSE: 0.575 noise: 0.475\n",
      "[20] - Loss: 0.694  MSE: 0.575 noise: 0.475\n",
      "[20] - Loss: 0.781  MSE: 0.575 noise: 0.475\n",
      "[20] - Loss: 0.716  MSE: 0.575 noise: 0.474\n",
      "[20] - Loss: 0.708  MSE: 0.575 noise: 0.474\n",
      "[20] - Loss: 0.719  MSE: 0.575 noise: 0.473\n",
      "[20] - Loss: 0.698  MSE: 0.575 noise: 0.473\n",
      "[20] - Loss: 0.691  MSE: 0.575 noise: 0.473\n",
      "[20] - Loss: 0.692  MSE: 0.576 noise: 0.472\n",
      "[20] - Loss: 0.716  MSE: 0.576 noise: 0.472\n",
      "[20] - Loss: 0.760  MSE: 0.576 noise: 0.472\n",
      "[20] - Loss: 0.717  MSE: 0.576 noise: 0.471\n",
      "[20] - Loss: 0.670  MSE: 0.576 noise: 0.471\n",
      "[20] - Loss: 0.754  MSE: 0.577 noise: 0.470\n",
      "[20] - Loss: 0.822  MSE: 0.577 noise: 0.470\n",
      "[20] - Loss: 0.739  MSE: 0.577 noise: 0.470\n",
      "[20] - Loss: 0.842  MSE: 0.577 noise: 0.469\n",
      "[20] - Loss: 0.786  MSE: 0.577 noise: 0.469\n",
      "[20] - Loss: 0.685  MSE: 0.577 noise: 0.469\n",
      "[20] - Loss: 0.674  MSE: 0.577 noise: 0.468\n",
      "[20] - Loss: 0.674  MSE: 0.577 noise: 0.468\n",
      "[30] - Loss: 0.703  MSE: 0.330 noise: 0.387\n",
      "[30] - Loss: 0.670  MSE: 0.330 noise: 0.386\n",
      "[30] - Loss: 0.665  MSE: 0.330 noise: 0.386\n",
      "[30] - Loss: 0.659  MSE: 0.330 noise: 0.386\n",
      "[30] - Loss: 0.658  MSE: 0.330 noise: 0.385\n",
      "[30] - Loss: 0.709  MSE: 0.330 noise: 0.385\n",
      "[30] - Loss: 0.627  MSE: 0.330 noise: 0.385\n",
      "[30] - Loss: 0.703  MSE: 0.331 noise: 0.384\n",
      "[30] - Loss: 0.620  MSE: 0.331 noise: 0.384\n",
      "[30] - Loss: 0.587  MSE: 0.332 noise: 0.384\n",
      "[30] - Loss: 0.622  MSE: 0.332 noise: 0.383\n",
      "[30] - Loss: 0.638  MSE: 0.333 noise: 0.383\n",
      "[30] - Loss: 0.586  MSE: 0.333 noise: 0.383\n",
      "[30] - Loss: 0.615  MSE: 0.334 noise: 0.382\n",
      "[30] - Loss: 0.562  MSE: 0.334 noise: 0.382\n",
      "[30] - Loss: 0.557  MSE: 0.335 noise: 0.382\n",
      "[30] - Loss: 0.650  MSE: 0.335 noise: 0.381\n",
      "[30] - Loss: 0.620  MSE: 0.336 noise: 0.381\n",
      "[30] - Loss: 0.591  MSE: 0.336 noise: 0.381\n",
      "[30] - Loss: 0.707  MSE: 0.336 noise: 0.381\n",
      "[30] - Loss: 0.617  MSE: 0.337 noise: 0.380\n",
      "[30] - Loss: 0.546  MSE: 0.337 noise: 0.380\n",
      "[30] - Loss: 0.550  MSE: 0.337 noise: 0.380\n",
      "[30] - Loss: 0.565  MSE: 0.338 noise: 0.379\n",
      "[40] - Loss: 0.477  MSE: 0.335 noise: 0.313\n",
      "[40] - Loss: 0.464  MSE: 0.335 noise: 0.313\n",
      "[40] - Loss: 0.465  MSE: 0.335 noise: 0.313\n",
      "[40] - Loss: 0.480  MSE: 0.335 noise: 0.312\n",
      "[40] - Loss: 0.495  MSE: 0.335 noise: 0.312\n",
      "[40] - Loss: 0.549  MSE: 0.335 noise: 0.312\n",
      "[40] - Loss: 0.488  MSE: 0.335 noise: 0.311\n",
      "[40] - Loss: 0.530  MSE: 0.336 noise: 0.311\n",
      "[40] - Loss: 0.466  MSE: 0.336 noise: 0.311\n",
      "[40] - Loss: 0.474  MSE: 0.336 noise: 0.311\n",
      "[40] - Loss: 0.493  MSE: 0.337 noise: 0.310\n",
      "[40] - Loss: 0.499  MSE: 0.337 noise: 0.310\n",
      "[40] - Loss: 0.466  MSE: 0.337 noise: 0.310\n",
      "[40] - Loss: 0.481  MSE: 0.337 noise: 0.309\n",
      "[40] - Loss: 0.442  MSE: 0.337 noise: 0.309\n",
      "[40] - Loss: 0.449  MSE: 0.338 noise: 0.309\n",
      "[40] - Loss: 0.486  MSE: 0.338 noise: 0.309\n",
      "[40] - Loss: 0.478  MSE: 0.338 noise: 0.308\n",
      "[40] - Loss: 0.492  MSE: 0.338 noise: 0.308\n",
      "[40] - Loss: 0.557  MSE: 0.338 noise: 0.308\n",
      "[40] - Loss: 0.505  MSE: 0.338 noise: 0.307\n",
      "[40] - Loss: 0.437  MSE: 0.338 noise: 0.307\n",
      "[40] - Loss: 0.436  MSE: 0.337 noise: 0.307\n",
      "[40] - Loss: 0.442  MSE: 0.337 noise: 0.307\n",
      "[50] - Loss: 0.318  MSE: 0.131 noise: 0.251\n",
      "[50] - Loss: 0.361  MSE: 0.131 noise: 0.251\n",
      "[50] - Loss: 0.361  MSE: 0.132 noise: 0.250\n",
      "[50] - Loss: 0.336  MSE: 0.132 noise: 0.250\n",
      "[50] - Loss: 0.380  MSE: 0.131 noise: 0.250\n",
      "[50] - Loss: 0.371  MSE: 0.131 noise: 0.250\n",
      "[50] - Loss: 0.358  MSE: 0.131 noise: 0.250\n",
      "[50] - Loss: 0.355  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.383  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.392  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.376  MSE: 0.131 noise: 0.249\n",
      "[50] - Loss: 0.387  MSE: 0.131 noise: 0.248\n",
      "[50] - Loss: 0.353  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.350  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.321  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.329  MSE: 0.130 noise: 0.248\n",
      "[50] - Loss: 0.357  MSE: 0.130 noise: 0.247\n",
      "[50] - Loss: 0.342  MSE: 0.130 noise: 0.247\n",
      "[50] - Loss: 0.367  MSE: 0.130 noise: 0.247\n",
      "[50] - Loss: 0.408  MSE: 0.129 noise: 0.247\n",
      "[50] - Loss: 0.368  MSE: 0.129 noise: 0.246\n",
      "[50] - Loss: 0.337  MSE: 0.129 noise: 0.246\n",
      "[50] - Loss: 0.318  MSE: 0.129 noise: 0.246\n",
      "[50] - Loss: 0.324  MSE: 0.129 noise: 0.246\n",
      "[60] - Loss: 0.464  MSE: 0.569 noise: 0.201\n",
      "[60] - Loss: 0.312  MSE: 0.569 noise: 0.201\n",
      "[60] - Loss: 0.298  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.297  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.319  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.506  MSE: 0.569 noise: 0.200\n",
      "[60] - Loss: 0.317  MSE: 0.570 noise: 0.200\n",
      "[60] - Loss: 0.364  MSE: 0.570 noise: 0.200\n",
      "[60] - Loss: 0.285  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.445  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.349  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.517  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.256  MSE: 0.571 noise: 0.199\n",
      "[60] - Loss: 0.303  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.245  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.232  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.324  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.367  MSE: 0.571 noise: 0.198\n",
      "[60] - Loss: 0.335  MSE: 0.570 noise: 0.198\n",
      "[60] - Loss: 0.484  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.362  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.268  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.250  MSE: 0.570 noise: 0.197\n",
      "[60] - Loss: 0.270  MSE: 0.570 noise: 0.197\n",
      "[70] - Loss: 0.147  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.125  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.168  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.103  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.113  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.124  MSE: 0.060 noise: 0.160\n",
      "[70] - Loss: 0.095  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.129  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.098  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.151  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.114  MSE: 0.060 noise: 0.159\n",
      "[70] - Loss: 0.116  MSE: 0.059 noise: 0.159\n",
      "[70] - Loss: 0.092  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.098  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.122  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.090  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.098  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.133  MSE: 0.059 noise: 0.158\n",
      "[70] - Loss: 0.167  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.090  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.109  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.080  MSE: 0.058 noise: 0.157\n",
      "[70] - Loss: 0.084  MSE: 0.057 noise: 0.157\n",
      "[70] - Loss: 0.077  MSE: 0.057 noise: 0.157\n",
      "[80] - Loss: 0.051  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.054  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.061  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.031  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.119  MSE: 0.224 noise: 0.128\n",
      "[80] - Loss: 0.079  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.038  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.077  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.008  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.097  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.096  MSE: 0.224 noise: 0.127\n",
      "[80] - Loss: 0.087  MSE: 0.223 noise: 0.127\n",
      "[80] - Loss: 0.055  MSE: 0.223 noise: 0.127\n",
      "[80] - Loss: 0.067  MSE: 0.223 noise: 0.127\n",
      "[80] - Loss: 0.025  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.031  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.054  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.030  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.047  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.168  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.069  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: 0.046  MSE: 0.223 noise: 0.126\n",
      "[80] - Loss: -0.004  MSE: 0.223 noise: 0.125\n",
      "[80] - Loss: 0.023  MSE: 0.223 noise: 0.125\n",
      "[90] - Loss: -0.061  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.084  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.027  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.104  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.029  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.069  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.058  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: 0.007  MSE: 0.114 noise: 0.101\n",
      "[90] - Loss: -0.055  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.043  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.057  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.044  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.059  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.031  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.078  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.094  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.101  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.085  MSE: 0.114 noise: 0.100\n",
      "[90] - Loss: -0.061  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.018  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.065  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.108  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.120  MSE: 0.114 noise: 0.099\n",
      "[90] - Loss: -0.084  MSE: 0.114 noise: 0.099\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import configs\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "from io_utils import parse_args_regression, get_resume_file\n",
    "from methods.DKT_regression import DKT\n",
    "from methods.feature_transfer_regression import FeatureTransfer\n",
    "import backbone\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "bb           = backbone.Conv3().cuda()\n",
    "simple_net   = backbone.simple_net().cuda()\n",
    "\n",
    "model = UnLiMiTDI(bb, simple_net).cuda()\n",
    "optimizer = torch.optim.Adam([{'params': model.model.parameters(), 'lr': 0.001},\n",
    "                                {'params': model.feature_extractor.parameters(), 'lr': 0.001}])\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train_loop(epoch, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "841204c4-4d98-4421-ac24-4541d5728551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Average MSE: 0.05402122889645398 +- 0.03329951535135017\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "mse_list = []\n",
    "for epoch in range(10):\n",
    "    mse = float(model.test_loop(5, optimizer).cpu().detach().numpy())\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8f55a8f-3de1-454a-b802-8ccfd2c03dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original packages\n",
    "import backbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "## Our packages\n",
    "import gpytorch\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "from statistics import mean\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "\n",
    "class UnLiMiTDproj(nn.Module):\n",
    "    def __init__(self, conv_net, diff_net, P):\n",
    "        super(UnLiMiTDproj, self).__init__()\n",
    "        ## GP parameters\n",
    "        self.feature_extractor = conv_net\n",
    "        self.diff_net = diff_net  #Differentiable network\n",
    "        \n",
    "        input_dimension = sum(p.numel() for p in diff_net.parameters())\n",
    "        self.P = P\n",
    "        self.get_model_likelihood_mll() #Init model, likelihood, and mll\n",
    "\n",
    "    def get_model_likelihood_mll(self, train_x=None, train_y=None):\n",
    "        if(train_x is None): train_x=torch.ones(19, 2916).cuda()\n",
    "        if(train_y is None): train_y=torch.ones(19).cuda()\n",
    "\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = ExactGPLayer(train_x=train_x, train_y=train_y, likelihood=likelihood, diff_net = self.diff_net, P = self.P, kernel='NTK')\n",
    "\n",
    "        self.model      = model.cuda()\n",
    "        self.likelihood = likelihood.cuda()\n",
    "        self.mll        = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model).cuda()\n",
    "        self.mse        = nn.MSELoss()\n",
    "\n",
    "        return self.model, self.likelihood, self.mll\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def train_loop(self, epoch, optimizer):\n",
    "        batch, batch_labels = get_batch(train_people)\n",
    "        batch, batch_labels = batch.cuda(), batch_labels.cuda()\n",
    "        for inputs, labels in zip(batch, batch_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_conv = self.feature_extractor(inputs)\n",
    "            self.model.set_train_data(inputs=inputs_conv, targets=labels - self.diff_net(inputs_conv).reshape(-1))  \n",
    "            predictions = self.model(inputs_conv)\n",
    "            loss = -self.mll(predictions, self.model.train_targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = self.mse(predictions.mean, labels)\n",
    "\n",
    "            if (epoch%10==0):\n",
    "                print('[%d] - Loss: %.3f  MSE: %.3f noise: %.3f' % (\n",
    "                    epoch, loss.item(), mse.item(),\n",
    "                    self.model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "    def test_loop(self, n_support, optimizer=None): # no optimizer needed for GP\n",
    "        inputs, targets = get_batch(test_people)\n",
    "\n",
    "        support_ind = list(np.random.choice(list(range(19)), replace=False, size=n_support))\n",
    "        query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "        x_all = inputs.cuda()\n",
    "        y_all = targets.cuda()\n",
    "\n",
    "        x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "        y_support = targets[:,support_ind].cuda()\n",
    "\n",
    "        # choose a random test person\n",
    "        n = np.random.randint(0, len(test_people)-1)\n",
    "    \n",
    "        x_conv_support = self.feature_extractor(x_support[n]).detach()\n",
    "        self.model.set_train_data(inputs=x_conv_support, targets=y_support[n] - self.diff_net(x_conv_support).reshape(-1), strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_conv_query = self.feature_extractor(x_all[n]).detach()\n",
    "            pred    = self.likelihood(self.model(x_conv_query))\n",
    "            lower, upper = pred.confidence_region() #2 standard deviations above and below the mean\n",
    "            lower += self.diff_net(x_conv_query).reshape(-1)\n",
    "            upper += self.diff_net(x_conv_query).reshape(-1)\n",
    "        mse = self.mse(pred.mean + self.diff_net(self.feature_extractor(x_all[n])).reshape(-1), y_all[n])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def save_checkpoint(self, checkpoint):\n",
    "        # save state\n",
    "        gp_state_dict         = self.model.state_dict()\n",
    "        likelihood_state_dict = self.likelihood.state_dict()\n",
    "        conv_net_state_dict   = self.feature_extractor.state_dict()\n",
    "        diff_net_state_dict   = self.diff_net.state_dict()\n",
    "        torch.save({\n",
    "            'gp': gp_state_dict,\n",
    "            'likelihood': likelihood_state_dict,\n",
    "            'conv_net': conv_net_state_dict,\n",
    "            'diff_net': diff_net_state_dict,\n",
    "            'proj_matrix': self.P  # Save the tensor directly\n",
    "        }, checkpoint)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        if 'covar_module.scaling_param' not in ckpt['gp'].keys():\n",
    "            ckpt['gp']['covar_module.scaling_param'] = torch.ones(self.P.shape[0]).cuda()\n",
    "        self.model.load_state_dict(ckpt['gp'])\n",
    "        self.likelihood.load_state_dict(ckpt['likelihood'])\n",
    "        self.feature_extractor.load_state_dict(ckpt['conv_net'])\n",
    "        self.diff_net.load_state_dict(ckpt['diff_net'])\n",
    "        if 'proj_matrix' in ckpt.keys():\n",
    "            self.P = ckpt['proj_matrix']\n",
    "        \n",
    "        print(f\"Total number of param that requires grad : {sum(p.numel() for p in self.model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "\n",
    "# ##################\n",
    "# NTKernel\n",
    "# ##################\n",
    "\n",
    "class NTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, P, **kwargs):\n",
    "        super(NTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        self.P = P # Projection matrix\n",
    "        \n",
    "        # Add subspace_dimension scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(P.shape[0]))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        D = torch.diag(torch.pow(self.scaling_param, 2))\n",
    "        \n",
    "        result = torch.chain_matmul(jac1, self.P.T, D, self.P, jac2.T)\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "\n",
    "    \n",
    "# ##################\n",
    "# NTKernel CosSim\n",
    "# ##################\n",
    "\n",
    "class CosSimNTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, P, **kwargs):\n",
    "        super(CosSimNTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        self.P = P # Projection matrix\n",
    "        \n",
    "        # Add subspace_dimension scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(P.shape[0]))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        D = torch.diag(self.scaling_param)\n",
    "        \n",
    "        result_1 = torch.chain_matmul(D, self.P, jac1.T)\n",
    "        result_2 = torch.chain_matmul(D, self.P, jac2.T)\n",
    "        \n",
    "        result_1_norm = result_1.norm(dim=0, keepdim=True)\n",
    "        result_1_normalized = result_1/result_1_norm\n",
    "        #print(result_1.shape)\n",
    "        #print(result_1.norm(dim=0, keepdim=True).shape)\n",
    "        result_2_norm = result_2.norm(dim=0, keepdim=True)\n",
    "        result_2_normalized = result_2/result_2_norm\n",
    "        \n",
    "        result = self.alpha * result_1_normalized.T@result_2_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "# ##################\n",
    "# GP layer\n",
    "# ##################\n",
    "class ExactGPLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, diff_net, P, kernel='NTK'):\n",
    "        super(ExactGPLayer, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gpytorch.means.ConstantMean()\n",
    "\n",
    "        ## NTKernel\n",
    "        if(kernel=='NTK'):\n",
    "            self.covar_module = NTKernel_proj(diff_net, P)\n",
    "        elif(kernel=='NTKcossim'):\n",
    "            self.covar_module = CosSimNTKernel_proj(diff_net, P)  \n",
    "        else:\n",
    "            raise ValueError(\"[ERROR] the kernel '\" + str(kernel) + \"' is not supported for regression, use 'rbf' or 'spectral'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1800d31-2dc1-4ef9-b1a0-8cc5b197cfca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118361\n",
      "U shape: torch.Size([118361, 402])\n",
      "Index tensor: tensor([401, 400, 399, 398, 397, 396, 395, 394, 393, 392, 391, 390, 389, 388,\n",
      "        387, 386, 385, 384, 383, 382, 381, 380, 379, 378, 377, 376, 375, 374,\n",
      "        373, 372, 371, 370, 369, 368, 367, 366, 365, 364, 363, 362, 361, 360,\n",
      "        359, 358, 357, 356, 355, 354, 353, 352, 351, 350, 349, 348, 347, 346,\n",
      "        345, 344, 343, 342, 341, 340, 339, 338, 337, 336, 335, 334, 333, 332,\n",
      "        331, 330, 329, 328, 327, 326, 325, 324, 323, 322, 321, 320, 319, 318,\n",
      "        317, 316, 315, 314, 313, 312, 311, 310, 309, 308, 307, 306, 305, 304,\n",
      "        303, 302, 301, 300, 299, 298, 297, 296, 295, 294, 293, 292, 291, 290,\n",
      "        289, 288, 287, 286, 285, 284, 283, 282, 281, 280, 279, 278, 277, 276,\n",
      "        275, 274, 273, 272, 271, 270, 269, 268, 267, 266, 265, 264, 263, 262,\n",
      "        261, 260, 259, 258, 257, 256, 255, 254, 253, 252, 251, 250, 249, 248,\n",
      "        247, 246, 245, 244, 243, 242, 241, 240, 239, 238, 237, 236, 235, 234,\n",
      "        233, 232, 231, 230, 229, 228, 227, 226, 225, 224, 223, 222, 221, 220,\n",
      "        219, 218, 217, 216, 215, 214, 213, 212, 211, 210, 209, 208, 207, 206,\n",
      "        205, 204, 203, 202, 201, 200, 199, 198, 197, 196, 195, 194, 193, 192,\n",
      "        191, 190, 189, 188, 187, 186, 185, 184, 183, 182, 181, 180, 179, 178,\n",
      "        177, 176, 175, 174, 173, 172, 171, 170, 169, 168, 167, 166, 165, 164,\n",
      "        163, 162, 161, 160, 159, 158, 157, 156, 155, 154, 153, 152, 151, 150,\n",
      "        149, 148, 147, 146, 145, 144, 143, 142, 141, 140, 139, 138, 137, 136,\n",
      "        135, 134, 133, 132, 131, 130, 129, 128, 127, 126, 125, 124, 123, 122,\n",
      "        121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108,\n",
      "        107, 106, 105, 104, 103, 102, 101, 100,  99,  98,  97,  96,  95,  94,\n",
      "         93,  92,  91,  90,  89,  88,  87,  86,  85,  84,  83,  82,  81,  80,\n",
      "         79,  78,  77,  76,  75,  74,  73,  72,  71,  70,  69,  68,  67,  66,\n",
      "         65,  64,  63,  62,  61,  60,  59,  58,  57,  56,  55,  54,  53,  52,\n",
      "         51,  50,  49,  48,  47,  46,  45,  44,  43,  42,  41,  40,  39,  38,\n",
      "         37,  36,  35,  34,  33,  32,  31,  30,  29,  28,  27,  26,  25,  24,\n",
      "         23,  22,  21,  20,  19,  18,  17,  16,  15,  14,  13,  12,  11,  10,\n",
      "          9,   8,   7,   6,   5,   4,   3,   2,   1,   0], device='cuda:0')\n",
      "Requested subspace dimension: 100\n",
      "Done sketching in 1.6088 s\n",
      "torch.Size([100, 118361])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Unlimitd-F training\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mUnLiMiTDproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimple_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m},\n\u001b[1;32m     32\u001b[0m                             {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mfeature_extractor\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m}])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n",
      "Cell \u001b[0;32mIn[39], line 27\u001b[0m, in \u001b[0;36mUnLiMiTDproj.__init__\u001b[0;34m(self, conv_net, diff_net, P)\u001b[0m\n\u001b[1;32m     25\u001b[0m input_dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m diff_net\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP \u001b[38;5;241m=\u001b[39m P\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_likelihood_mll\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 34\u001b[0m, in \u001b[0;36mUnLiMiTDproj.get_model_likelihood_mll\u001b[0;34m(self, train_x, train_y)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(train_y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m): train_y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m19\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     33\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m gpytorch\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mGaussianLikelihood()\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mExactGPLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNTK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel      \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood \u001b[38;5;241m=\u001b[39m likelihood\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'P'"
     ]
    }
   ],
   "source": [
    "from projection import create_random_projection_matrix, proj_sketch\n",
    "\n",
    "# FIM proj search needs no gradient\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad_(False)\n",
    "optimizer = None\n",
    "# Batch preparation\n",
    "nb_batch_proj = 10\n",
    "    \n",
    "batches = []\n",
    "for _ in range(nb_batch_proj):\n",
    "    batch, batch_labels = get_batch(train_people)\n",
    "    for person_task in batch :\n",
    "        person_conv = model.feature_extractor(person_task.cuda()).detach()\n",
    "        batches.append(person_conv)  \n",
    "batches = torch.stack(batches)\n",
    "# FIM projection computation\n",
    "input_dimension = sum(p.numel() for p in simple_net.parameters())\n",
    "P = proj_sketch(model.diff_net, batches, 100).cuda()\n",
    "print(P.shape)\n",
    "# Gradients back to training mode\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad_(True)\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    \n",
    "# Unlimitd-F training\n",
    "model = UnLiMiTDproj(bb, simple_net, P).cuda()\n",
    "optimizer = torch.optim.Adam([{'params': model.model.parameters(), 'lr': 0.001},\n",
    "                            {'params': model.feature_extractor.parameters(), 'lr': 0.001}])\n",
    "for epoch in range(100):\n",
    "    model.train_loop(epoch, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13a052f1-c765-4ae4-9282-814b8b8192d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Average MSE: 0.08749693734571337 +- 0.06433782456210793\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "mse_list = []\n",
    "for epoch in range(10):\n",
    "    mse = float(model.test_loop(5, optimizer).cpu().detach().numpy())\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35fe2cd0-470d-44d3-989f-e4489abf662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 5.6665e-01,  2.3517e-01,  4.5221e-01,  4.8291e-01,  4.1789e-03,\n",
      "         8.4329e-04,  3.0930e-04,  7.5234e-03,  3.8418e-01,  2.3201e-01,\n",
      "         1.6636e-02,  1.0108e-02,  1.6200e-01,  5.6738e-01,  5.9045e-02,\n",
      "         7.1729e-02,  1.4106e-02,  4.5085e-01,  6.1197e-02,  8.2781e-03,\n",
      "         3.5597e-01,  1.2833e-02,  1.2755e-02,  2.1864e-02,  8.9433e-02,\n",
      "         1.4421e-01,  3.4839e-01,  1.1730e-01,  7.4477e-03,  1.7662e-01,\n",
      "         1.4371e-01,  1.7320e-01,  7.3214e-02,  1.7837e-01,  1.1239e-01,\n",
      "         6.7366e-02,  2.5198e-01,  7.3716e-02,  4.1369e-02,  1.7291e-03,\n",
      "         3.3347e-01,  2.5798e-02,  3.7711e-02,  2.8154e-02,  1.3358e-01,\n",
      "         4.4981e-02,  1.5259e-01,  1.5888e-01,  5.7192e-02,  1.6061e-01,\n",
      "         4.8635e-02,  1.3826e-01,  1.9027e-01,  1.8114e-02,  2.7171e-02,\n",
      "         1.8298e-04,  1.4453e-01,  2.5789e-01, -1.8308e-08,  1.7037e-01,\n",
      "         2.6889e-01,  6.8537e-05,  4.8230e-02,  2.5538e-02,  1.6207e-04,\n",
      "         2.9230e-02,  1.2941e-01,  6.7404e-02,  1.4661e-02,  1.4958e-02,\n",
      "         7.3852e-03,  2.1549e-01,  2.3426e-02,  1.3731e-01,  8.2958e-02,\n",
      "         5.0807e-04,  2.1585e-02,  4.2821e-02,  7.0613e-03,  1.1765e-05,\n",
      "         2.8796e-05,  3.8597e-03,  3.0684e-01,  6.8718e-07,  6.9779e-02,\n",
      "         7.7000e-02,  2.2665e-01,  5.5692e-02,  4.6667e-04,  9.0762e-02,\n",
      "         1.7998e-02,  2.0314e-07,  1.8746e-03,  4.0245e-03,  2.8976e-04,\n",
      "         1.5474e-01,  1.2782e-03,  4.0606e-04,  9.7343e-02,  8.2803e-05],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.model.covar_module.scaling_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe9fafc6-c3c7-41eb-8c52-4de0f3d361e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Original packages\n",
    "import backbone\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "\n",
    "## Our packages\n",
    "import gpytorch\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "from statistics import mean\n",
    "from data.qmul_loader import get_batch, train_people, test_people\n",
    "\n",
    "\n",
    "class UnLiMiTDcov(nn.Module):\n",
    "    def __init__(self, conv_net, diff_net):\n",
    "        super(UnLiMiTDcov, self).__init__()\n",
    "        ## GP parameters\n",
    "        self.feature_extractor = conv_net\n",
    "        self.diff_net = diff_net  #Differentiable network\n",
    "        self.get_model_likelihood_mll() #Init model, likelihood, and mll\n",
    "\n",
    "    def get_model_likelihood_mll(self, train_x=None, train_y=None):\n",
    "        if(train_x is None): train_x=torch.ones(19, 2916).cuda()\n",
    "        if(train_y is None): train_y=torch.ones(19).cuda()\n",
    "\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = ExactGPLayer(train_x=train_x, train_y=train_y, likelihood=likelihood, diff_net = self.diff_net, kernel='NTKcossim')\n",
    "\n",
    "        self.model      = model.cuda()\n",
    "        self.likelihood = likelihood.cuda()\n",
    "        self.mll        = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model).cuda()\n",
    "        self.mse        = nn.MSELoss()\n",
    "\n",
    "        return self.model, self.likelihood, self.mll\n",
    "\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def train_loop(self, epoch, optimizer):\n",
    "        batch, batch_labels = get_batch(train_people)\n",
    "        batch, batch_labels = batch.cuda(), batch_labels.cuda()\n",
    "        for inputs, labels in zip(batch, batch_labels):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs_conv = self.feature_extractor(inputs)\n",
    "            self.model.set_train_data(inputs=inputs_conv, targets=labels - self.diff_net(inputs_conv).reshape(-1))  \n",
    "            predictions = self.model(inputs_conv)\n",
    "            loss = -self.mll(predictions, self.model.train_targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            mse = self.mse(predictions.mean, labels)\n",
    "\n",
    "            if (epoch%10==0):\n",
    "                print('[%d] - Loss: %.3f  MSE: %.3f noise: %.3f' % (\n",
    "                    epoch, loss.item(), mse.item(),\n",
    "                    self.model.likelihood.noise.item()\n",
    "                ))\n",
    "\n",
    "    def test_loop(self, n_support, optimizer=None): # no optimizer needed for GP\n",
    "        inputs, targets = get_batch(test_people)\n",
    "\n",
    "        support_ind = list(np.random.choice(list(range(19)), replace=False, size=n_support))\n",
    "        query_ind   = [i for i in range(19) if i not in support_ind]\n",
    "\n",
    "        x_all = inputs.cuda()\n",
    "        y_all = targets.cuda()\n",
    "\n",
    "        x_support = inputs[:,support_ind,:,:,:].cuda()\n",
    "        y_support = targets[:,support_ind].cuda()\n",
    "\n",
    "        # choose a random test person\n",
    "        n = np.random.randint(0, len(test_people)-1)\n",
    "    \n",
    "        x_conv_support = self.feature_extractor(x_support[n]).detach()\n",
    "        self.model.set_train_data(inputs=x_conv_support, targets=y_support[n] - self.diff_net(x_conv_support).reshape(-1), strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        self.feature_extractor.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_conv_query = self.feature_extractor(x_all[n]).detach()\n",
    "            pred    = self.likelihood(self.model(x_conv_query))\n",
    "            lower, upper = pred.confidence_region() #2 standard deviations above and below the mean\n",
    "            lower += self.diff_net(x_conv_query).reshape(-1)\n",
    "            upper += self.diff_net(x_conv_query).reshape(-1)\n",
    "        mse = self.mse(pred.mean + self.diff_net(self.feature_extractor(x_all[n])).reshape(-1), y_all[n])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def save_checkpoint(self, checkpoint):\n",
    "        # save state\n",
    "        gp_state_dict         = self.model.state_dict()\n",
    "        likelihood_state_dict = self.likelihood.state_dict()\n",
    "        conv_net_state_dict   = self.feature_extractor.state_dict()\n",
    "        diff_net_state_dict   = self.diff_net.state_dict()\n",
    "        torch.save({'gp': gp_state_dict, 'likelihood': likelihood_state_dict, 'conv_net':conv_net_state_dict, 'diff_net':diff_net_state_dict}, checkpoint)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        ckpt = torch.load(checkpoint)\n",
    "        self.model.load_state_dict(ckpt['gp'])\n",
    "        self.likelihood.load_state_dict(ckpt['likelihood'])\n",
    "        self.feature_extractor.load_state_dict(ckpt['conv_net'])\n",
    "        self.diff_net.load_state_dict(ckpt['diff_net'])\n",
    "\n",
    "        \n",
    "# ##################\n",
    "# NTKernel\n",
    "# ##################\n",
    "\n",
    "class NTKernelcov(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernelcov, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        # Add number of params scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(sum(p.numel() for p in simple_net.parameters())))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        result1 = self.scaling_param * jac1\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        result2 = self.scaling_param * jac2\n",
    "        \n",
    "        result = result1 @ result2.T\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "\n",
    "    \n",
    "# ##################\n",
    "# NTKernel CosSim\n",
    "# ##################\n",
    "\n",
    "class CosSimNTKernelcov(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(CosSimNTKernelcov, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # Add subspace_dimension scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(sum(p.numel() for p in simple_net.parameters())))\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        result_1 = (self.scaling_param * jac1).T\n",
    "        result_2 = (self.scaling_param * jac2).T\n",
    "        \n",
    "        result_1_norm = result_1.norm(dim=0, keepdim=True)\n",
    "        result_1_normalized = result_1/result_1_norm\n",
    "        #print(result_1.shape)\n",
    "        #print(result_1.norm(dim=0, keepdim=True).shape)\n",
    "        result_2_norm = result_2.norm(dim=0, keepdim=True)\n",
    "        result_2_normalized = result_2/result_2_norm\n",
    "        \n",
    "        result = self.alpha * result_1_normalized.T@result_2_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T\n",
    "\n",
    "    \n",
    "###################\n",
    "#GP\n",
    "###################    \n",
    "class ExactGPLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, diff_net, kernel='NTK'):\n",
    "        super(ExactGPLayer, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module  = gpytorch.means.ConstantMean()\n",
    "\n",
    "        ## NTKernel\n",
    "        if(kernel=='NTK'):\n",
    "            self.covar_module = NTKernelcov(diff_net)\n",
    "        elif(kernel=='NTKcossim'):\n",
    "            self.covar_module = CosSimNTKernelcov(diff_net)        \n",
    "        else:\n",
    "            raise ValueError(\"[ERROR] the kernel '\" + str(kernel) + \"' is not supported for regression, use 'rbf' or 'spectral'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x  = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "331d41a6-6573-4604-8f45-4db33299b1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - Loss: 0.828  MSE: 0.000 noise: 0.693\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.824  MSE: 0.000 noise: 0.692\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.823  MSE: 0.000 noise: 0.691\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.822  MSE: 0.000 noise: 0.690\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.689\n",
      "[0] - Loss: 0.821  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.688\n",
      "[0] - Loss: 0.820  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.687\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.819  MSE: 0.000 noise: 0.686\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.685\n",
      "[0] - Loss: 0.818  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.684\n",
      "[0] - Loss: 0.817  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.683\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.816  MSE: 0.000 noise: 0.682\n",
      "[0] - Loss: 0.815  MSE: 0.000 noise: 0.681\n",
      "[10] - Loss: 0.775  MSE: 0.218 noise: 0.580\n",
      "[10] - Loss: 0.818  MSE: 0.218 noise: 0.579\n",
      "[10] - Loss: 0.798  MSE: 0.218 noise: 0.579\n",
      "[10] - Loss: 0.792  MSE: 0.218 noise: 0.578\n",
      "[10] - Loss: 0.824  MSE: 0.219 noise: 0.578\n",
      "[10] - Loss: 0.843  MSE: 0.219 noise: 0.577\n",
      "[10] - Loss: 0.793  MSE: 0.219 noise: 0.577\n",
      "[10] - Loss: 0.797  MSE: 0.219 noise: 0.576\n",
      "[10] - Loss: 0.805  MSE: 0.219 noise: 0.576\n",
      "[10] - Loss: 0.802  MSE: 0.220 noise: 0.576\n",
      "[10] - Loss: 0.808  MSE: 0.220 noise: 0.575\n",
      "[10] - Loss: 0.811  MSE: 0.220 noise: 0.575\n",
      "[10] - Loss: 0.794  MSE: 0.220 noise: 0.574\n",
      "[10] - Loss: 0.786  MSE: 0.220 noise: 0.574\n",
      "[10] - Loss: 0.781  MSE: 0.220 noise: 0.573\n",
      "[10] - Loss: 0.773  MSE: 0.221 noise: 0.573\n",
      "[10] - Loss: 0.811  MSE: 0.221 noise: 0.572\n",
      "[10] - Loss: 0.813  MSE: 0.221 noise: 0.572\n",
      "[10] - Loss: 0.794  MSE: 0.221 noise: 0.571\n",
      "[10] - Loss: 0.854  MSE: 0.221 noise: 0.571\n",
      "[10] - Loss: 0.863  MSE: 0.221 noise: 0.571\n",
      "[10] - Loss: 0.781  MSE: 0.222 noise: 0.570\n",
      "[10] - Loss: 0.766  MSE: 0.222 noise: 0.570\n",
      "[10] - Loss: 0.774  MSE: 0.222 noise: 0.569\n",
      "[20] - Loss: 0.709  MSE: 0.594 noise: 0.478\n",
      "[20] - Loss: 0.725  MSE: 0.595 noise: 0.478\n",
      "[20] - Loss: 0.696  MSE: 0.595 noise: 0.477\n",
      "[20] - Loss: 0.693  MSE: 0.596 noise: 0.477\n",
      "[20] - Loss: 0.704  MSE: 0.596 noise: 0.476\n",
      "[20] - Loss: 0.810  MSE: 0.596 noise: 0.476\n",
      "[20] - Loss: 0.740  MSE: 0.597 noise: 0.476\n",
      "[20] - Loss: 0.712  MSE: 0.597 noise: 0.475\n",
      "[20] - Loss: 0.721  MSE: 0.598 noise: 0.475\n",
      "[20] - Loss: 0.725  MSE: 0.598 noise: 0.474\n",
      "[20] - Loss: 0.736  MSE: 0.599 noise: 0.474\n",
      "[20] - Loss: 0.783  MSE: 0.599 noise: 0.474\n",
      "[20] - Loss: 0.720  MSE: 0.599 noise: 0.473\n",
      "[20] - Loss: 0.714  MSE: 0.600 noise: 0.473\n",
      "[20] - Loss: 0.683  MSE: 0.600 noise: 0.473\n",
      "[20] - Loss: 0.678  MSE: 0.601 noise: 0.472\n",
      "[20] - Loss: 0.727  MSE: 0.601 noise: 0.472\n",
      "[20] - Loss: 0.750  MSE: 0.601 noise: 0.471\n",
      "[20] - Loss: 0.701  MSE: 0.601 noise: 0.471\n",
      "[20] - Loss: 0.866  MSE: 0.602 noise: 0.471\n",
      "[20] - Loss: 0.786  MSE: 0.602 noise: 0.470\n",
      "[20] - Loss: 0.675  MSE: 0.603 noise: 0.470\n",
      "[20] - Loss: 0.670  MSE: 0.603 noise: 0.470\n",
      "[20] - Loss: 0.690  MSE: 0.604 noise: 0.469\n",
      "[30] - Loss: 0.709  MSE: 0.377 noise: 0.388\n",
      "[30] - Loss: 0.654  MSE: 0.377 noise: 0.387\n",
      "[30] - Loss: 0.637  MSE: 0.377 noise: 0.387\n",
      "[30] - Loss: 0.636  MSE: 0.377 noise: 0.387\n",
      "[30] - Loss: 0.640  MSE: 0.377 noise: 0.386\n",
      "[30] - Loss: 0.786  MSE: 0.378 noise: 0.386\n",
      "[30] - Loss: 0.650  MSE: 0.378 noise: 0.386\n",
      "[30] - Loss: 0.665  MSE: 0.379 noise: 0.385\n",
      "[30] - Loss: 0.595  MSE: 0.380 noise: 0.385\n",
      "[30] - Loss: 0.642  MSE: 0.381 noise: 0.385\n",
      "[30] - Loss: 0.671  MSE: 0.381 noise: 0.384\n",
      "[30] - Loss: 0.703  MSE: 0.382 noise: 0.384\n",
      "[30] - Loss: 0.622  MSE: 0.382 noise: 0.384\n",
      "[30] - Loss: 0.591  MSE: 0.383 noise: 0.384\n",
      "[30] - Loss: 0.560  MSE: 0.383 noise: 0.383\n",
      "[30] - Loss: 0.568  MSE: 0.383 noise: 0.383\n",
      "[30] - Loss: 0.599  MSE: 0.384 noise: 0.383\n",
      "[30] - Loss: 0.579  MSE: 0.384 noise: 0.382\n",
      "[30] - Loss: 0.603  MSE: 0.385 noise: 0.382\n",
      "[30] - Loss: 0.791  MSE: 0.385 noise: 0.382\n",
      "[30] - Loss: 0.643  MSE: 0.386 noise: 0.381\n",
      "[30] - Loss: 0.557  MSE: 0.386 noise: 0.381\n",
      "[30] - Loss: 0.556  MSE: 0.387 noise: 0.381\n",
      "[30] - Loss: 0.550  MSE: 0.387 noise: 0.380\n",
      "[40] - Loss: 0.469  MSE: 0.393 noise: 0.314\n",
      "[40] - Loss: 0.467  MSE: 0.393 noise: 0.314\n",
      "[40] - Loss: 0.468  MSE: 0.393 noise: 0.314\n",
      "[40] - Loss: 0.481  MSE: 0.393 noise: 0.313\n",
      "[40] - Loss: 0.499  MSE: 0.393 noise: 0.313\n",
      "[40] - Loss: 0.567  MSE: 0.393 noise: 0.313\n",
      "[40] - Loss: 0.503  MSE: 0.393 noise: 0.312\n",
      "[40] - Loss: 0.529  MSE: 0.394 noise: 0.312\n",
      "[40] - Loss: 0.482  MSE: 0.394 noise: 0.312\n",
      "[40] - Loss: 0.467  MSE: 0.395 noise: 0.312\n",
      "[40] - Loss: 0.492  MSE: 0.395 noise: 0.311\n",
      "[40] - Loss: 0.505  MSE: 0.395 noise: 0.311\n",
      "[40] - Loss: 0.481  MSE: 0.395 noise: 0.311\n",
      "[40] - Loss: 0.483  MSE: 0.396 noise: 0.310\n",
      "[40] - Loss: 0.442  MSE: 0.396 noise: 0.310\n",
      "[40] - Loss: 0.449  MSE: 0.396 noise: 0.310\n",
      "[40] - Loss: 0.509  MSE: 0.397 noise: 0.310\n",
      "[40] - Loss: 0.477  MSE: 0.397 noise: 0.309\n",
      "[40] - Loss: 0.484  MSE: 0.397 noise: 0.309\n",
      "[40] - Loss: 0.607  MSE: 0.397 noise: 0.309\n",
      "[40] - Loss: 0.549  MSE: 0.397 noise: 0.309\n",
      "[40] - Loss: 0.442  MSE: 0.397 noise: 0.308\n",
      "[40] - Loss: 0.442  MSE: 0.397 noise: 0.308\n",
      "[40] - Loss: 0.458  MSE: 0.398 noise: 0.308\n",
      "[50] - Loss: 0.321  MSE: 0.191 noise: 0.251\n",
      "[50] - Loss: 0.405  MSE: 0.191 noise: 0.251\n",
      "[50] - Loss: 0.378  MSE: 0.191 noise: 0.251\n",
      "[50] - Loss: 0.329  MSE: 0.191 noise: 0.250\n",
      "[50] - Loss: 0.377  MSE: 0.191 noise: 0.250\n",
      "[50] - Loss: 0.392  MSE: 0.192 noise: 0.250\n",
      "[50] - Loss: 0.347  MSE: 0.192 noise: 0.250\n",
      "[50] - Loss: 0.360  MSE: 0.192 noise: 0.250\n",
      "[50] - Loss: 0.385  MSE: 0.192 noise: 0.249\n",
      "[50] - Loss: 0.397  MSE: 0.192 noise: 0.249\n",
      "[50] - Loss: 0.375  MSE: 0.192 noise: 0.249\n",
      "[50] - Loss: 0.389  MSE: 0.191 noise: 0.249\n",
      "[50] - Loss: 0.331  MSE: 0.191 noise: 0.248\n",
      "[50] - Loss: 0.341  MSE: 0.191 noise: 0.248\n",
      "[50] - Loss: 0.320  MSE: 0.191 noise: 0.248\n",
      "[50] - Loss: 0.327  MSE: 0.190 noise: 0.248\n",
      "[50] - Loss: 0.365  MSE: 0.190 noise: 0.247\n",
      "[50] - Loss: 0.356  MSE: 0.190 noise: 0.247\n",
      "[50] - Loss: 0.380  MSE: 0.189 noise: 0.247\n",
      "[50] - Loss: 0.397  MSE: 0.189 noise: 0.247\n",
      "[50] - Loss: 0.417  MSE: 0.189 noise: 0.247\n",
      "[50] - Loss: 0.332  MSE: 0.189 noise: 0.246\n",
      "[50] - Loss: 0.315  MSE: 0.188 noise: 0.246\n",
      "[50] - Loss: 0.317  MSE: 0.188 noise: 0.246\n",
      "[60] - Loss: 0.493  MSE: 0.635 noise: 0.201\n",
      "[60] - Loss: 0.332  MSE: 0.635 noise: 0.201\n",
      "[60] - Loss: 0.278  MSE: 0.635 noise: 0.200\n",
      "[60] - Loss: 0.274  MSE: 0.635 noise: 0.200\n",
      "[60] - Loss: 0.297  MSE: 0.635 noise: 0.200\n",
      "[60] - Loss: 0.569  MSE: 0.636 noise: 0.200\n",
      "[60] - Loss: 0.308  MSE: 0.636 noise: 0.200\n",
      "[60] - Loss: 0.362  MSE: 0.637 noise: 0.199\n",
      "[60] - Loss: 0.302  MSE: 0.637 noise: 0.199\n",
      "[60] - Loss: 0.406  MSE: 0.638 noise: 0.199\n",
      "[60] - Loss: 0.318  MSE: 0.638 noise: 0.199\n",
      "[60] - Loss: 0.417  MSE: 0.638 noise: 0.199\n",
      "[60] - Loss: 0.251  MSE: 0.638 noise: 0.199\n",
      "[60] - Loss: 0.333  MSE: 0.638 noise: 0.198\n",
      "[60] - Loss: 0.308  MSE: 0.638 noise: 0.198\n",
      "[60] - Loss: 0.228  MSE: 0.638 noise: 0.198\n",
      "[60] - Loss: 0.280  MSE: 0.638 noise: 0.198\n",
      "[60] - Loss: 0.382  MSE: 0.637 noise: 0.198\n",
      "[60] - Loss: 0.333  MSE: 0.637 noise: 0.198\n",
      "[60] - Loss: 0.513  MSE: 0.637 noise: 0.197\n",
      "[60] - Loss: 0.439  MSE: 0.637 noise: 0.197\n",
      "[60] - Loss: 0.254  MSE: 0.637 noise: 0.197\n",
      "[60] - Loss: 0.238  MSE: 0.636 noise: 0.197\n",
      "[60] - Loss: 0.270  MSE: 0.636 noise: 0.197\n",
      "[70] - Loss: 0.131  MSE: 0.119 noise: 0.160\n",
      "[70] - Loss: 0.127  MSE: 0.119 noise: 0.160\n",
      "[70] - Loss: 0.182  MSE: 0.119 noise: 0.160\n",
      "[70] - Loss: 0.114  MSE: 0.119 noise: 0.159\n",
      "[70] - Loss: 0.110  MSE: 0.119 noise: 0.159\n",
      "[70] - Loss: 0.114  MSE: 0.119 noise: 0.159\n",
      "[70] - Loss: 0.090  MSE: 0.119 noise: 0.159\n",
      "[70] - Loss: 0.118  MSE: 0.119 noise: 0.159\n",
      "[70] - Loss: 0.106  MSE: 0.119 noise: 0.159\n",
      "[70] - Loss: 0.147  MSE: 0.119 noise: 0.158\n",
      "[70] - Loss: 0.111  MSE: 0.119 noise: 0.158\n",
      "[70] - Loss: 0.122  MSE: 0.119 noise: 0.158\n",
      "[70] - Loss: 0.085  MSE: 0.118 noise: 0.158\n",
      "[70] - Loss: 0.099  MSE: 0.118 noise: 0.158\n",
      "[70] - Loss: 0.092  MSE: 0.118 noise: 0.158\n",
      "[70] - Loss: 0.087  MSE: 0.118 noise: 0.157\n",
      "[70] - Loss: 0.115  MSE: 0.118 noise: 0.157\n",
      "[70] - Loss: 0.130  MSE: 0.117 noise: 0.157\n",
      "[70] - Loss: 0.137  MSE: 0.117 noise: 0.157\n",
      "[70] - Loss: 0.094  MSE: 0.117 noise: 0.157\n",
      "[70] - Loss: 0.130  MSE: 0.116 noise: 0.157\n",
      "[70] - Loss: 0.082  MSE: 0.116 noise: 0.156\n",
      "[70] - Loss: 0.079  MSE: 0.116 noise: 0.156\n",
      "[70] - Loss: 0.076  MSE: 0.115 noise: 0.156\n",
      "[80] - Loss: 0.031  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.064  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.053  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.009  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.082  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.070  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.037  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.126  MSE: 0.280 noise: 0.127\n",
      "[80] - Loss: 0.010  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.072  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.078  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.119  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.061  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.104  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.010  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.053  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.067  MSE: 0.280 noise: 0.126\n",
      "[80] - Loss: 0.009  MSE: 0.280 noise: 0.125\n",
      "[80] - Loss: 0.078  MSE: 0.280 noise: 0.125\n",
      "[80] - Loss: 0.178  MSE: 0.280 noise: 0.125\n",
      "[80] - Loss: 0.085  MSE: 0.280 noise: 0.125\n",
      "[80] - Loss: 0.010  MSE: 0.280 noise: 0.125\n",
      "[80] - Loss: 0.017  MSE: 0.280 noise: 0.125\n",
      "[80] - Loss: 0.012  MSE: 0.280 noise: 0.125\n",
      "[90] - Loss: -0.093  MSE: 0.170 noise: 0.100\n",
      "[90] - Loss: -0.107  MSE: 0.170 noise: 0.100\n",
      "[90] - Loss: -0.044  MSE: 0.170 noise: 0.100\n",
      "[90] - Loss: -0.106  MSE: 0.170 noise: 0.100\n",
      "[90] - Loss: -0.026  MSE: 0.170 noise: 0.100\n",
      "[90] - Loss: -0.106  MSE: 0.169 noise: 0.100\n",
      "[90] - Loss: -0.110  MSE: 0.169 noise: 0.100\n",
      "[90] - Loss: -0.040  MSE: 0.169 noise: 0.100\n",
      "[90] - Loss: -0.119  MSE: 0.169 noise: 0.100\n",
      "[90] - Loss: -0.063  MSE: 0.169 noise: 0.099\n",
      "[90] - Loss: -0.088  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.084  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.080  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: 0.003  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.118  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.125  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.095  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.067  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.065  MSE: 0.168 noise: 0.099\n",
      "[90] - Loss: -0.017  MSE: 0.167 noise: 0.099\n",
      "[90] - Loss: -0.012  MSE: 0.167 noise: 0.098\n",
      "[90] - Loss: -0.120  MSE: 0.167 noise: 0.098\n",
      "[90] - Loss: -0.112  MSE: 0.167 noise: 0.098\n",
      "[90] - Loss: -0.129  MSE: 0.167 noise: 0.098\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "bb           = backbone.Conv3().cuda()\n",
    "simple_net   = backbone.simple_net().cuda()\n",
    "\n",
    "model = UnLiMiTDcov(bb, simple_net).cuda()\n",
    "optimizer = torch.optim.Adam([{'params': model.model.parameters(), 'lr': 0.001},\n",
    "                                {'params': model.feature_extractor.parameters(), 'lr': 0.001}])\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train_loop(epoch, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "282de7bd-525f-46e8-970f-1530cf1f78bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Average MSE: 0.056465080054476854 +- 0.03701971053232096\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "mse_list = []\n",
    "for epoch in range(10):\n",
    "    mse = float(model.test_loop(5, optimizer).cpu().detach().numpy())\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4c1e2ca-6e6e-4863-b56c-38d937979e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.9964, 0.9960, 0.9960,  ..., 0.9583, 1.0000, 1.0113], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.model.covar_module.scaling_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892e2bb-fc8a-4639-924c-aa4f739b16de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
