{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e82cf160-d685-4355-9bf2-044b3e7d8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nll\n",
    "from jax import numpy as np\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import dataset_sines_infinite\n",
    "import dataset_sines_finite\n",
    "import dataset_step_infinite\n",
    "from jax import vmap\n",
    "from jax import scipy\n",
    "from jax.scipy.special import gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6706391-9598-4182-92d2-4e2ddd8f3cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655235988902897757\n"
     ]
    }
   ],
   "source": [
    "seed = 1655235988902897757\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f68c9fbf-ca25-4567-8a7d-9e847d8bec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_posterior_full(kernel_matrix, x_a, y_a, x_b, maddox_noise):\n",
    "    \"\"\"\n",
    "    Computes the gaussian posterior with this kernel and this data, on the queried inputs.\n",
    "    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\n",
    "    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\n",
    "    Returns the posterior covariance matrix\n",
    "    \"\"\"\n",
    "    y_a = np.reshape(y_a, (-1,))\n",
    "\n",
    "    cov_a_a = kernel_matrix(x_a, x_a)\n",
    "    cov_a_a = cov_a_a + maddox_noise ** 2 * np.eye(cov_a_a.shape[0])\n",
    "    cov_b_a = kernel_matrix(x_b, x_a)\n",
    "    cov_b_b = kernel_matrix(x_b, x_b)\n",
    "\n",
    "    print(cov_a_a.shape)\n",
    "    print(cov_b_a.shape)\n",
    "    print(cov_b_b.shape)\n",
    "    print(y_a.shape)\n",
    "\n",
    "    L = scipy.linalg.cho_factor(cov_a_a)\n",
    "    alpha = scipy.linalg.cho_solve(L, y_a)\n",
    "    post_mean = cov_b_a @ alpha\n",
    "    \n",
    "    v = scipy.linalg.cho_solve(L, cov_b_a.T)\n",
    "    post_cov = cov_b_b - cov_b_a @ v\n",
    "    \n",
    "    return post_mean, post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df383f16-aff5-4c90-b2cb-8496d38c28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpr(x_a_all, y_a_all, x_b, y_b, kernel_matrix, K, dataset_provider):\n",
    "    \"\"\"\n",
    "    Make an informative prediction plot in the singGP case (for the kernel specified)\n",
    "    K is the number of context inputs\n",
    "    Change dataset_provider to test on other datasets (e.g. dataset_sines_infinite)\n",
    "    \"\"\"\n",
    "    y_min, y_max = np.min(y_b) - 0.5, np.max(y_b) + 0.5\n",
    "\n",
    "    x_a = x_a_all[:K]\n",
    "    y_a = y_a_all[:K]\n",
    "    prediction, cov = gaussian_posterior_full(kernel_matrix, x_a, y_a, x_b, 0.05)\n",
    "\n",
    "    error = dataset_provider.error_fn(prediction, y_b)\n",
    "    loss = nll.nll(kernel_self_matrix, x_a, y_a, maddox_noise=0.05)\n",
    "\n",
    "    variances = np.diag(cov)\n",
    "    stds = np.sqrt(variances)\n",
    "\n",
    "    plt.plot(x_b, y_b, \"g--\", label=\"Target\")\n",
    "    plt.plot(x_a, y_a, \"ro\", label=\"Context data\")\n",
    "    plt.plot(x_b, prediction, \"b\", label=\"Prediction\")\n",
    "    plt.fill_between(x_b, prediction - 1.96 * stds, prediction + 1.96 * stds, color='blue', alpha=0.1, label=\"+/- 1.96$\\sigma$\")\n",
    "    plt.title(f\"NLL={loss:.4f}, MSE={error:.4f} ($K$={K})\")\n",
    "    plt.legend()\n",
    "    plt.gca().set_ylim([np.min(prediction), np.max(prediction)])\n",
    "    plt.gca().set_xlabel(\"$x$\")\n",
    "    plt.gca().set_ylabel(\"$y$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5af5bf-a131-4bf2-8832-3d1a8f01a1e5",
   "metadata": {},
   "source": [
    "## Choice of kernels : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf849b91-0237-4928-b290-eb68347c40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1\n",
    "\n",
    "def RBF_kernel(x1, x2):\n",
    "    # Now x1 and x2 are compatible for broadcasting\n",
    "    # Compute squared Euclidean distance\n",
    "    squared_diff = (x1 - x2) ** 2 / (2*l**2)\n",
    "    return np.exp(-squared_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ffd83707-021b-4138-b3f7-62ee8b93f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosSim_kernel(x1, x2):\n",
    "    normalized_factor = np.linalg.norm(x1)*np.linalg.norm(x2)\n",
    "    return np.dot(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55ab77a4-0845-44f4-9b1b-1aeefede2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "c = 1\n",
    "\n",
    "def polynomial_kernel(x1, x2):\n",
    "    return (np.dot(x1, x2) + c)**p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6948a007-013e-499c-bacd-333ed027dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 1\n",
    "nu = 2.5\n",
    "\n",
    "def matern_kernel(x1, x2):\n",
    "    # Euclidian distance\n",
    "    sqdist = np.sum((x1 - x2) ** 2)\n",
    "    r = np.sqrt(sqdist)\n",
    "\n",
    "    # Scaling factor\n",
    "    sqrt_2_nu_r_over_l = np.sqrt(2 * nu) * r / length_scale\n",
    "\n",
    "    # Matérn kernel formula\n",
    "    coefficient = (2 ** (1 - nu)) / gamma(nu)\n",
    "    result = coefficient * (sqrt_2_nu_r_over_l ** nu) * bessel_k(nu, sqrt_2_nu_r_over_l)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e4f5d4d-114b-4bdc-8f73-ffe763e0ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose here: \n",
    "kernel = matern_kernel\n",
    "\n",
    "# Apply vmap to vectorize kernel function over pairs of inputs\n",
    "kernel_matrix = vmap(vmap(kernel, in_axes=(None, 0)), in_axes=(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dad833d9-2a0f-40f4-ad0f-43ad3cde95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a2f48e5-7f4b-40d2-87a6-be0373246605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,)\n",
      "(100,) (100,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'trapz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m y_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(y_b, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_b\u001b[38;5;241m.\u001b[39mshape, y_b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mplot_gpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_a_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sines_infinite\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 11\u001b[0m, in \u001b[0;36mplot_gpr\u001b[0;34m(x_a_all, y_a_all, x_b, y_b, kernel_matrix, K, dataset_provider)\u001b[0m\n\u001b[1;32m      9\u001b[0m x_a \u001b[38;5;241m=\u001b[39m x_a_all[:K]\n\u001b[1;32m     10\u001b[0m y_a \u001b[38;5;241m=\u001b[39m y_a_all[:K]\n\u001b[0;32m---> 11\u001b[0m prediction, cov \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_posterior_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m error \u001b[38;5;241m=\u001b[39m dataset_provider\u001b[38;5;241m.\u001b[39merror_fn(prediction, y_b)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m nll\u001b[38;5;241m.\u001b[39mnll(kernel_self_matrix, x_a, y_a, maddox_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 10\u001b[0m, in \u001b[0;36mgaussian_posterior_full\u001b[0;34m(kernel_matrix, x_a, y_a, x_b, maddox_noise)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mComputes the gaussian posterior with this kernel and this data, on the queried inputs.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mx_a is a (batch_size, input_dims) array (! has lost n_tasks)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03my_a is a (batch_size, reg_dim) array (! has lost n_tasks)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mReturns the posterior covariance matrix\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(y_a, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m---> 10\u001b[0m cov_a_a \u001b[38;5;241m=\u001b[39m \u001b[43mkernel_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m cov_a_a \u001b[38;5;241m=\u001b[39m cov_a_a \u001b[38;5;241m+\u001b[39m maddox_noise \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(cov_a_a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m cov_b_a \u001b[38;5;241m=\u001b[39m kernel_matrix(x_b, x_a)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[99], line 14\u001b[0m, in \u001b[0;36mmatern_kernel\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Matérn kernel formula\u001b[39;00m\n\u001b[1;32m     13\u001b[0m coefficient \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m nu)) \u001b[38;5;241m/\u001b[39m gamma(nu)\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m coefficient \u001b[38;5;241m*\u001b[39m (sqrt_2_nu_r_over_l \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m nu) \u001b[38;5;241m*\u001b[39m \u001b[43mbessel_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqrt_2_nu_r_over_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[105], line 24\u001b[0m, in \u001b[0;36mbessel_k\u001b[0;34m(nu, z)\u001b[0m\n\u001b[1;32m     21\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m101\u001b[39m)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m     22\u001b[0m integrand \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mz)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnu\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mphi(t)\u001b[38;5;241m-\u001b[39mz\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39mphi(t)))\u001b[38;5;241m*\u001b[39mphi(t)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39mnu\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mdphi(t)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrapz\u001b[49m(integrand, x\u001b[38;5;241m=\u001b[39mt, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.numpy' has no attribute 'trapz'"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "c = 1\n",
    "\n",
    "K = 100\n",
    "x, y, fun = dataset_sines_infinite.get_fancy_test_batch(key, K=K, L=0, data_noise=0.05)\n",
    "\n",
    "x_a_all = x[0, :K]\n",
    "y_a_all = y[0, :K]\n",
    "x_a_all = np.reshape(x_a_all, (-1,))\n",
    "y_a_all = np.reshape(y_a_all, (-1,))\n",
    "print(x_a_all.shape, y_a_all.shape)\n",
    "\n",
    "x_b = np.linspace(-5, 5, 100)[:, np.newaxis]\n",
    "y_b = fun(x_b)\n",
    "x_b = np.reshape(x_b, (-1,))\n",
    "y_b = np.reshape(y_b, (-1,))\n",
    "print(x_b.shape, y_b.shape)\n",
    "\n",
    "plot_gpr(x_a_all, y_a_all, x_b, y_b, kernel_matrix, K, dataset_sines_infinite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3953f2-4960-41cd-aa39-e2e4bf4a7e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
