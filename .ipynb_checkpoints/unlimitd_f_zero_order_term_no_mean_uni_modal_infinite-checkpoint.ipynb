{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05e490cb-34a9-4163-9053-f24161a24b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_UNIFIED_MEMORY=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_UNIFIED_MEMORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34f9178a-a7e8-45a5-a963-d606cbdfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unlimtd_f\n",
    "import time\n",
    "from jax import random, jit, pmap, value_and_grad, lax\n",
    "import dataset_multi_infinite\n",
    "import dataset_lines_infinite\n",
    "import test\n",
    "import plots\n",
    "import ntk\n",
    "import nll\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "import pickle\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9a7d732-78fc-4f92-8ca7-d8f7cb2a07fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655235988902897757\n"
     ]
    }
   ],
   "source": [
    "seed = 1655235988902897757\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "815a68c8-dec0-4665-be22-23a13d4f2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise):\n",
    "    \"\"\"\n",
    "    Computes the NLL of this data (one task only) wrt the kernel\n",
    "    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\n",
    "    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\n",
    "    \"\"\"\n",
    "    cov_a_a = kernel_self(x_a)\n",
    "    K = cov_a_a.shape[0]\n",
    "    cov_a_a = cov_a_a + maddox_noise ** 2 * np.eye(K)\n",
    "    \n",
    "    # prior mean is 0\n",
    "    y_a = np.reshape(y_a, (-1))\n",
    "\n",
    "    L = jax.scipy.linalg.cho_factor(cov_a_a)\n",
    "    ypred_a = np.reshape(part_apply_fn(inputs = x_a), (-1,))\n",
    "    alpha = jax.scipy.linalg.cho_solve(L, y_a - ypred_a)\n",
    "    return 0.5 * (y_a - ypred_a).T @ alpha + np.sum(np.log(np.diag(L[0]))) + 0.5 * K * np.log(2 * np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f4dea8a-7338-4799-8dc1-21fbf6e80e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian):\n",
    "    \"\"\"\n",
    "    NLL for a batch of tasks, when there is only one kernel (singGP)\n",
    "    x_a is (n_tasks, batch_size, input_dims) (input_dims are (128, 128, 1) for vision, (1,) for toy problems)\n",
    "    y_a is (n_tasks, batch_size, reg_dim)\n",
    "    \"\"\"\n",
    "    def f(carry, task_data):\n",
    "        x_a, y_a = task_data\n",
    "        loss_here = nll(kernel_self, part_apply_fn, x_a, y_a, maddox_noise)\n",
    "        return None, loss_here\n",
    "\n",
    "    _, losses = lax.scan(f, None, (x_a, y_a))\n",
    "\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b4f811d-a0ac-49ea-ad24-39f3f56142b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def nll_batch_average_identity_cov(current_params, apply_fn, current_batch_stats, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n",
    "    part_apply_fn = (partial(apply_fn, current_params, current_batch_stats))\n",
    "    \n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86f2b355-f145-46fb-9e58-299ac9a43530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_identity_cov(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p) = value_and_grad(nll_batch_average_identity_cov, argnums = (0) )(current_state.params,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4f8da8f-2182-47e9-9af3-9aa84fcd0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_stats_updater(current_state, x_a):\n",
    "    # shape of x_a is (n_tasks, batch_size, inputs_dims...)\n",
    "    \n",
    "    batch_stats = current_state.batch_stats\n",
    "    \n",
    "    def f(old_batch_stats, _x_a):\n",
    "        # shape of _x_a is (batch_size, input_dims)\n",
    "        _, mutated_vars = current_state.apply_fn_raw({\"params\":current_state.params,\n",
    "                                                      \"batch_stats\": old_batch_stats},\n",
    "                                                     _x_a,\n",
    "                                                     mutable=[\"batch_stats\"])\n",
    "        \n",
    "        new_batch_stats = mutated_vars[\"batch_stats\"]\n",
    "        return new_batch_stats, None\n",
    "\n",
    "    batch_stats = dict(batch_stats)\n",
    "    print(type(batch_stats))\n",
    "    batch_stats, _ = lax.scan(f, batch_stats, x_a)\n",
    "    return batch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24a6bfef-9868-4c45-aece-3f4514c99f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def grad_applier_identity_cov(current_state, gradients_p, new_batch_stats):\n",
    "    return current_state.apply_gradients(grads_params=gradients_p, new_batch_stats=new_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b951430-4a3a-477f-947c-c0b71b69d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_identity_cov(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p) = pmap(pmapable_loss_identity_cov,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_identity_cov(current_state, current_gradients_p, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad8598-b20a-4235-9a59-2b29010815f5",
   "metadata": {},
   "source": [
    "## Unlimitd f training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c5acfed-43d4-48d9-ab79-44472a07f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntk import get_kernel_and_jac_lowdim_cov\n",
    "\n",
    "def nll_batch_average_lowdim_cov_singGP(current_params, current_scale, apply_fn, current_batch_stats, proj, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_scale, current_batch_stats, proj)\n",
    "\n",
    "    part_apply_fn = (partial(apply_fn, current_params, current_batch_stats))\n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, part_apply_fn, x_a, y_a, maddox_noise, jacobian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72165a52-b07d-4f90-b308-0773e747a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_lowdim_cov_singGP(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_s) = value_and_grad(nll_batch_average_lowdim_cov_singGP, argnums = (0, 1) )(current_state.params,\n",
    "                                                              current_state.scale,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              current_state.proj,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b845a68b-ee0f-473f-bf0f-1b105fa0bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import grad_applier_lowdim_cov_singGP\n",
    "\n",
    "def step_lowdim_cov_singGP(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_s) = pmap(pmapable_loss_lowdim_cov_singGP,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_s = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_s)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_lowdim_cov_singGP(current_state, current_gradients_p, current_gradients_s, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "154087b2-a619-4bc9-8b46-8ebf39ae7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(key, step, n_epochs, state, n_tasks, K, data_noise, maddox_noise, get_train_batch_fn, eval_during_training_fn):\n",
    "    \"\"\"\n",
    "    Available step functions:\n",
    "    * step_identity_cov\n",
    "    * step_lowdim_cov_singGP\n",
    "    * step_lowdim_cov_mixture\n",
    "\n",
    "    Available get_train_batch_fn functions:\n",
    "    * dataset_sines_infinite.get_training_batch\n",
    "    * dataset_sines_finite.get_training_batch\n",
    "    * dataset_lines_infinite.get_training_batch\n",
    "    * dataset_multi_infinite.get_training_batch\n",
    "    * dataset_shapenet1d.get_training_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "    print(\"Starting training with:\")\n",
    "    print(f\"-n_epochs={n_epochs}\")\n",
    "    print(f\"-n_tasks={n_tasks}\")\n",
    "    print(f\"-K={K}\")\n",
    "    print(f\"-data_noise={data_noise}\")\n",
    "    print(f\"-maddox_noise={maddox_noise}\")\n",
    "\n",
    "    losses = []\n",
    "    evals = []\n",
    "    t = time.time_ns()\n",
    "\n",
    "    for epoch_index in range(n_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        state, current_loss = step(subkey, state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn)\n",
    "\n",
    "        if(np.isnan(current_loss)):\n",
    "            print(\"Nan, aborting\")\n",
    "            break\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "\n",
    "        if epoch_index % 10 == 0:\n",
    "            print(f\"{epoch_index}  | {current_loss:.4f} ({(time.time_ns() - t)/ 10**9:.4f} s)\")\n",
    "        t = time.time_ns()\n",
    "\n",
    "        if epoch_index % 500 == 0:\n",
    "            key, subkey = random.split(key)\n",
    "            current_eval = eval_during_training_fn(subkey, state)\n",
    "            evals.append( current_eval )\n",
    "            print(f\"Eval: {current_eval}\")\n",
    "    print(\"Completed training\")\n",
    "\n",
    "    return state, losses, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc73a5c7-6d2d-4e1a-af84-86dccdf1019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable\n",
    "\n",
    "from flax import core\n",
    "from flax import struct\n",
    "from jax import numpy as np\n",
    "import optax\n",
    "\n",
    "#Train state for the identity covariance training (UNLIMTD-I, or the first part of UNLIMTD-F)\n",
    "\n",
    "class TrainStateIdentityCovariance(struct.PyTreeNode):\n",
    "    step: int\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    apply_fn_raw: Callable = struct.field(pytree_node=False)\n",
    "    params: core.FrozenDict[str, Any]\n",
    "    tx_params: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    batch_stats: core.FrozenDict[str, Any]\n",
    "    opt_state_params: optax.OptState\n",
    "    \n",
    "    def apply_gradients(self, *, grads_params, new_batch_stats, **kwargs):\n",
    "        \"\"\"\n",
    "        Updates both the params and the scaling matrix\n",
    "        Also requires new_batch_stats to keep track of what has been seen by the network\n",
    "        \"\"\"\n",
    "\n",
    "        # params part\n",
    "        updates_params, new_opt_state_params = self.tx_params.update(grads_params, self.opt_state_params, self.params)\n",
    "        new_params = optax.apply_updates(self.params, updates_params)\n",
    "\n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state_params=new_opt_state_params,\n",
    "            batch_stats=new_batch_stats,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, apply_fn_raw, params, tx_params, batch_stats, **kwargs):\n",
    "        opt_state_params = tx_params.init(params)\n",
    "        return cls(\n",
    "            step=0,\n",
    "            apply_fn=apply_fn,\n",
    "            apply_fn_raw=apply_fn_raw,\n",
    "            params=params,\n",
    "            tx_params=tx_params,\n",
    "            batch_stats=batch_stats,\n",
    "            opt_state_params=opt_state_params,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "466bcd57-c23a-4237-a150-99dab7505141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainStateLowDimCovSingGP(struct.PyTreeNode):\n",
    "    step: int\n",
    "    apply_fn: Callable = struct.field(pytree_node=False)\n",
    "    apply_fn_raw: Callable = struct.field(pytree_node=False)\n",
    "    params: core.FrozenDict[str, Any]\n",
    "    scale: np.ndarray\n",
    "    tx_params: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    tx_scale: optax.GradientTransformation = struct.field(pytree_node=False)\n",
    "    batch_stats: core.FrozenDict[str, Any]\n",
    "    opt_state_params: optax.OptState\n",
    "    opt_state_scale: optax.OptState\n",
    "    proj: np.ndarray\n",
    "    \n",
    "    def apply_gradients(self, *, grads_params, grads_scale, new_batch_stats, **kwargs):\n",
    "        \"\"\"\n",
    "        Updates both the params and the scaling matrix\n",
    "        Also requires new_batch_stats to keep track of what has been seen by the network\n",
    "        \"\"\"\n",
    "\n",
    "        # params part\n",
    "        updates_params, new_opt_state_params = self.tx_params.update(grads_params, self.opt_state_params, self.params)\n",
    "        new_params = optax.apply_updates(self.params, updates_params)\n",
    "\n",
    "        # scaling matrix part\n",
    "        updates_scale, new_opt_state_scale = self.tx_scale.update(grads_scale, self.opt_state_scale, self.scale)\n",
    "        new_scale = optax.apply_updates(self.scale, updates_scale)\n",
    "\n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            scale=new_scale,\n",
    "            opt_state_params=new_opt_state_params,\n",
    "            opt_state_scale=new_opt_state_scale,\n",
    "            batch_stats=new_batch_stats,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, apply_fn, apply_fn_raw, params, scale, tx_params, tx_scale, proj, batch_stats, **kwargs):\n",
    "        opt_state_params = tx_params.init(params)\n",
    "        opt_state_scale = tx_scale.init(scale)\n",
    "        return cls(\n",
    "            step=0,\n",
    "            apply_fn=apply_fn,\n",
    "            apply_fn_raw=apply_fn_raw,\n",
    "            params=params,\n",
    "            scale=scale,\n",
    "            tx_params=tx_params,\n",
    "            tx_scale=tx_scale,\n",
    "            batch_stats=batch_stats,\n",
    "            opt_state_params=opt_state_params,\n",
    "            opt_state_scale=opt_state_scale,\n",
    "            proj=proj,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ee0b3b9-4db5-4c4b-b438-8402aab5d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nll_one_kernel(key, kernel_self, jacobian, get_test_batch_fn, K, n_tasks, data_noise, maddox_noise):\n",
    "    \"\"\"\n",
    "    Returns the NLLs for n_tasks random tasks, in the singGP case.\n",
    "    \"\"\"\n",
    "    x_a, y_a, _, _ = get_test_batch_fn(key, n_tasks, K, 0, data_noise)\n",
    "    all_nlls = nll_batch_one_kernel(kernel_self, x_a, y_a, maddox_noise, jacobian)\n",
    "\n",
    "    return all_nlls\n",
    "\n",
    "def test_error_one_kernel(key, kernel, kernel_self, jacobian, get_test_batch_fn, error_fn, K, L, n_tasks, data_noise, maddox_noise):\n",
    "    \"\"\"\n",
    "    Returns the error for n_tasks random tasks, in the singGP case.\n",
    "    \"\"\"\n",
    "    x_a, y_a, x_b, y_b = get_test_batch_fn(key, n_tasks, K, L, data_noise)\n",
    "\n",
    "    def f(carry, task_data):\n",
    "        _x_a, _y_a, _x_b, _y_b = task_data\n",
    "        predictions = nll.gaussian_posterior(kernel, kernel_self, _x_a, _y_a, _x_b, maddox_noise)\n",
    "        return None, error_fn(predictions, _y_b)\n",
    "    \n",
    "    _, all_errors = lax.scan(f, None, (x_a, y_a, x_b, y_b))\n",
    "\n",
    "    return all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa26b2e5-e61e-45e2-bc15-75c589e8a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer\n",
    "import ntk\n",
    "import test\n",
    "import train_states\n",
    "import models\n",
    "import utils\n",
    "import fim\n",
    "\n",
    "import dataset_sines_infinite\n",
    "import dataset_sines_finite\n",
    "import dataset_multi_infinite\n",
    "\n",
    "from jax import random\n",
    "from jax import numpy as np\n",
    "from flax.core import FrozenDict\n",
    "import optax\n",
    "\n",
    "from ntk import get_kernel_and_jac_identity_cov\n",
    "from ntk import get_kernel_and_jac_lowdim_cov\n",
    "\n",
    "def unlimtd_f_uni_modal_infinite(seed, pre_n_epochs, pre_n_tasks, pre_K, post_n_epochs, post_n_tasks, post_K, data_noise, maddox_noise, meta_lr, subspace_dimension):\n",
    "    key = random.PRNGKey(seed)\n",
    "    key_init, key = random.split(key)\n",
    "    \n",
    "    print(\"===============\")\n",
    "    print(\"This is UNLIMTD-F\")\n",
    "    print(\"For the uni-modal dataset: infinite sine dataset\")\n",
    "    print(\"This variant of UNLIMTD-F approaches the distribution with a single GP\")\n",
    "    print(\"===============\")\n",
    "    print(\"Creating model\")\n",
    "    model = models.small_network(40, \"relu\", 1)\n",
    "    batch = random.uniform(key_init, shape=(5,1), minval=-5, maxval=5)\n",
    "    init_vars = model.init(key_init, batch)\n",
    "    apply_fn = utils.apply_fn_wrapper(model.apply, True)\n",
    "    apply_fn_raw = model.apply\n",
    "\n",
    "    # Training before finding the FIM matrix\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_identity_cov\n",
    "    get_train_batch_fn = dataset_sines_infinite.get_training_batch\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    mean_init = np.zeros( (utils.get_param_size(init_vars[\"params\"]),) )\n",
    "\n",
    "    pre_state = TrainStateIdentityCovariance.create(apply_fn=apply_fn, apply_fn_raw=apply_fn_raw, params=init_vars[\"params\"], tx_params=optimizer_params, batch_stats=FrozenDict())\n",
    "\n",
    "    def eval_during_pre_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_batch_stats = state.batch_stats\n",
    "        kernel, kernel_self, jacobian = ntk.get_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        nlls = test_nll_one_kernel(subkey_1, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "        mses = test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, dataset_sines_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting first part of training (identity covariance)\")\n",
    "    key_pre, key = random.split(key)\n",
    "    pre_state, pre_losses, pre_evals = train_and_eval(key_pre, step, pre_n_epochs, pre_state, pre_n_tasks, pre_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_pre_training)\n",
    "    print(\"Finished first part of training\")\n",
    "\n",
    "    # FIM\n",
    "    print(\"Finding projection matrix\")\n",
    "    key_fim, key_data, key = random.split(key, 3)\n",
    "    # here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "    # P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "    P1 = fim.proj_sketch(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, batch_stats=pre_state.batch_stats, batches=random.uniform(key_data, shape=(100, 1761, 1), minval=-5, maxval=5), subspace_dimension=subspace_dimension)\n",
    "    print(\"Found projection matrix\")\n",
    "\n",
    "    # Usual training with projection\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_lowdim_cov_singGP\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_scale = optax.adam(learning_rate = meta_lr)\n",
    "    init_scale = np.ones( (subspace_dimension,) )\n",
    "\n",
    "    post_state = TrainStateLowDimCovSingGP.create(apply_fn = apply_fn, apply_fn_raw=apply_fn_raw, params = pre_state.params, scale=init_scale, tx_params = optimizer_params, tx_scale = optimizer_scale, batch_stats=pre_state.batch_stats, proj = P1)\n",
    "\n",
    "    def eval_during_post_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_batch_stats = state.batch_stats\n",
    "        current_scale = state.scale\n",
    "        kernel, kernel_self, jacobian = ntk.get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_scale, current_batch_stats, P1)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        nlls = test_nll_one_kernel(subkey_1, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "        mses = test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, dataset_sines_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    key_post, key = random.split(key)\n",
    "    post_state, post_losses, post_evals = train_and_eval(key_post, step, post_n_epochs, post_state, post_n_tasks, post_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_post_training)\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # Returning everything\n",
    "    return init_vars, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "24763136-051f-4eb5-aa90-7d962166d704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "This is UNLIMTD-F\n",
      "For the uni-modal dataset: infinite sine dataset\n",
      "This variant of UNLIMTD-F approaches the distribution with a single GP\n",
      "===============\n",
      "Creating model\n",
      "Creating optimizers\n",
      "Starting first part of training (identity covariance)\n",
      "Starting training with:\n",
      "-n_epochs=1000\n",
      "-n_tasks=24\n",
      "-K=10\n",
      "-data_noise=0.05\n",
      "-maddox_noise=0.05\n",
      "<class 'dict'>\n",
      "0  | 1452.8583 (1.2538 s)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "nll_batch_one_kernel() missing 1 required positional argument: 'jacobian'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m init_params, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals \u001b[38;5;241m=\u001b[39m \u001b[43munlimtd_f_uni_modal_infinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpre_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpre_n_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpre_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpost_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpost_n_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mpost_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mdata_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mmeta_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43msubspace_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[69], line 59\u001b[0m, in \u001b[0;36munlimtd_f_uni_modal_infinite\u001b[0;34m(seed, pre_n_epochs, pre_n_tasks, pre_K, post_n_epochs, post_n_tasks, post_K, data_noise, maddox_noise, meta_lr, subspace_dimension)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting first part of training (identity covariance)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m key_pre, key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 59\u001b[0m pre_state, pre_losses, pre_evals \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_n_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_n_tasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_train_batch_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_during_pre_training\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished first part of training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# FIM\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[53], line 45\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(key, step, n_epochs, state, n_tasks, K, data_noise, maddox_noise, get_train_batch_fn, eval_during_training_fn)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch_index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m     key, subkey \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 45\u001b[0m     current_eval \u001b[38;5;241m=\u001b[39m \u001b[43meval_during_training_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     evals\u001b[38;5;241m.\u001b[39mappend( current_eval )\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_eval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 52\u001b[0m, in \u001b[0;36munlimtd_f_uni_modal_infinite.<locals>.eval_during_pre_training\u001b[0;34m(key, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m kernel, kernel_self, jacobian \u001b[38;5;241m=\u001b[39m ntk\u001b[38;5;241m.\u001b[39mget_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n\u001b[1;32m     51\u001b[0m subkey_1, subkey_2 \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 52\u001b[0m nlls \u001b[38;5;241m=\u001b[39m \u001b[43mtest_nll_one_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjacobian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_sines_infinite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_test_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m mses \u001b[38;5;241m=\u001b[39m test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_sines_infinite\u001b[38;5;241m.\u001b[39mget_test_batch, dataset_sines_infinite\u001b[38;5;241m.\u001b[39merror_fn, K\u001b[38;5;241m=\u001b[39mpre_K, L\u001b[38;5;241m=\u001b[39mpre_K, n_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, data_noise\u001b[38;5;241m=\u001b[39mdata_noise, maddox_noise\u001b[38;5;241m=\u001b[39mmaddox_noise)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(nlls), np\u001b[38;5;241m.\u001b[39mmean(mses)\n",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m, in \u001b[0;36mtest_nll_one_kernel\u001b[0;34m(key, kernel_self, jacobian, get_test_batch_fn, K, n_tasks, data_noise, maddox_noise)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mReturns the NLLs for n_tasks random tasks, in the singGP case.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x_a, y_a, _, _ \u001b[38;5;241m=\u001b[39m get_test_batch_fn(key, n_tasks, K, \u001b[38;5;241m0\u001b[39m, data_noise)\n\u001b[0;32m----> 6\u001b[0m all_nlls \u001b[38;5;241m=\u001b[39m \u001b[43mnll_batch_one_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaddox_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjacobian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_nlls\n",
      "\u001b[0;31mTypeError\u001b[0m: nll_batch_one_kernel() missing 1 required positional argument: 'jacobian'"
     ]
    }
   ],
   "source": [
    "init_params, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals = unlimtd_f_uni_modal_infinite(seed=seed,\n",
    "                                                                                     pre_n_epochs=1000,\n",
    "                                                                                     pre_n_tasks=24,\n",
    "                                                                                     pre_K=10,\n",
    "                                                                                     post_n_epochs=1000,\n",
    "                                                                                     post_n_tasks=24,\n",
    "                                                                                     post_K=10,\n",
    "                                                                                     data_noise=0.05, \n",
    "                                                                                     maddox_noise=0.05,\n",
    "                                                                                     meta_lr=0.001,\n",
    "                                                                                     subspace_dimension=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2e0d5-c4c9-49eb-9c10-9b734e15a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output[\"seed\"] = seed\n",
    "output[\"pre_n_epochs\"]=30000\n",
    "output[\"pre_n_tasks\"]=24\n",
    "output[\"pre_K\"]=10\n",
    "output[\"post_n_epochs\"]=30000\n",
    "output[\"post_n_tasks\"]=24\n",
    "output[\"post_K\"]=10\n",
    "output[\"data_noise\"]=0.05\n",
    "output[\"maddox_noise\"]=0.05\n",
    "output[\"meta_lr\"]=0.001\n",
    "output[\"subspace_dimension\"]=10\n",
    "output[\"pre_losses\"]=pre_losses\n",
    "output[\"post_losses\"]=post_losses\n",
    "output[\"init_params\"]=init_params\n",
    "output[\"intermediate_params\"]=pre_state.params\n",
    "output[\"trained_params\"]=post_state.params\n",
    "output[\"intermediate_mean\"]=pre_state.mean\n",
    "output[\"trained_mean\"]=post_state.mean\n",
    "output[\"intermediate_batch_stats\"]=pre_state.batch_stats\n",
    "output[\"trained_batch_stats\"]=post_state.batch_stats\n",
    "output[\"trained_scale\"]=post_state.scale\n",
    "output[\"proj\"]=post_state.proj\n",
    "output[\"pre_evals\"]=pre_evals\n",
    "output[\"post_evals\"]=post_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2feddb-f127-4fa6-87c8-39c291866f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_zeroth_order.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b512739-7cb9-4ada-b6e3-b6dfbdd81c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_zeroth_order.pickle\", \"rb\") as handle:\n",
    "    output = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d670f-320d-4d4f-b4e8-c488e2396979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.small_network(40, \"relu\", 1)\n",
    "apply_fn = utils.apply_fn_wrapper(model.apply, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3980a3-ffcf-4e75-865c-3bea596accc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel, kernel_self, jac = ntk.get_kernel_and_jac_lowdim_cov(apply_fn, output[\"trained_params\"], output[\"trained_scale\"], output[\"trained_batch_stats\"], output[\"proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e894-9ccf-4674-a9e1-a93981b51b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nll import gaussian_posterior_full\n",
    "\n",
    "def plot_notebooks(key, part_apply_fn, kernel, kernel_self, jac, mean, K, dataset_provider):\n",
    "    \"\"\"\n",
    "    Make an informative prediction plot in the singGP case (for the kernel specified)\n",
    "    K is the number of context inputs\n",
    "    Change dataset_provider to test on other datasets (e.g. dataset_sines_infinite)\n",
    "    \"\"\"\n",
    "    x, y, fun = dataset_provider.get_fancy_test_batch(key, K=10, L=0, data_noise=0.05)\n",
    "\n",
    "    x_a_all = x[0, :10]\n",
    "    y_a_all = y[0, :10]\n",
    "    x_b = np.linspace(-5, 5, 100)[:, np.newaxis]\n",
    "    y_b = fun(x_b)\n",
    "\n",
    "    y_min, y_max = np.min(y_b) - 0.5, np.max(y_b) + 0.5\n",
    "\n",
    "    correction_a_all = utils.falseaffine_correction0(jac, mean, x_a_all)\n",
    "    correction_b = utils.falseaffine_correction0(jac, mean, x_b)\n",
    "\n",
    "    x_a = x_a_all[:K]\n",
    "    y_a = y_a_all[:K]\n",
    "    correction_a = correction_a_all[:K]\n",
    "\n",
    "    prediction, cov = gaussian_posterior_full(kernel, kernel_self, x_a, y_a - part_apply_fn(x_a) - correction_a, x_b, 0.05)\n",
    "    prediction = prediction + part_apply_fn(x_b) + correction_b\n",
    "\n",
    "    error = dataset_provider.error_fn(prediction, y_b)\n",
    "    loss = nll(kernel_self, part_apply_fn, x_a, y_a - correction_a, maddox_noise=0.05)\n",
    "\n",
    "    variances = np.diag(cov)\n",
    "    stds = np.sqrt(variances)\n",
    "\n",
    "    plt.plot(x_b, y_b, \"g--\", label=\"Target\")\n",
    "    plt.plot(x_b, part_apply_fn(x_b), \"k--\", label=\"apply_fn\")\n",
    "    plt.plot(x_b, correction_b, \"p--\", label=\"correction_b\")\n",
    "    plt.plot(x_a, y_a, \"ro\", label=\"Context data\")\n",
    "    plt.plot(x_b, prediction, \"b\", label=\"Prediction\")\n",
    "    plt.fill_between(x_b[:, 0], prediction[:, 0] - 1.96 * stds, prediction[:, 0] + 1.96 * stds, color='blue', alpha=0.1, label=\"+/- 1.96$\\sigma$\")\n",
    "    plt.title(f\"NLL={loss:.4f}, MSE={error:.4f} ($K$={K})\")\n",
    "    plt.legend()\n",
    "    plt.gca().set_ylim([y_min, y_max])\n",
    "    plt.gca().set_xlabel(\"$x$\")\n",
    "    plt.gca().set_ylabel(\"$y$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a535e-3b83-4637-b609-47e65df82237",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ce2a6-e479-4d27-a69c-2f8e15b25782",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_apply_fn = partial(apply_fn, output[\"trained_params\"], output[\"trained_batch_stats\"])\n",
    "key, subkey = random.split(key)\n",
    "plot_notebooks(subkey, part_apply_fn, kernel, kernel_self, jac, output[\"trained_mean\"], 10, dataset_sines_infinite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9843d7-29df-4e97-9f90-97c4020c62d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709778f-cf48-46da-ac64-52537a230c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
