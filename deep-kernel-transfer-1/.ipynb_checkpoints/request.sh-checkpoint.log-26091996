/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/functional.py:1626: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at ../aten/src/ATen/native/LinearAlgebra.cpp:1079.)
  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]
[0] - Loss: 0.738  MSE: 0.000 noise: 0.693
[0] - Loss: 0.735  MSE: 0.000 noise: 0.692
[0] - Loss: 0.736  MSE: 0.000 noise: 0.692
[0] - Loss: 0.735  MSE: 0.000 noise: 0.691
[0] - Loss: 0.734  MSE: 0.000 noise: 0.691
[0] - Loss: 0.734  MSE: 0.000 noise: 0.690
[0] - Loss: 0.734  MSE: 0.000 noise: 0.690
[0] - Loss: 0.733  MSE: 0.000 noise: 0.689
[0] - Loss: 0.733  MSE: 0.000 noise: 0.689
[0] - Loss: 0.733  MSE: 0.000 noise: 0.688
[0] - Loss: 0.732  MSE: 0.000 noise: 0.688
[0] - Loss: 0.732  MSE: 0.000 noise: 0.687
[0] - Loss: 0.732  MSE: 0.000 noise: 0.687
[0] - Loss: 0.731  MSE: 0.000 noise: 0.686
[0] - Loss: 0.731  MSE: 0.000 noise: 0.686
[0] - Loss: 0.730  MSE: 0.000 noise: 0.685
[0] - Loss: 0.730  MSE: 0.000 noise: 0.685
[0] - Loss: 0.730  MSE: 0.000 noise: 0.684
[0] - Loss: 0.729  MSE: 0.000 noise: 0.684
[0] - Loss: 0.729  MSE: 0.000 noise: 0.683
[0] - Loss: 0.729  MSE: 0.000 noise: 0.683
[0] - Loss: 0.728  MSE: 0.000 noise: 0.682
[0] - Loss: 0.728  MSE: 0.000 noise: 0.682
[0] - Loss: 0.727  MSE: 0.000 noise: 0.681
[10] - Loss: 0.778  MSE: 0.209 noise: 0.585
[10] - Loss: 0.768  MSE: 0.209 noise: 0.584
[10] - Loss: 0.724  MSE: 0.209 noise: 0.584
[10] - Loss: 0.772  MSE: 0.209 noise: 0.583
[10] - Loss: 0.749  MSE: 0.209 noise: 0.583
[10] - Loss: 0.815  MSE: 0.209 noise: 0.583
[10] - Loss: 0.711  MSE: 0.209 noise: 0.582
[10] - Loss: 0.783  MSE: 0.209 noise: 0.582
[10] - Loss: 0.741  MSE: 0.209 noise: 0.581
[10] - Loss: 0.771  MSE: 0.209 noise: 0.581
[10] - Loss: 0.755  MSE: 0.209 noise: 0.580
[10] - Loss: 0.776  MSE: 0.209 noise: 0.580
[10] - Loss: 0.717  MSE: 0.209 noise: 0.580
[10] - Loss: 0.805  MSE: 0.209 noise: 0.579
[10] - Loss: 0.675  MSE: 0.209 noise: 0.579
[10] - Loss: 0.709  MSE: 0.209 noise: 0.578
[10] - Loss: 0.762  MSE: 0.209 noise: 0.578
[10] - Loss: 0.826  MSE: 0.209 noise: 0.577
[10] - Loss: 0.791  MSE: 0.209 noise: 0.577
[10] - Loss: 0.929  MSE: 0.209 noise: 0.576
[10] - Loss: 0.801  MSE: 0.209 noise: 0.576
[10] - Loss: 0.713  MSE: 0.209 noise: 0.576
[10] - Loss: 0.766  MSE: 0.209 noise: 0.575
[10] - Loss: 0.737  MSE: 0.209 noise: 0.575
[20] - Loss: 0.856  MSE: 0.532 noise: 0.487
[20] - Loss: 0.710  MSE: 0.532 noise: 0.487
[20] - Loss: 0.660  MSE: 0.532 noise: 0.487
[20] - Loss: 0.773  MSE: 0.532 noise: 0.486
[20] - Loss: 0.734  MSE: 0.532 noise: 0.486
[20] - Loss: 0.865  MSE: 0.532 noise: 0.486
[20] - Loss: 0.618  MSE: 0.532 noise: 0.485
[20] - Loss: 0.836  MSE: 0.532 noise: 0.485
[20] - Loss: 0.651  MSE: 0.532 noise: 0.485
[20] - Loss: 0.717  MSE: 0.532 noise: 0.485
[20] - Loss: 0.672  MSE: 0.532 noise: 0.484
[20] - Loss: 0.915  MSE: 0.532 noise: 0.484
[20] - Loss: 0.800  MSE: 0.532 noise: 0.484
[20] - Loss: 0.843  MSE: 0.532 noise: 0.483
[20] - Loss: 0.776  MSE: 0.532 noise: 0.483
[20] - Loss: 0.687  MSE: 0.532 noise: 0.483
[20] - Loss: 0.858  MSE: 0.532 noise: 0.482
[20] - Loss: 0.710  MSE: 0.532 noise: 0.482
[20] - Loss: 0.669  MSE: 0.532 noise: 0.482
[20] - Loss: 0.959  MSE: 0.532 noise: 0.481
[20] - Loss: 0.765  MSE: 0.532 noise: 0.481
[20] - Loss: 0.664  MSE: 0.532 noise: 0.481
[20] - Loss: 0.685  MSE: 0.532 noise: 0.481
[20] - Loss: 0.598  MSE: 0.532 noise: 0.480
[30] - Loss: 0.738  MSE: 0.286 noise: 0.397
[30] - Loss: 0.669  MSE: 0.286 noise: 0.396
[30] - Loss: 0.660  MSE: 0.286 noise: 0.396
[30] - Loss: 0.714  MSE: 0.286 noise: 0.395
[30] - Loss: 0.687  MSE: 0.286 noise: 0.395
[30] - Loss: 0.724  MSE: 0.286 noise: 0.395
[30] - Loss: 0.648  MSE: 0.286 noise: 0.394
[30] - Loss: 0.719  MSE: 0.286 noise: 0.394
[30] - Loss: 0.637  MSE: 0.286 noise: 0.394
[30] - Loss: 0.649  MSE: 0.286 noise: 0.394
[30] - Loss: 0.648  MSE: 0.286 noise: 0.393
[30] - Loss: 0.668  MSE: 0.286 noise: 0.393
[30] - Loss: 0.645  MSE: 0.286 noise: 0.393
[30] - Loss: 0.693  MSE: 0.286 noise: 0.392
[30] - Loss: 0.542  MSE: 0.286 noise: 0.392
[30] - Loss: 0.582  MSE: 0.286 noise: 0.392
[30] - Loss: 0.614  MSE: 0.286 noise: 0.392
[30] - Loss: 0.569  MSE: 0.286 noise: 0.391
[30] - Loss: 0.565  MSE: 0.286 noise: 0.391
[30] - Loss: 0.735  MSE: 0.286 noise: 0.391
[30] - Loss: 0.580  MSE: 0.286 noise: 0.390
[30] - Loss: 0.549  MSE: 0.286 noise: 0.390
[30] - Loss: 0.543  MSE: 0.286 noise: 0.390
[30] - Loss: 0.500  MSE: 0.286 noise: 0.389
[40] - Loss: 0.535  MSE: 0.269 noise: 0.326
[40] - Loss: 0.455  MSE: 0.269 noise: 0.326
[40] - Loss: 0.432  MSE: 0.269 noise: 0.325
[40] - Loss: 0.532  MSE: 0.269 noise: 0.325
[40] - Loss: 0.469  MSE: 0.269 noise: 0.325
[40] - Loss: 0.525  MSE: 0.269 noise: 0.324
[40] - Loss: 0.483  MSE: 0.269 noise: 0.324
[40] - Loss: 0.630  MSE: 0.269 noise: 0.324
[40] - Loss: 0.437  MSE: 0.269 noise: 0.324
[40] - Loss: 0.517  MSE: 0.269 noise: 0.323
[40] - Loss: 0.445  MSE: 0.269 noise: 0.323
[40] - Loss: 0.453  MSE: 0.269 noise: 0.323
[40] - Loss: 0.464  MSE: 0.269 noise: 0.322
[40] - Loss: 0.598  MSE: 0.269 noise: 0.322
[40] - Loss: 0.396  MSE: 0.269 noise: 0.322
[40] - Loss: 0.417  MSE: 0.269 noise: 0.322
[40] - Loss: 0.452  MSE: 0.269 noise: 0.321
[40] - Loss: 0.390  MSE: 0.269 noise: 0.321
[40] - Loss: 0.494  MSE: 0.269 noise: 0.321
[40] - Loss: 0.618  MSE: 0.269 noise: 0.320
[40] - Loss: 0.571  MSE: 0.269 noise: 0.320
[40] - Loss: 0.405  MSE: 0.269 noise: 0.320
[40] - Loss: 0.458  MSE: 0.269 noise: 0.320
[40] - Loss: 0.435  MSE: 0.269 noise: 0.319
[50] - Loss: 0.290  MSE: 0.076 noise: 0.260
[50] - Loss: 0.374  MSE: 0.076 noise: 0.260
[50] - Loss: 0.493  MSE: 0.076 noise: 0.260
[50] - Loss: 0.466  MSE: 0.076 noise: 0.260
[50] - Loss: 0.399  MSE: 0.076 noise: 0.260
[50] - Loss: 0.302  MSE: 0.076 noise: 0.259
[50] - Loss: 0.299  MSE: 0.076 noise: 0.259
[50] - Loss: 0.409  MSE: 0.076 noise: 0.259
[50] - Loss: 0.293  MSE: 0.076 noise: 0.259
[50] - Loss: 0.456  MSE: 0.076 noise: 0.259
[50] - Loss: 0.337  MSE: 0.076 noise: 0.258
[50] - Loss: 0.321  MSE: 0.076 noise: 0.258
[50] - Loss: 0.318  MSE: 0.076 noise: 0.258
[50] - Loss: 0.310  MSE: 0.076 noise: 0.258
[50] - Loss: 0.280  MSE: 0.076 noise: 0.258
[50] - Loss: 0.297  MSE: 0.076 noise: 0.257
[50] - Loss: 0.381  MSE: 0.076 noise: 0.257
[50] - Loss: 0.277  MSE: 0.076 noise: 0.257
[50] - Loss: 0.328  MSE: 0.076 noise: 0.257
[50] - Loss: 0.317  MSE: 0.076 noise: 0.256
[50] - Loss: 0.306  MSE: 0.076 noise: 0.256
[50] - Loss: 0.341  MSE: 0.076 noise: 0.256
[50] - Loss: 0.294  MSE: 0.076 noise: 0.256
[50] - Loss: 0.279  MSE: 0.076 noise: 0.255
[60] - Loss: 0.647  MSE: 0.497 noise: 0.211
[60] - Loss: 0.420  MSE: 0.497 noise: 0.211
[60] - Loss: 0.421  MSE: 0.497 noise: 0.211
[60] - Loss: 0.497  MSE: 0.497 noise: 0.211
[60] - Loss: 0.570  MSE: 0.497 noise: 0.210
[60] - Loss: 0.425  MSE: 0.497 noise: 0.210
[60] - Loss: 0.245  MSE: 0.497 noise: 0.210
[60] - Loss: 0.452  MSE: 0.497 noise: 0.210
[60] - Loss: 0.258  MSE: 0.497 noise: 0.210
[60] - Loss: 0.293  MSE: 0.497 noise: 0.210
[60] - Loss: 0.237  MSE: 0.497 noise: 0.210
[60] - Loss: 0.410  MSE: 0.497 noise: 0.209
[60] - Loss: 0.222  MSE: 0.497 noise: 0.209
[60] - Loss: 0.521  MSE: 0.497 noise: 0.209
[60] - Loss: 0.213  MSE: 0.497 noise: 0.209
[60] - Loss: 0.286  MSE: 0.497 noise: 0.209
[60] - Loss: 0.398  MSE: 0.497 noise: 0.209
[60] - Loss: 0.554  MSE: 0.497 noise: 0.208
[60] - Loss: 0.428  MSE: 0.497 noise: 0.208
[60] - Loss: 0.344  MSE: 0.497 noise: 0.208
[60] - Loss: 0.419  MSE: 0.497 noise: 0.208
[60] - Loss: 0.353  MSE: 0.497 noise: 0.208
[60] - Loss: 0.319  MSE: 0.497 noise: 0.208
[60] - Loss: 0.464  MSE: 0.497 noise: 0.208
[70] - Loss: 0.110  MSE: 0.000 noise: 0.170
[70] - Loss: 0.145  MSE: 0.000 noise: 0.169
[70] - Loss: 0.139  MSE: 0.000 noise: 0.169
[70] - Loss: 0.123  MSE: 0.000 noise: 0.169
[70] - Loss: 0.095  MSE: 0.000 noise: 0.169
[70] - Loss: 0.102  MSE: 0.000 noise: 0.169
[70] - Loss: 0.115  MSE: 0.000 noise: 0.169
[70] - Loss: 0.110  MSE: 0.000 noise: 0.168
[70] - Loss: 0.057  MSE: 0.000 noise: 0.168
[70] - Loss: 0.109  MSE: 0.000 noise: 0.168
[70] - Loss: 0.109  MSE: 0.000 noise: 0.168
[70] - Loss: 0.077  MSE: 0.000 noise: 0.168
[70] - Loss: 0.099  MSE: 0.000 noise: 0.168
[70] - Loss: 0.040  MSE: 0.000 noise: 0.167
[70] - Loss: 0.058  MSE: 0.000 noise: 0.167
[70] - Loss: 0.063  MSE: 0.000 noise: 0.167
[70] - Loss: 0.053  MSE: 0.000 noise: 0.167
[70] - Loss: 0.069  MSE: 0.000 noise: 0.167
[70] - Loss: 0.117  MSE: 0.000 noise: 0.166
[70] - Loss: 0.073  MSE: 0.000 noise: 0.166
[70] - Loss: 0.068  MSE: 0.000 noise: 0.166
[70] - Loss: 0.044  MSE: 0.000 noise: 0.166
[70] - Loss: 0.046  MSE: 0.000 noise: 0.166
[70] - Loss: 0.087  MSE: 0.000 noise: 0.166
[80] - Loss: 0.089  MSE: 0.164 noise: 0.136
[80] - Loss: 0.120  MSE: 0.164 noise: 0.136
[80] - Loss: 0.030  MSE: 0.164 noise: 0.135
[80] - Loss: 0.022  MSE: 0.164 noise: 0.135
[80] - Loss: 0.149  MSE: 0.164 noise: 0.135
[80] - Loss: 0.051  MSE: 0.164 noise: 0.135
[80] - Loss: -0.002  MSE: 0.164 noise: 0.135
[80] - Loss: 0.077  MSE: 0.164 noise: 0.135
[80] - Loss: -0.046  MSE: 0.164 noise: 0.135
[80] - Loss: 0.141  MSE: 0.164 noise: 0.135
[80] - Loss: 0.072  MSE: 0.164 noise: 0.135
[80] - Loss: 0.029  MSE: 0.164 noise: 0.134
[80] - Loss: 0.017  MSE: 0.164 noise: 0.134
[80] - Loss: 0.083  MSE: 0.164 noise: 0.134
[80] - Loss: -0.001  MSE: 0.164 noise: 0.134
[80] - Loss: 0.004  MSE: 0.164 noise: 0.134
[80] - Loss: -0.019  MSE: 0.164 noise: 0.134
[80] - Loss: -0.051  MSE: 0.164 noise: 0.134
[80] - Loss: -0.020  MSE: 0.164 noise: 0.134
[80] - Loss: 0.013  MSE: 0.164 noise: 0.133
[80] - Loss: -0.026  MSE: 0.164 noise: 0.133
[80] - Loss: -0.018  MSE: 0.164 noise: 0.133
[80] - Loss: 0.010  MSE: 0.164 noise: 0.133
[80] - Loss: -0.018  MSE: 0.164 noise: 0.133
[90] - Loss: -0.052  MSE: 0.059 noise: 0.106
[90] - Loss: -0.015  MSE: 0.059 noise: 0.106
[90] - Loss: -0.074  MSE: 0.059 noise: 0.106
[90] - Loss: -0.125  MSE: 0.059 noise: 0.106
[90] - Loss: 0.032  MSE: 0.059 noise: 0.105
[90] - Loss: -0.113  MSE: 0.059 noise: 0.105
[90] - Loss: -0.124  MSE: 0.059 noise: 0.105
[90] - Loss: -0.102  MSE: 0.059 noise: 0.105
[90] - Loss: -0.162  MSE: 0.059 noise: 0.105
[90] - Loss: -0.119  MSE: 0.059 noise: 0.105
[90] - Loss: -0.103  MSE: 0.059 noise: 0.105
[90] - Loss: -0.136  MSE: 0.059 noise: 0.105
[90] - Loss: -0.027  MSE: 0.059 noise: 0.105
[90] - Loss: -0.101  MSE: 0.059 noise: 0.105
[90] - Loss: -0.149  MSE: 0.059 noise: 0.105
[90] - Loss: -0.128  MSE: 0.059 noise: 0.104
[90] - Loss: -0.080  MSE: 0.059 noise: 0.104
[90] - Loss: -0.145  MSE: 0.059 noise: 0.104
[90] - Loss: -0.135  MSE: 0.059 noise: 0.104
[90] - Loss: -0.093  MSE: 0.059 noise: 0.104
[90] - Loss: -0.140  MSE: 0.059 noise: 0.104
[90] - Loss: -0.121  MSE: 0.059 noise: 0.104
[90] - Loss: -0.124  MSE: 0.059 noise: 0.104
[90] - Loss: -0.158  MSE: 0.059 noise: 0.104
/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/functional.py:1626: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at ../aten/src/ATen/native/LinearAlgebra.cpp:1079.)
  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]
-------------------
Average MSE: 0.14383364645764232 +- 0.10886903675801045
-------------------
