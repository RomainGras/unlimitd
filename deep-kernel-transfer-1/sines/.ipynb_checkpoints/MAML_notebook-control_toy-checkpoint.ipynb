{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c68cc5-d418-4d8a-a549-72b625e224f0",
   "metadata": {},
   "source": [
    "# MAML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70915dc1-7afe-4c87-8236-c7be19fd151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "class Control_toy_task():\n",
    "    \"\"\"\n",
    "    A control theory inspired system with interfaces designed for MAML.\n",
    "    Given a 2*2 matrix A, randomly samples a 2d vector X in a squared-space.\n",
    "    The task is given this sequence, to predict the next state X_1 = A X\n",
    "    \"\"\"\n",
    "    def __init__(self, A, xmin, xmax):  #xmin and xmax are the higher and lower bounds of a square in 2d from which the first state will be sample uniformally\n",
    "        self.A = A\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "\n",
    "    def true_label(self, X):\n",
    "        \"\"\"\n",
    "        Compute the true function on the given x.\n",
    "        \"\"\"\n",
    "        return self.A @ X\n",
    "\n",
    "    def sample_data(self, size=1, noise=0.0, gpu=False):\n",
    "        \"\"\"\n",
    "        Sample data from this task.\n",
    "\n",
    "        returns:\n",
    "            x: the feature vector of length size\n",
    "            y: the target vector of length size\n",
    "        \"\"\"\n",
    "        x1 = np.random.uniform(self.xmin, self.xmax, size)\n",
    "        x1 = torch.tensor(x1, dtype=torch.float).unsqueeze(1)\n",
    "        x2 = np.random.uniform(self.xmin, self.xmax, size)\n",
    "        x2 = torch.tensor(x2, dtype=torch.float).unsqueeze(1)\n",
    "        X = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        Y = self.A @ X.T\n",
    "        Y = Y.T\n",
    "        if(noise>0): Y += np.random.normal(loc=0.0, scale=noise, size=(Y.size(0), Y.size(1))).astype(np.float32)\n",
    "        \n",
    "        if(gpu): return X.cuda(), X.cuda()\n",
    "        else: return X, Y\n",
    "\n",
    "\n",
    "class Task_Distribution():\n",
    "    \"\"\"\n",
    "    The task distribution for sine regression tasks for MAML\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_min, x_max, args):\n",
    "        self.args = args\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "    def sample_task(self):\n",
    "        \"\"\"\n",
    "        Sample from the task distribution.\n",
    "\n",
    "        returns:\n",
    "            Sine_Task object\n",
    "        \"\"\"\n",
    "        rates = np.random.uniform(self.args[\"min_rate_val\"], self.args[\"max_rate_val\"], size=(2,))\n",
    "        Lambda = np.diag(np.exp(rates))\n",
    "\n",
    "        random_matrix = np.random.randn(2, 2)\n",
    "        Q, _ = np.linalg.qr(random_matrix)\n",
    "\n",
    "        A = Q @ Lambda @ Q.T\n",
    "\n",
    "        A = torch.tensor(A, dtype=torch.float)\n",
    "        return Control_toy_task(A, self.x_min, self.x_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0dc3c9-7c99-4fd6-b97d-8af6aee15005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAMLModel, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('l1', nn.Linear(2,40)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('l2', nn.Linear(40,40)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('l3', nn.Linear(40,2))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def parameterised(self, x, weights):\n",
    "        # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "        # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "        x = nn.functional.linear(x, weights[0], weights[1])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[2], weights[3])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[4], weights[5])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d033495-33c2-4611-b600-7cd0e25614c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML():\n",
    "    def __init__(self, model, tasks, inner_lr, meta_lr, K=10, inner_steps=1, tasks_per_meta_batch=1000):\n",
    "        \n",
    "        # important objects\n",
    "        self.tasks = tasks\n",
    "        self.model = model\n",
    "        self.weights = list(model.parameters()) # the maml weights we will be meta-optimising\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.meta_optimiser = torch.optim.Adam(self.weights, meta_lr)\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.K = K\n",
    "        self.inner_steps = inner_steps # with the current design of MAML, >1 is unlikely to work well \n",
    "        self.tasks_per_meta_batch = tasks_per_meta_batch \n",
    "        \n",
    "        # metrics\n",
    "        self.plot_every = 10\n",
    "        self.print_every = 100\n",
    "        self.meta_losses = []\n",
    "    \n",
    "    def inner_loop(self, task):\n",
    "        # reset inner model to current maml weights\n",
    "        temp_weights = [w.clone() for w in self.weights]\n",
    "        \n",
    "        # perform training on data sampled from task\n",
    "        X, y = task.sample_data(self.K, noise=0.1)\n",
    "        for step in range(self.inner_steps):\n",
    "            loss = self.criterion(self.model.parameterised(X, temp_weights), y) / self.K\n",
    "            \n",
    "            # compute grad and update inner loop weights\n",
    "            grad = torch.autograd.grad(loss, temp_weights)\n",
    "            temp_weights = [w - self.inner_lr * g for w, g in zip(temp_weights, grad)]\n",
    "        \n",
    "        # sample new data for meta-update and compute loss\n",
    "        X, y = task.sample_data(self.K, noise=0.1)\n",
    "        loss = self.criterion(self.model.parameterised(X, temp_weights), y) / self.K\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def main_loop(self, num_iterations):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for iteration in range(1, num_iterations+1):\n",
    "            \n",
    "            # compute meta loss\n",
    "            meta_loss = 0\n",
    "            for i in range(self.tasks_per_meta_batch):\n",
    "                task = self.tasks.sample_task()\n",
    "                meta_loss += self.inner_loop(task)\n",
    "            \n",
    "            # compute meta gradient of loss with respect to maml weights\n",
    "            meta_grads = torch.autograd.grad(meta_loss, self.weights)\n",
    "            \n",
    "            # assign meta gradient to weights and take optimisation step\n",
    "            for w, g in zip(self.weights, meta_grads):\n",
    "                w.grad = g\n",
    "            self.meta_optimiser.step()\n",
    "            \n",
    "            # log metrics\n",
    "            epoch_loss += meta_loss.item() / self.tasks_per_meta_batch\n",
    "            \n",
    "            if iteration % self.print_every == 0:\n",
    "                print(\"{}/{}. loss: {}\".format(iteration, num_iterations, epoch_loss / self.plot_every))\n",
    "            \n",
    "            if iteration % self.plot_every == 0:\n",
    "                self.meta_losses.append(epoch_loss / self.plot_every)\n",
    "                epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0954a74d-4585-4d7b-8c96-f0cfde9dbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_on_random_task(initial_model, K, num_steps, tasks, optim=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    trains the model on a random sine task and measures the loss curve.\n",
    "    \n",
    "    for each n in num_steps_measured, records the model function after n gradient updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # copy MAML model into a new object to preserve MAML weights during training\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "        ('l1', nn.Linear(1,40)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('l2', nn.Linear(40,40)),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('l3', nn.Linear(40,1))\n",
    "    ]))\n",
    "    model.load_state_dict(initial_model.state_dict())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim(model.parameters(), 0.01)\n",
    "\n",
    "    # train model on a random task\n",
    "    task = tasks.sample_task()\n",
    "    X, y = task.sample_data(200, noise=0.1, sort=True)    \n",
    "    indices = np.arange(200)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:K])\n",
    "    query_indices = np.sort(indices[K:])\n",
    "    X_support = X[support_indices]\n",
    "    y_support = y[support_indices]\n",
    "    X_query = X[query_indices]\n",
    "    y_query = y[query_indices]\n",
    "        \n",
    "    for step in range(1, num_steps+1):\n",
    "        loss = criterion(model(X_support), y_support) / K\n",
    "        # compute grad and update inner loop weights\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    #Evaluate on query set\n",
    "    loss = criterion(model(X_query), y_query[:,None])       \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7679e5f5-d02e-457f-aa15-dc1f8b36cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_losses(initial_model, n_samples, tasks, K=10, n_steps=10, optim=torch.optim.SGD):\n",
    "    \"\"\"\n",
    "    returns the average learning trajectory of the model trained for ``n_iterations`` over ``n_samples`` tasks\n",
    "    \"\"\"\n",
    "\n",
    "    #x = np.linspace(-5, 5, 2) # dummy input for test_on_new_task\n",
    "    avg_losses = list()\n",
    "    for i in range(n_samples):\n",
    "        loss = loss_on_random_task(initial_model, K, n_steps, tasks, optim)\n",
    "        avg_losses.append(loss.item())    \n",
    "    return avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba005ab4-bbae-4e7d-9169-6dbb9dbfd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_functions_at_training(initial_model, X, y, sampled_steps, x_axis, optim=torch.optim.SGD, lr=0.01):\n",
    "    \"\"\"\n",
    "    trains the model on X, y and measures the loss curve.\n",
    "    \n",
    "    for each n in sampled_steps, records model(x_axis) after n gradient updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # copy MAML model into a new object to preserve MAML weights during training\n",
    "    model = nn.Sequential(OrderedDict([\n",
    "        ('l1', nn.Linear(1,40)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('l2', nn.Linear(40,40)),\n",
    "        ('relu2', nn.ReLU()),\n",
    "        ('l3', nn.Linear(40,1))\n",
    "    ]))\n",
    "    model.load_state_dict(initial_model.state_dict())\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim(model.parameters(), lr)\n",
    "\n",
    "    # train model on a random task\n",
    "    num_steps = max(sampled_steps)\n",
    "    K = X.shape[0]\n",
    "    \n",
    "    losses = []\n",
    "    outputs = {}\n",
    "    for step in range(1, num_steps+1):\n",
    "        loss = criterion(model(X), y) / K\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # compute grad and update inner loop weights\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # plot the model function\n",
    "        if step in sampled_steps:\n",
    "            outputs[step] = model(torch.tensor(x_axis, dtype=torch.float).view(-1, 1)).detach().numpy()\n",
    "            \n",
    "    outputs['initial'] = initial_model(torch.tensor(x_axis, dtype=torch.float).view(-1, 1)).detach().numpy()\n",
    "    \n",
    "    return outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25746f92-3ea5-444f-b760-a291a7599ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sampled_performance(initial_model, model_name, task, X, y, test_range, train_range, name, sampled_steps, optim=torch.optim.SGD, lr=0.01):    \n",
    "    x_axis = np.linspace(test_range[0], test_range[1], 1000)\n",
    "    sampled_steps=sampled_steps #[1,10]\n",
    "    outputs, losses = model_functions_at_training(initial_model, \n",
    "                                                  X, y, \n",
    "                                                  sampled_steps=sampled_steps, \n",
    "                                                  x_axis=x_axis, \n",
    "                                                  optim=optim, lr=lr)    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Unpack points into separate lists for plotting\n",
    "    X1, X2 = zip(*X)  # Unpack series a\n",
    "    outputs1, outputs2 = zip(*outputs)\n",
    "    y1, y2 = zip(*y)  # Unpack series b\n",
    "\n",
    "    # Create scatter plots for both series\n",
    "    plt.scatter(X1, X2, color='blue', marker='o', label='Inputs')\n",
    "    plt.scatter(outputs1, outputs2, color='green', marker='x', label='Outputs')\n",
    "    plt.scatter(y1, y2, color='red', marker='x', label='Ground Truths')\n",
    "\n",
    "    # Draw lines between corresponding points\n",
    "    for (xa, ya), (xb, yb) in zip(outputs, y):\n",
    "        plt.plot([xa, xb], [ya, yb], 'gray', linestyle='--')  # Connecting lines\n",
    "\n",
    "    # Add labels and a legend\n",
    "    plt.title('Mapping Points from outputs to ground truths')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    \n",
    "    # step=sampled_steps[0]\n",
    "    # plt.plot(x_axis, outputs[step], color='red', linewidth=2.0,\n",
    "    #              label='model after {} steps'.format(sampled_steps))\n",
    "    # plot losses\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    plt.savefig('plot_regression_maml' + str(name) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e73ccf-326e-45d8-8eb5-d40207aacf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation Parameters\n",
    "train_iterations = 10000\n",
    "inner_steps_test = 50   \n",
    "train_range=(-5.0, 5.0)\n",
    "test_range=(-5.0, 5.0)  # This must be (-5, +10) for the out-of-range condition\n",
    "    \n",
    "## Train phase               \n",
    "#tasks = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "#                      phase_min=0.0, phase_max=np.pi, \n",
    "#                      x_min=train_range[0], x_max=train_range[1], \n",
    "#                      family=\"sine\")\n",
    "args = {\"min_rate_val\":-5, \"max_rate_val\":-0.1}\n",
    "\n",
    "tasks = Task_Distribution(x_min=train_range[0], x_max=train_range[1], \n",
    "                      args=args)\n",
    "\n",
    "maml = MAML(MAMLModel(), tasks, inner_lr=0.01, meta_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c6520-3cb0-444f-abe2-3cacfbb644e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/10000. loss: 0.007495240211486816\n",
      "200/10000. loss: 0.007524238777160644\n",
      "300/10000. loss: 0.007487429618835449\n",
      "400/10000. loss: 0.007508975028991699\n",
      "500/10000. loss: 0.007565425252914429\n",
      "600/10000. loss: 0.007553955221176148\n",
      "700/10000. loss: 0.007470874261856079\n",
      "800/10000. loss: 0.007515465831756591\n",
      "900/10000. loss: 0.0073707178592681884\n",
      "1000/10000. loss: 0.007383165407180786\n",
      "1100/10000. loss: 0.0071429701328277586\n",
      "1200/10000. loss: 0.007098234701156616\n",
      "1300/10000. loss: 0.0068079894065856935\n",
      "1400/10000. loss: 0.006322883176803589\n",
      "1500/10000. loss: 0.005942094612121581\n",
      "1600/10000. loss: 0.0056102831840515125\n",
      "1700/10000. loss: 0.0055192246437072755\n",
      "1800/10000. loss: 0.005518302488327026\n",
      "1900/10000. loss: 0.005425575447082519\n",
      "2000/10000. loss: 0.005389877033233642\n",
      "2100/10000. loss: 0.005376220703124999\n",
      "2200/10000. loss: 0.005444865131378173\n",
      "2300/10000. loss: 0.005391186666488647\n",
      "2400/10000. loss: 0.005454163360595703\n",
      "2500/10000. loss: 0.005400833177566528\n",
      "2600/10000. loss: 0.00540876612663269\n",
      "2700/10000. loss: 0.005269848012924194\n",
      "2800/10000. loss: 0.0053472298622131344\n",
      "2900/10000. loss: 0.005337245559692383\n",
      "3000/10000. loss: 0.005338824892044068\n",
      "3100/10000. loss: 0.005274713945388794\n",
      "3200/10000. loss: 0.005294630289077759\n",
      "3300/10000. loss: 0.005352867603302001\n",
      "3400/10000. loss: 0.005309253072738648\n",
      "3500/10000. loss: 0.0053143408775329585\n",
      "3600/10000. loss: 0.005324803590774536\n",
      "3700/10000. loss: 0.0053519966602325435\n",
      "3800/10000. loss: 0.005192332935333251\n",
      "3900/10000. loss: 0.005136107492446899\n",
      "4000/10000. loss: 0.005214655828475952\n",
      "4100/10000. loss: 0.005043995809555053\n",
      "4200/10000. loss: 0.005059854221343993\n"
     ]
    }
   ],
   "source": [
    "maml.main_loop(num_iterations=train_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a476566-c262-4a45-8742-75151f7b2751",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test phase\n",
    "K = 5\n",
    "print(\"Test, please wait...\")\n",
    "mse_list = average_losses(maml.model.model, n_samples=500, tasks=tasks, K=5, n_steps=inner_steps_test, optim=torch.optim.Adam)\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626692c-07f1-4a7b-8980-a0596b02746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    task = tasks.sample_task()\n",
    "    X, y = task.sample_data(K, noise=0.1)\n",
    "    plot_sampled_performance(maml.model.model, 'MAML', task, X, y, test_range, train_range, name=\"_seed\"+str(i), sampled_steps=list(inner_steps_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413a6be-f7dc-49b1-b60b-163d3006f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = maml.model.model.state_dict()\n",
    "torch.save(model_params, 'maml_params'+str(train_iterations)+'control.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794437c-1a24-45b0-97ae-69b03bba208f",
   "metadata": {},
   "source": [
    "# Feature transfer with UnLiMiTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8664c-4b37-4267-a86b-a83f9c940d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = torch.load('maml_params'+str(train_iterations)+'control.pth')\n",
    "maml.model.model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae68db5-6a6b-468a-99ef-10120e23afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "sns.set()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(device)\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(gpytorch.__version__)\n",
    "\n",
    "sigma = 10  #Hyperparam, std of w, sigma -> 0 implies sticking to the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe865d-78a9-4118-a8a5-551ef715019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        result = sigma * jac1@jac2.T\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47db5b8-5007-41e3-9abc-5dbb182431a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, net):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = NTKernel(net)\n",
    "        #self.covar_module = CosSimNTKernel(net)\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=40)\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = self.feature_extractor(x)\n",
    "        #z_normalized = z - z.min(0)[0]\n",
    "        #z_normalized = 2 * (z_normalized / z_normalized.max(0)[0]) - 1\n",
    "        #x_normalized = x - x.min(0)[0]\n",
    "        #x_normalized = 2 * (x_normalized / x_normalized.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be7021-e683-4fcc-8fb3-af00d28dbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-4))\n",
    "# likelihood.noise = 1e-4\n",
    "n_shot_train = 5\n",
    "n_shot_test = 5\n",
    "\n",
    "dummy_inputs = torch.zeros([n_shot_train,1])\n",
    "dummy_labels = torch.zeros([n_shot_train])\n",
    "gp = ExactGPModel(dummy_inputs, dummy_labels, likelihood, maml.model.model)\n",
    "trainable_params_gp = sum(p.numel() for p in gp.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters of GP : {trainable_params_gp}\")\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a34611-cc8a-4d55-9b38-8f1b92fe32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = ExactGPModel(dummy_inputs, dummy_labels, likelihood, maml.model.model)\n",
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "maml.model.model.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in maml.model.model.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    sample_task = tasks.sample_task()\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - maml.model.model(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(x_all)).mean + maml.model.model(x_all).reshape(-1)\n",
    "    lower, upper = likelihood(gp(x_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "    lower += maml.model.model(x_all).reshape(-1)\n",
    "    upper += maml.model.model(x_all).reshape(-1)    \n",
    "    \n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.plot(np.squeeze(x_all), maml.model.model(x_all).reshape(-1).detach().numpy(), color='black', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    #plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcca515-0d2f-4693-b641-010911598e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "maml.model.model.eval()\n",
    "tot_iterations=50\n",
    "criterion = nn.MSELoss()\n",
    "mse_list = list()\n",
    "\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - maml.model.model(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(x_query)).mean + maml.model.model(x_query).reshape(-1)\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a11be0-aaed-4fcf-9c5f-c5c54339201f",
   "metadata": {},
   "source": [
    "# Sketching FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d07b1b-1672-4db8-9485-e111d1191c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in maml.model.model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d57dd4-055b-41ae-8bc8-8bd27cb62030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(Y, W, Psi):\n",
    "    \"\"\"\n",
    "    given Y = A @ Om, (N, k)\n",
    "    and W = Psi @ A, (l, M)\n",
    "    and Psi(X) = Psi @ X, (N,...) -> (l,...)\n",
    "    where Om and Psi and random sketching operators\n",
    "    returns Q (N x k), X (k x M) such that A ~= QX\n",
    "    \"\"\"\n",
    "    # Perform QR decomposition on Y to get orthonormal basis Q\n",
    "    Q, _ = torch.linalg.qr(Y, mode='reduced')\n",
    "    \n",
    "    # Apply Psi to Q and then perform QR decomposition\n",
    "    U, T = torch.linalg.qr(torch.matmul(Psi, Q), mode='reduced')\n",
    "    \n",
    "    # Solve the triangular system T @ X = U^T @ W for X\n",
    "    # PyTorch does not have a direct equivalent to scipy.linalg.solve_triangular,\n",
    "    # so we use torch.linalg.solve which can handle triangular matrices if specified.\n",
    "    X = torch.linalg.solve(T, torch.matmul(U.T, W))\n",
    "    \n",
    "    return Q, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603eb767-e5c9-42d4-994d-5a4b0ffb5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_low_rank_approx(Y, W, Psi):\n",
    "    \"\"\"\n",
    "    Perform a symmetric low-rank approximation of the matrix A.\n",
    "    \"\"\"\n",
    "    Q, X = low_rank_approx(Y, W, Psi)  # Assuming Psi is now correctly handled\n",
    "    k = Q.shape[-1]  # Dimension of the sketches\n",
    "    \n",
    "    # Concatenate Q and X.T along columns to form a larger matrix\n",
    "    tmp = torch.cat((Q, X.T), dim=1)  # Correctly access the transpose\n",
    "    \n",
    "    # Perform QR decomposition on the concatenated matrix\n",
    "    U, T = torch.linalg.qr(tmp, mode='reduced')\n",
    "    \n",
    "    # Extract T1 and T2 from T\n",
    "    T1 = T[:, :k]\n",
    "    T2 = T[:, k:2*k]\n",
    "    \n",
    "    # Compute symmetric matrix S\n",
    "    S = (T1 @ T2.T + T2 @ T1.T) / 2\n",
    "    \n",
    "    return U, S\n",
    "\n",
    "# Example usage\n",
    "N, k, l = 100, 10, 50  # Example dimensions\n",
    "Y = torch.randn(N, k)  # Random Y matrix\n",
    "W = torch.randn(l, N)  # Random W matrix\n",
    "Psi = torch.randn(l, N)  # Random Psi matrix\n",
    "\n",
    "# Call the function\n",
    "U, S = sym_low_rank_approx(Y, W, Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253372c-92a1-4098-b2c6-2ee47a201a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_rank_eig_approx(Y, W, psi, r):\n",
    "    \"\"\"\n",
    "    Returns U (N x r), D (r) such that A ~= U diag(D) U^T using PyTorch.\n",
    "    \"\"\"\n",
    "    # Obtain symmetric low-rank approximation\n",
    "    U, S = sym_low_rank_approx(Y, W, psi)\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    D, V = torch.linalg.eigh(S)\n",
    "    \n",
    "    # Truncate to keep the top-r eigenvalues and corresponding eigenvectors\n",
    "    D = D[-r:]  # Top r eigenvalues\n",
    "    V = V[:, -r:]  # Corresponding eigenvectors\n",
    "    \n",
    "    # Update U to be U @ V\n",
    "    U = U @ V\n",
    "    \n",
    "    return U, D\n",
    "\n",
    "# Example usage\n",
    "N, k, l, r = 100, 10, 50, 5  # Example dimensions\n",
    "Y = torch.randn(N, k)  # Random Y matrix\n",
    "W = torch.randn(l, N)  # Random W matrix\n",
    "Psi = torch.randn(l, N)  # Random Psi matrix\n",
    "\n",
    "# Call the function\n",
    "U, D = fixed_rank_eig_approx(Y, W, Psi, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64436764-ed25-4042-ab99-ed6e05b048d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch(net, batches, k, l):\n",
    "    \"\"\"\n",
    "    Returns a good rank 2k approximation of the FIM using PyTorch.\n",
    "    \"\"\"\n",
    "    M = batches.size(0)\n",
    "    N_params = sum(p.numel() for p in net.parameters())\n",
    "    print(N_params)\n",
    "\n",
    "    om = torch.randn(k, N_params)\n",
    "    psi = torch.randn(l, N_params)\n",
    "\n",
    "    Y = torch.zeros(N_params, k)\n",
    "    W = torch.zeros(l, N_params)\n",
    "\n",
    "    for batch in batches:\n",
    "        JT = jacobian(net, batch).T\n",
    "        Y += (om @ JT @ JT.T).T / M\n",
    "        W += (psi @ JT @ JT.T) / M\n",
    "\n",
    "    # Compute the rank-2k approximation\n",
    "    U, D = fixed_rank_eig_approx(Y, W, psi, 2 * k)\n",
    "\n",
    "    return U, D\n",
    "\n",
    "def jacobian(net, batch):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian for the batch. This needs to be adapted based on the actual function.\n",
    "    \"\"\"\n",
    "    net.zero_grad()\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "    def fnet_single(params, x):\n",
    "        return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, batch)\n",
    "    jac = jac.values()\n",
    "    # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    reshaped_tensors = [\n",
    "        j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "            .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "            .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "        for j in jac\n",
    "    ]\n",
    "    return torch.cat(reshaped_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f1ade-b4eb-4d22-80d6-af0923119b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def proj_sketch(net, batches, subspace_dimension):\n",
    "    t = time.time_ns()\n",
    "\n",
    "    T = 6 * subspace_dimension + 4 \n",
    "    k = (T - 1) // 3                    # k = 2 * subspace_dimension + 1\n",
    "    l = T - k                           # l = 4 * subspace_dimension + 3\n",
    "\n",
    "    U, D = sketch(net, batches, k, l)\n",
    "    idx = D.argsort(descending=True)\n",
    "    print(\"U shape:\", U.shape)\n",
    "    print(\"Index tensor:\", idx)\n",
    "    print(\"Requested subspace dimension:\", subspace_dimension)\n",
    "    \n",
    "    # Ensure idx is of type long for indexing\n",
    "    # idx = idx.long()\n",
    "\n",
    "    P1 = U[:, idx[:subspace_dimension]].T\n",
    "\n",
    "    print(f\"Done sketching in {(time.time_ns() - t) / 1e9:.4f} s\")\n",
    "\n",
    "    return P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b788011-7415-4b66-8dd8-340ab16ea250",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_dimension = 10\n",
    "\n",
    "print(\"Finding projection matrix\")\n",
    "# here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "# P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "\n",
    "\n",
    "# Generate batches in the range [-5, 5]\n",
    "batch_size = 100\n",
    "input_dimensions = sum(p.numel() for p in maml.model.model.parameters())\n",
    "batches = 10 * torch.rand(batch_size, input_dimensions, 1) - 5  # Scaled from [0, 1] to [-5, 5]\n",
    "\n",
    "# Call the projection sketch function\n",
    "P1 = proj_sketch(net=maml.model.model, batches=batches, subspace_dimension=subspace_dimension)\n",
    "\n",
    "# Still part of the computation graph ; detach it :\n",
    "P1 = P1.detach()\n",
    "print(f\"PROJECTION MATRIX : {P1}\")\n",
    "print(\"Found projection matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b62d7-7d04-4094-b677-a80f82f0beff",
   "metadata": {},
   "source": [
    "# UNLiMiTDproj feature transfer, no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc19d84-75bc-40ad-aa92-bbd5400ee91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, subspace_dimension, P1, **kwargs):\n",
    "        super(NTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        self.sub_dim = subspace_dimension\n",
    "        self.P1 = P1 # Projection matrix\n",
    "        \n",
    "        # Add 10 scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(subspace_dimension))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        D = torch.diag(torch.pow(self.scaling_param, 2))\n",
    "        \n",
    "        # print(\"jac1.T shape:\", jac1.T.shape)\n",
    "        # print(\"P1 shape:\", P1.T.shape)\n",
    "        # print(\"D shape:\", D.shape)\n",
    "        # print(\"P1.T shape:\", P1.shape)\n",
    "        # print(\"jac2 shape:\", jac2.shape)\n",
    "        \n",
    "        result = sigma * torch.chain_matmul(jac1, P1.T, D, P1, jac2.T)\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(self.net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b0a2e-a4ad-46b6-8ce2-8e5e2da5b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel_proj(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, net, subspace_dimension, P1):\n",
    "        super(ExactGPModel_proj, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = NTKernel_proj(net, subspace_dimension, P1)\n",
    "        #self.covar_module = CosSimNTKernel_proj(net, subspace_dimension, P1)\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=40)\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = self.feature_extractor(x)\n",
    "        #z_normalized = z - z.min(0)[0]\n",
    "        #z_normalized = 2 * (z_normalized / z_normalized.max(0)[0]) - 1\n",
    "        #x_normalized = x - x.min(0)[0]\n",
    "        #x_normalized = 2 * (x_normalized / x_normalized.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21906c5b-99e2-464d-8615-3b2edee761b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maml.model.model.train()\n",
    "for param in maml.model.model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-4))\n",
    "# likelihood.noise = 1e-4\n",
    "dummy_inputs = torch.zeros([n_shot_train,1])\n",
    "dummy_labels = torch.zeros([n_shot_train])\n",
    "gp = ExactGPModel_proj(dummy_inputs, dummy_labels, likelihood, maml.model.model, subspace_dimension, P1)\n",
    "trainable_params = sum(p.numel() for p in gp.parameters() if p.requires_grad)\n",
    "print(trainable_params)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "optimizer = torch.optim.Adam([{'params': gp.parameters(), 'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf69634-41d7-404d-8525-1fb9a7cf667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "maml.model.model.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in maml.model.model.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    sample_task = tasks.sample_task()\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - maml.model.model(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(x_all)).mean + maml.model.model(x_all).reshape(-1)\n",
    "    lower, upper = likelihood(gp(x_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "    lower += maml.model.model(x_all).reshape(-1)\n",
    "    upper += maml.model.model(x_all).reshape(-1)    \n",
    "    \n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.plot(np.squeeze(x_all), maml.model.model(x_all).reshape(-1).detach().numpy(), color='black', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    #plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc101a-7413-45e3-aa26-f44c0a867aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "maml.model.model.eval()\n",
    "tot_iterations=500\n",
    "mse_list = list()\n",
    "criterion = nn.MSELoss()\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - maml.model.model(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(x_query)).mean + maml.model.model(x_query).reshape(-1)\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651d287-65f8-4f9c-aaae-b8478af76131",
   "metadata": {},
   "source": [
    "# Training UNLIMITD-F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1608141-6a24-472b-8472-4bebd15dd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c913b-6866-4ecd-a2c3-09ce2cd5e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maml.model.model.train()\n",
    "for param in maml.model.model.parameters():\n",
    "    param.requires_grad_(True)\n",
    "gp.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': gp.parameters(), 'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9379c43-c374-4ad0-a6a4-5a5b7d05433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_iterations=50000\n",
    "for epoch in range(tot_iterations):\n",
    "    # gp.likelihood.noise = 1e-2\n",
    "    optimizer.zero_grad()\n",
    "    inputs, labels = tasks.sample_task().sample_data(n_shot_train, noise=0.05)\n",
    "    \n",
    "    gp.set_train_data(inputs=inputs, targets=labels - maml.model.model(inputs).reshape(-1))  \n",
    "    predictions = gp(inputs)\n",
    "    loss = -mll(predictions, gp.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #---- print some stuff ----\n",
    "    if(epoch%1000==0):\n",
    "        mse = criterion(predictions.mean, labels)\n",
    "        print(predictions.mean)\n",
    "        print('[%d] - Loss: %.3f  MSE: %.3f  lengthscale: %.3f   noise: %.3f' % (\n",
    "            epoch, loss.item(), mse.item(),\n",
    "            0.0, #gp.covar_module.base_kernel.lengthscale.item(),\n",
    "            gp.likelihood.noise.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683c791-5287-4789-afe0-3ff2e258c141",
   "metadata": {},
   "source": [
    "# Testing new UNLIMITD-F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43281085-9771-431b-9b91-431af0d2bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "maml.model.model.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in maml.model.model.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    sample_task = tasks.sample_task()\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - maml.model.model(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(x_all)).mean + maml.model.model(x_all).reshape(-1)\n",
    "    lower, upper = likelihood(gp(x_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "    lower += maml.model.model(x_all).reshape(-1)\n",
    "    upper += maml.model.model(x_all).reshape(-1)    \n",
    "    \n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.plot(np.squeeze(x_all), maml.model.model(x_all).reshape(-1).detach().numpy(), color='black', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    #plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e7c99-2b5b-4361-b645-746d625ff95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "maml.model.model.eval()\n",
    "tot_iterations=500\n",
    "mse_list = list()\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - maml.model.model(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(x_query)).mean + maml.model.model(x_query).reshape(-1)\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5f097-d67d-4965-a6b0-1e3ffaad4df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e1a3b-e081-4a68-8d94-b2fb14fd9120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
