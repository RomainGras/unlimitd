{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd4fbd2-cbc9-4563-9292-8a3daaef7ffc",
   "metadata": {},
   "source": [
    "## This is almost UNLIMITD, without the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e60582-ccfd-441f-ac07-6fac8634c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.24.3\n",
      "2.0.0+cu117\n",
      "1.11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(device)\n",
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(gpytorch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9e954-b4fc-4471-9d37-f328d2e9a85c",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65146f80-9533-4b28-8400-746674ce3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine_Task():\n",
    "    \"\"\"\n",
    "    A sine wave data distribution object with interfaces designed for MAML.\n",
    "    \"\"\"\n",
    "    def __init__(self, amplitude, phase, xmin, xmax):\n",
    "        self.amplitude = amplitude\n",
    "        self.phase = phase\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "\n",
    "    def true_function(self, x):\n",
    "        \"\"\"\n",
    "        Compute the true function on the given x.\n",
    "        \"\"\"\n",
    "        return self.amplitude * np.sin(self.phase + x)\n",
    "\n",
    "    def sample_data(self, size=1, noise=0.0, sort=False):\n",
    "        \"\"\"\n",
    "        Sample data from this task.\n",
    "\n",
    "        returns:\n",
    "            x: the feature vector of length size\n",
    "            y: the target vector of length size\n",
    "        \"\"\"\n",
    "        x = np.random.uniform(self.xmin, self.xmax, size)\n",
    "        if(sort): x = np.sort(x)\n",
    "        y = self.true_function(x)\n",
    "        if(noise>0): y += np.random.normal(loc=0.0, scale=noise, size=y.shape)\n",
    "        x = torch.tensor(x, dtype=torch.float).unsqueeze(1)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d5d9de5-fd39-4a2c-8871-2610c7db49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_Distribution():\n",
    "    \"\"\"\n",
    "    The task distribution for sine regression tasks for MAML\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, amplitude_min, amplitude_max, phase_min, phase_max, x_min, x_max, family=\"sine\"):\n",
    "        self.amplitude_min = amplitude_min\n",
    "        self.amplitude_max = amplitude_max\n",
    "        self.phase_min = phase_min\n",
    "        self.phase_max = phase_max\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.family = family\n",
    "\n",
    "    def sample_task(self):\n",
    "        \"\"\"\n",
    "        Sample from the task distribution.\n",
    "\n",
    "        returns:\n",
    "            Sine_Task object\n",
    "        \"\"\"\n",
    "        amplitude = np.random.uniform(self.amplitude_min, self.amplitude_max)\n",
    "        phase = np.random.uniform(self.phase_min, self.phase_max)\n",
    "        if(self.family==\"sine\"):\n",
    "            return Sine_Task(amplitude, phase, self.x_min, self.x_max)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79671c89-dd12-4d80-a235-07d93b72995e",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164157a4-969b-4845-9739-18c0960110bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Feature, self).__init__()\n",
    "        self.layer1 = nn.Linear(1, 40)\n",
    "        self.layer2 = nn.Linear(40,40)\n",
    "        self.layer3 = nn.Linear(40,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b90ca0-95e2-4720-924f-63a34b51683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        result = jac1@jac2.T\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3735edf4-e860-48b5-8413-cae87e02b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosSimNTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(CosSimNTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1T = self.compute_jacobian(x1).T\n",
    "        jac1T_norm = jac1T.norm(dim=0, keepdim=True)\n",
    "        jac1T_normalized = jac1T/jac1T_norm\n",
    "        #print(jac1.shape)\n",
    "        #print(jac1.norm(dim=0, keepdim=True).shape)\n",
    "        jac2T = self.compute_jacobian(x2).T if x1 is not x2 else jac1T\n",
    "        jac2T_norm = jac2T.norm(dim=0, keepdim=True)\n",
    "        jac2T_normalized = jac2T/jac2T_norm\n",
    "        \n",
    "        result = self.alpha * jac1T_normalized.T@jac2T_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49d2889b-1dbe-4305-8216-02a4f29fa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        #self.covar_module = NTKernel(net)\n",
    "        self.covar_module = gpytorch.kernels.LinearKernel()\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=40)\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = self.feature_extractor(x)\n",
    "        #z_normalized = z - z.min(0)[0]\n",
    "        #z_normalized = 2 * (z_normalized / z_normalized.max(0)[0]) - 1\n",
    "        #x_normalized = x - x.min(0)[0]\n",
    "        #x_normalized = 2 * (x_normalized / x_normalized.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6557cb4-568e-43b5-89b5-9f84c7536232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian(net, inputs):\n",
    "    \"\"\"\n",
    "    Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "    \"\"\"\n",
    "    net.zero_grad()\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "    def fnet_single(params, x):\n",
    "        return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "    jac = jac.values()\n",
    "    # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    reshaped_tensors = [\n",
    "        j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "            .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "            .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "        for j in jac\n",
    "    ]\n",
    "    return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be4f355e-716e-429d-8495-2db8483d6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters of GP : 2\n"
     ]
    }
   ],
   "source": [
    "n_shot_train = 10\n",
    "n_shot_test = 5\n",
    "train_range=(-5.0, 5.0)\n",
    "test_range=(-5.0, 5.0) # This must be (-5, +10) for the out-of-range condition\n",
    "criterion = nn.MSELoss()\n",
    "tasks     = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                  phase_min=0.0, phase_max=np.pi, \n",
    "                                  x_min=train_range[0], x_max=train_range[1], \n",
    "                                  family=\"sine\")\n",
    "net       = Feature()\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "# likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-4))\n",
    "# likelihood.noise = 1e-4\n",
    "dummy_inputs = torch.zeros([n_shot_train,1])\n",
    "dummy_labels = torch.zeros([n_shot_train])\n",
    "dummy_z = compute_jacobian(net, inputs)\n",
    "gp = ExactGPModel(dummy_z, dummy_labels, likelihood)\n",
    "trainable_params_gp = sum(p.numel() for p in gp.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters of GP : {trainable_params_gp}\")\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "optimizer = torch.optim.Adam([{'params': gp.parameters(), 'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a936d9-90c6-4491-bfeb-cab3da9287ba",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5656afc-3fdd-40b2-9ee3-84a81a5d29bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[0] - Loss: 2.277  MSE: 7.841  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[1] - Loss: 2.000  MSE: 5.972  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[2] - Loss: 1.483  MSE: 0.320  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[3] - Loss: 1.772  MSE: 1.132  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[4] - Loss: 1.450  MSE: 0.270  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[5] - Loss: 1.365  MSE: 0.124  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[6] - Loss: 1.614  MSE: 2.735  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[7] - Loss: 1.483  MSE: 0.975  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[8] - Loss: 1.634  MSE: 1.336  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[9] - Loss: 2.291  MSE: 10.380  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[10] - Loss: 1.612  MSE: 1.519  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[11] - Loss: 1.764  MSE: 1.648  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[12] - Loss: 1.523  MSE: 3.102  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[13] - Loss: 2.002  MSE: 6.849  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[14] - Loss: 2.008  MSE: 5.389  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[15] - Loss: 3.026  MSE: 11.646  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[16] - Loss: 2.131  MSE: 8.107  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[17] - Loss: 3.272  MSE: 7.760  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[18] - Loss: 2.372  MSE: 5.036  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[19] - Loss: 1.572  MSE: 1.027  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[20] - Loss: 3.551  MSE: 11.081  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[21] - Loss: 1.835  MSE: 2.438  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[22] - Loss: 2.746  MSE: 9.945  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[23] - Loss: 2.231  MSE: 5.459  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[24] - Loss: 1.503  MSE: 0.273  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[25] - Loss: 2.072  MSE: 2.792  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[26] - Loss: 1.613  MSE: 2.940  lengthscale: 0.000   noise: 0.718\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[27] - Loss: 1.556  MSE: 0.592  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[28] - Loss: 1.798  MSE: 1.548  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[29] - Loss: 2.718  MSE: 6.571  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[30] - Loss: 3.381  MSE: 11.638  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[31] - Loss: 2.285  MSE: 5.060  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[32] - Loss: 2.148  MSE: 7.089  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[33] - Loss: 1.912  MSE: 2.704  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[34] - Loss: 1.406  MSE: 0.172  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[35] - Loss: 1.683  MSE: 1.753  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[36] - Loss: 1.637  MSE: 2.237  lengthscale: 0.000   noise: 0.719\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[37] - Loss: 1.507  MSE: 0.013  lengthscale: 0.000   noise: 0.719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m gp(z)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(predictions, gp\u001b[38;5;241m.\u001b[39mtrain_targets)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#---- print some stuff ----\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "likelihood.train()\n",
    "gp.train()\n",
    "net.train()\n",
    "\n",
    "tot_iterations=10000\n",
    "\n",
    "t = time.time_ns()\n",
    "\n",
    "for epoch in range(tot_iterations):\n",
    "    # gp.likelihood.noise = 1e-2\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    inputs, labels = tasks.sample_task().sample_data(n_shot_train, noise=0.05)\n",
    "    \n",
    "    z = compute_jacobian(net, inputs)\n",
    "    gp.set_train_data(inputs=z, targets=labels - net(inputs).reshape(-1))  \n",
    "    predictions = gp(z)\n",
    "    loss = -mll(predictions, gp.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #---- print some stuff ----\n",
    "    if(epoch%1000==0):\n",
    "        mse = criterion(predictions.mean, labels)\n",
    "        print(predictions.mean)\n",
    "        print('[%d] - Loss: %.3f  MSE: %.3f  lengthscale: %.3f   noise: %.3f' % (\n",
    "            epoch, loss.item(), mse.item(),\n",
    "            0.0, #gp.covar_module.base_kernel.lengthscale.item(),\n",
    "            gp.likelihood.noise.item()\n",
    "        ))\n",
    "        \n",
    "print(f\"Total time : {(time.time_ns()-t)/1e9} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9898f0-8fb6-4f06-9f2f-febfdfdf2a54",
   "metadata": {},
   "source": [
    "## Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73ff8e-705c-4e89-a477-43119a1dddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_test = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                phase_min=0.0, phase_max=np.pi, \n",
    "                                x_min=test_range[0], x_max=test_range[1], \n",
    "                                family=\"sine\")\n",
    "\n",
    "sample_task = tasks_test.sample_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39f387-17a4-446c-879a-aa11b6d7d810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp = ExactGPModel(dummy_inputs, dummy_labels, likelihood, net)\n",
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - net(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(x_all)).mean + net(x_all).reshape(-1)\n",
    "    lower, upper = likelihood(gp(x_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "    lower += net(x_all).reshape(-1)\n",
    "    upper += net(x_all).reshape(-1)    \n",
    "    \n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.plot(np.squeeze(x_all), net(x_all).reshape(-1).detach().numpy(), color='black', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    #plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48930c65-2172-43a9-a3e0-24e095e76206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test, please wait...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tasks_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m mse_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tot_iterations):\n\u001b[0;32m----> 8\u001b[0m     sample_task \u001b[38;5;241m=\u001b[39m \u001b[43mtasks_test\u001b[49m\u001b[38;5;241m.\u001b[39msample_task()\n\u001b[1;32m      9\u001b[0m     sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     10\u001b[0m     x_all, y_all \u001b[38;5;241m=\u001b[39m sample_task\u001b[38;5;241m.\u001b[39msample_data(sample_size, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tasks_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "tot_iterations=500\n",
    "mse_list = list()\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - net(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(x_query)).mean + net(x_query).reshape(-1)\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc363e85-24b8-4d32-839d-90f8d957377e",
   "metadata": {},
   "source": [
    "## Sketching FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235e575-f82f-4e83-8a8d-c35144afb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1430333-1852-44f2-964f-0472624477b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(Y, W, Psi):\n",
    "    \"\"\"\n",
    "    given Y = A @ Om, (N, k)\n",
    "    and W = Psi @ A, (l, M)\n",
    "    and Psi(X) = Psi @ X, (N,...) -> (l,...)\n",
    "    where Om and Psi and random sketching operators\n",
    "    returns Q (N x k), X (k x M) such that A ~= QX\n",
    "    \"\"\"\n",
    "    # Perform QR decomposition on Y to get orthonormal basis Q\n",
    "    Q, _ = torch.linalg.qr(Y, mode='reduced')\n",
    "    \n",
    "    # Apply Psi to Q and then perform QR decomposition\n",
    "    U, T = torch.linalg.qr(torch.matmul(Psi, Q), mode='reduced')\n",
    "    \n",
    "    # Solve the triangular system T @ X = U^T @ W for X\n",
    "    # PyTorch does not have a direct equivalent to scipy.linalg.solve_triangular,\n",
    "    # so we use torch.linalg.solve which can handle triangular matrices if specified.\n",
    "    X = torch.linalg.solve(T, torch.matmul(U.T, W))\n",
    "    \n",
    "    return Q, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743d193-5aea-4fa0-9749-420b58d3e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_low_rank_approx(Y, W, Psi):\n",
    "    \"\"\"\n",
    "    Perform a symmetric low-rank approximation of the matrix A.\n",
    "    \"\"\"\n",
    "    Q, X = low_rank_approx(Y, W, Psi)  # Assuming Psi is now correctly handled\n",
    "    k = Q.shape[-1]  # Dimension of the sketches\n",
    "    \n",
    "    # Concatenate Q and X.T along columns to form a larger matrix\n",
    "    tmp = torch.cat((Q, X.T), dim=1)  # Correctly access the transpose\n",
    "    \n",
    "    # Perform QR decomposition on the concatenated matrix\n",
    "    U, T = torch.linalg.qr(tmp, mode='reduced')\n",
    "    \n",
    "    # Extract T1 and T2 from T\n",
    "    T1 = T[:, :k]\n",
    "    T2 = T[:, k:2*k]\n",
    "    \n",
    "    # Compute symmetric matrix S\n",
    "    S = (T1 @ T2.T + T2 @ T1.T) / 2\n",
    "    \n",
    "    return U, S\n",
    "\n",
    "# Example usage\n",
    "N, k, l = 100, 10, 50  # Example dimensions\n",
    "Y = torch.randn(N, k)  # Random Y matrix\n",
    "W = torch.randn(l, N)  # Random W matrix\n",
    "Psi = torch.randn(l, N)  # Random Psi matrix\n",
    "\n",
    "# Call the function\n",
    "U, S = sym_low_rank_approx(Y, W, Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aaa719-6caf-4b9e-9786-7afea76dce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_rank_eig_approx(Y, W, psi, r):\n",
    "    \"\"\"\n",
    "    Returns U (N x r), D (r) such that A ~= U diag(D) U^T using PyTorch.\n",
    "    \"\"\"\n",
    "    # Obtain symmetric low-rank approximation\n",
    "    U, S = sym_low_rank_approx(Y, W, psi)\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    D, V = torch.linalg.eigh(S)\n",
    "    \n",
    "    # Truncate to keep the top-r eigenvalues and corresponding eigenvectors\n",
    "    D = D[-r:]  # Top r eigenvalues\n",
    "    V = V[:, -r:]  # Corresponding eigenvectors\n",
    "    \n",
    "    # Update U to be U @ V\n",
    "    U = U @ V\n",
    "    \n",
    "    return U, D\n",
    "\n",
    "# Example usage\n",
    "N, k, l, r = 100, 10, 50, 5  # Example dimensions\n",
    "Y = torch.randn(N, k)  # Random Y matrix\n",
    "W = torch.randn(l, N)  # Random W matrix\n",
    "Psi = torch.randn(l, N)  # Random Psi matrix\n",
    "\n",
    "# Call the function\n",
    "U, D = fixed_rank_eig_approx(Y, W, Psi, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40e27c-ca24-4155-9f8e-35c5f8ebd3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch(net, batches, k, l):\n",
    "    \"\"\"\n",
    "    Returns a good rank 2k approximation of the FIM using PyTorch.\n",
    "    \"\"\"\n",
    "    M = batches.size(0)\n",
    "    N_params = sum(p.numel() for p in net.parameters())\n",
    "    print(N_params)\n",
    "\n",
    "    om = torch.randn(k, N_params)\n",
    "    psi = torch.randn(l, N_params)\n",
    "\n",
    "    Y = torch.zeros(N_params, k)\n",
    "    W = torch.zeros(l, N_params)\n",
    "\n",
    "    for batch in batches:\n",
    "        JT = jacobian(net, batch).T\n",
    "        Y += (om @ JT @ JT.T).T / M\n",
    "        W += (psi @ JT @ JT.T) / M\n",
    "\n",
    "    # Compute the rank-2k approximation\n",
    "    U, D = fixed_rank_eig_approx(Y, W, psi, 2 * k)\n",
    "\n",
    "    return U, D\n",
    "\n",
    "def jacobian(net, batch):\n",
    "    \"\"\"\n",
    "    Compute the Jacobian for the batch. This needs to be adapted based on the actual function.\n",
    "    \"\"\"\n",
    "    net.zero_grad()\n",
    "    params = {k: v for k, v in net.named_parameters()}\n",
    "    def fnet_single(params, x):\n",
    "        return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "    jac = vmap(jacrev(fnet_single), (None, 0))(params, batch)\n",
    "    jac = jac.values()\n",
    "    # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    reshaped_tensors = [\n",
    "        j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "            .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "            .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "        for j in jac\n",
    "    ]\n",
    "    return torch.cat(reshaped_tensors, dim=0)\n",
    "\n",
    "# Example usage would require setting up a model, parameters, batches, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298db9f-e3d4-4ed7-b2f9-29884eb56bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def proj_sketch(net, batches, subspace_dimension):\n",
    "    t = time.time_ns()\n",
    "\n",
    "    T = 6 * subspace_dimension + 4 \n",
    "    k = (T - 1) // 3                    # k = 2 * subspace_dimension + 1\n",
    "    l = T - k                           # l = 4 * subspace_dimension + 3\n",
    "\n",
    "    U, D = sketch(net, batches, k, l)\n",
    "    idx = D.argsort(descending=True)\n",
    "    print(\"U shape:\", U.shape)\n",
    "    print(\"Index tensor:\", idx)\n",
    "    print(\"Requested subspace dimension:\", subspace_dimension)\n",
    "    \n",
    "    # Ensure idx is of type long for indexing\n",
    "    # idx = idx.long()\n",
    "\n",
    "    P1 = U[:, idx[:subspace_dimension]].T\n",
    "\n",
    "    print(f\"Done sketching in {(time.time_ns() - t) / 1e9:.4f} s\")\n",
    "\n",
    "    return P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18d459-8181-432b-8ad4-20eafca9012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subspace_dimension = 10\n",
    "\n",
    "print(\"Finding projection matrix\")\n",
    "# here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "# P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "\n",
    "\n",
    "# Generate batches in the range [-5, 5]\n",
    "batch_size = 100\n",
    "input_dimensions = sum(p.numel() for p in net.parameters())\n",
    "batches = 10 * torch.rand(batch_size, input_dimensions, 1) - 5  # Scaled from [0, 1] to [-5, 5]\n",
    "\n",
    "# Call the projection sketch function\n",
    "P1 = proj_sketch(net=net, batches=batches, subspace_dimension=subspace_dimension)\n",
    "\n",
    "# Still part of the computation graph ; detach it :\n",
    "P1 = P1.detach()\n",
    "print(f\"PROJECTION MATRIX : {P1}\")\n",
    "print(\"Found projection matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2591ace-5325-4ae3-b1e6-4ab71cfc60c9",
   "metadata": {},
   "source": [
    "## Unlimitd F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d9ab7-d85b-4b01-9493-a4c9f7380d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, subspace_dimension, P1, **kwargs):\n",
    "        super(NTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        self.sub_dim = subspace_dimension\n",
    "        self.P1 = P1 # Projection matrix\n",
    "        \n",
    "        # Add 10 scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(subspace_dimension))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        D = torch.diag(torch.pow(self.scaling_param, 2))\n",
    "        \n",
    "        # print(\"jac1.T shape:\", jac1.T.shape)\n",
    "        # print(\"P1 shape:\", P1.T.shape)\n",
    "        # print(\"D shape:\", D.shape)\n",
    "        # print(\"P1.T shape:\", P1.shape)\n",
    "        # print(\"jac2 shape:\", jac2.shape)\n",
    "        \n",
    "        result = torch.chain_matmul(jac1, P1.T, D, P1, jac2.T)\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113839b-affd-44a2-8fa6-70e8dd086146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosSimNTKernel_proj(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, subspace_dimension, P1, **kwargs):\n",
    "        super(CosSimNTKernel_proj, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        self.sub_dim = subspace_dimension\n",
    "        self.P1 = P1 # Projection matrix\n",
    "        \n",
    "        # Add 10 scaling parameters, initializing them as one\n",
    "        self.scaling_param = nn.Parameter(torch.ones(subspace_dimension))\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        jac1 = self.compute_jacobian(x1)\n",
    "        jac2 = self.compute_jacobian(x2) if x1 is not x2 else jac1\n",
    "        \n",
    "        D = torch.diag(self.scaling_param)\n",
    "        \n",
    "        result_1 = torch.chain_matmul(D, P1, jac1.T)\n",
    "        result_2 = torch.chain_matmul(D, P1, jac2.T)\n",
    "        \n",
    "        result_1_norm = result_1.norm(dim=0, keepdim=True)\n",
    "        result_1_normalized = result_1/result_1_norm\n",
    "        #print(result_1.shape)\n",
    "        #print(result_1.norm(dim=0, keepdim=True).shape)\n",
    "        result_2_norm = result_2.norm(dim=0, keepdim=True)\n",
    "        result_2_normalized = result_2/result_2_norm\n",
    "        \n",
    "        result = self.alpha * result_1_normalized.T@result_2_normalized\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "    \n",
    "    def compute_jacobian(self, inputs):\n",
    "        \"\"\"\n",
    "        Return the jacobian of a batch of inputs, thanks to the vmap functionality\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        jac = vmap(jacrev(fnet_single), (None, 0))(params, inputs)\n",
    "        jac = jac.values()\n",
    "        # jac1 of dimensions [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "                .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "                .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jac\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada96e1-d211-40f1-99dd-0ce5948a9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel_proj(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, net, subspace_dimension, P1):\n",
    "        super(ExactGPModel_proj, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        #self.covar_module = NTKernel_proj(net, subspace_dimension, P1)\n",
    "        self.covar_module = CosSimNTKernel_proj(net, subspace_dimension, P1)\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "        #self.covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=40)\n",
    "        #self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #z = self.feature_extractor(x)\n",
    "        #z_normalized = z - z.min(0)[0]\n",
    "        #z_normalized = 2 * (z_normalized / z_normalized.max(0)[0]) - 1\n",
    "        #x_normalized = x - x.min(0)[0]\n",
    "        #x_normalized = 2 * (x_normalized / x_normalized.max(0)[0]) - 1\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbab25-9d6b-49f0-a71c-635f317251bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shot_train = 10\n",
    "n_shot_test = 5\n",
    "train_range=(-5.0, 5.0)\n",
    "test_range=(-5.0, 5.0) # This must be (-5, +10) for the out-of-range condition\n",
    "criterion = nn.MSELoss()\n",
    "tasks     = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                  phase_min=0.0, phase_max=np.pi, \n",
    "                                  x_min=train_range[0], x_max=train_range[1], \n",
    "                                  family=\"sine\")\n",
    "\n",
    "net.train()\n",
    "for param in net.parameters():\n",
    "    param.requires_grad_(True)\n",
    "\n",
    "likelihood.train()\n",
    "# likelihood.noise_covar.register_constraint(\"raw_noise\", gpytorch.constraints.GreaterThan(1e-4))\n",
    "# likelihood.noise = 1e-4\n",
    "dummy_inputs = torch.zeros([n_shot_train,1])\n",
    "dummy_labels = torch.zeros([n_shot_train])\n",
    "gp = ExactGPModel_proj(dummy_inputs, dummy_labels, likelihood, net, subspace_dimension, P1)\n",
    "trainable_params = sum(p.numel() for p in gp.parameters() if p.requires_grad)\n",
    "print(trainable_params)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)\n",
    "optimizer = torch.optim.Adam([{'params': gp.parameters(), 'lr': 1e-3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0461df-9e69-43b0-b16b-0e0940b5bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_iterations=50000\n",
    "for epoch in range(tot_iterations):\n",
    "    # gp.likelihood.noise = 1e-2\n",
    "    optimizer.zero_grad()\n",
    "    inputs, labels = tasks.sample_task().sample_data(n_shot_train, noise=0.05)\n",
    "    \n",
    "    gp.set_train_data(inputs=inputs, targets=labels - net(inputs).reshape(-1))  \n",
    "    predictions = gp(inputs)\n",
    "    loss = -mll(predictions, gp.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #---- print some stuff ----\n",
    "    if(epoch%1000==0):\n",
    "        mse = criterion(predictions.mean, labels)\n",
    "        print(predictions.mean)\n",
    "        print('[%d] - Loss: %.3f  MSE: %.3f  lengthscale: %.3f   noise: %.3f' % (\n",
    "            epoch, loss.item(), mse.item(),\n",
    "            0.0, #gp.covar_module.base_kernel.lengthscale.item(),\n",
    "            gp.likelihood.noise.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62539e07-f4b8-4323-b6c3-ef11ed3eb8f1",
   "metadata": {},
   "source": [
    "## Second Test Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863fc82-1755-41d7-869e-11f298f257fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_test = Task_Distribution(amplitude_min=0.1, amplitude_max=5.0, \n",
    "                                phase_min=0.0, phase_max=np.pi, \n",
    "                                x_min=test_range[0], x_max=test_range[1], \n",
    "                                family=\"sine\")\n",
    "\n",
    "sample_task = tasks_test.sample_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5668c88-650b-4c91-8851-5f94bac6644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_size = 200\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "gp.covar_module.eval()\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "    \n",
    "for i in range(10):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - net(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "            \n",
    "    #Evaluation on all data\n",
    "    mean = likelihood(gp(x_all)).mean + net(x_all).reshape(-1)\n",
    "    lower, upper = likelihood(gp(x_all)).confidence_region() #2 standard deviations above and below the mean\n",
    "    lower += net(x_all).reshape(-1)\n",
    "    upper += net(x_all).reshape(-1)    \n",
    "    \n",
    "    #Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #true-curve\n",
    "    true_curve = np.linspace(train_range[0], train_range[1], 1000)\n",
    "    true_curve = [sample_task.true_function(x) for x in true_curve]\n",
    "    ax.plot(np.linspace(train_range[0], train_range[1], 1000), true_curve, color='blue', linewidth=2.0)\n",
    "    if(train_range[1]<test_range[1]):\n",
    "        dotted_curve = np.linspace(train_range[1], test_range[1], 1000)\n",
    "        dotted_curve = [sample_task.true_function(x) for x in dotted_curve]\n",
    "        ax.plot(np.linspace(train_range[1], test_range[1], 1000), dotted_curve, color='blue', linestyle=\"--\", linewidth=2.0)\n",
    "    #query points (ground-truth)\n",
    "    #ax.scatter(x_query, y_query, color='blue')\n",
    "    #query points (predicted)\n",
    "\n",
    "    ax.plot(np.squeeze(x_all), mean.detach().numpy(), color='red', linewidth=2.0)\n",
    "    ax.plot(np.squeeze(x_all), net(x_all).reshape(-1).detach().numpy(), color='black', linewidth=2.0)\n",
    "    ax.fill_between(np.squeeze(x_all),\n",
    "                    lower.detach().numpy(), upper.detach().numpy(),\n",
    "                    alpha=.1, color='red')\n",
    "    #support points\n",
    "    ax.scatter(x_support, y_support, color='darkblue', marker='*', s=50, zorder=10)\n",
    "                    \n",
    "    #all points\n",
    "    #ax.scatter(x_all.numpy(), y_all.numpy())\n",
    "    #plt.show()\n",
    "    plt.ylim(-6.0, 6.0)\n",
    "    plt.xlim(test_range[0], test_range[1])\n",
    "    #plt.savefig('plot_DKT_' + str(i) + '.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c57298-5913-4383-8b4e-585e4c66a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test, please wait...\")\n",
    "\n",
    "likelihood.eval()\n",
    "net.eval()\n",
    "tot_iterations=500\n",
    "mse_list = list()\n",
    "for epoch in range(tot_iterations):\n",
    "    sample_task = tasks_test.sample_task()\n",
    "    sample_size = 200\n",
    "    x_all, y_all = sample_task.sample_data(sample_size, noise=0.1, sort=True)\n",
    "    indices = np.arange(sample_size)\n",
    "    np.random.shuffle(indices)\n",
    "    support_indices = np.sort(indices[0:n_shot_test])\n",
    "\n",
    "    query_indices = np.sort(indices[n_shot_test:])\n",
    "    x_support = x_all[support_indices]\n",
    "    y_support = y_all[support_indices]\n",
    "    x_query = x_all[query_indices]\n",
    "    y_query = y_all[query_indices]\n",
    "\n",
    "    #Feed the support set\n",
    "    gp.train()\n",
    "    gp.set_train_data(inputs=x_support, targets=y_support - net(x_support).reshape(-1), strict=False)  \n",
    "    gp.eval()\n",
    "\n",
    "    #Evaluation on query set\n",
    "    mean = likelihood(gp(x_query)).mean + net(x_query).reshape(-1)\n",
    "\n",
    "    mse = criterion(mean, y_query)\n",
    "    mse_list.append(mse.item())\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Average MSE: \" + str(np.mean(mse_list)) + \" +- \" + str(np.std(mse_list)))\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffd45e-cd02-4e73-b6f3-472aaf9f7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp.covar_module.scaling_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06bbe4-d4f0-423f-833c-3d14c2f7384b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea226aa2-7776-48c3-b7b4-0e5543cdccc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f2c06c3f-650f-4e14-a91d-15e975b8432f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NNwithGP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[291], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m data_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Covariance noise already leant via gpytorch.likelihoods.GaussianLikelihood()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Define model, likelihood, and optimizer\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNNwithGP\u001b[49m(\u001b[38;5;241m40\u001b[39m, K, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mparameters()},\n\u001b[1;32m     13\u001b[0m ], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Training_loop\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NNwithGP' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "num_epochs = 50\n",
    "n_tasks_per_epoch = 10\n",
    "input_dim = 1\n",
    "K = 10 # Number of points per task\n",
    "data_noise = 0.05\n",
    "# Covariance noise already leant via gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# Define model, likelihood, and optimizer\n",
    "model = NNwithGP(40, K, 'relu', 10)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# Training_loop\n",
    "model.train()\n",
    "model.likelihood.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get a new batch of data\n",
    "    train_x, train_y = get_raw_batch(n_tasks_per_epoch, K, 0, data_noise)\n",
    "    \n",
    "    # Initialize loss accumulation\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Loop over each task\n",
    "    for i in range(train_x.size(0)):  # Assuming train_x is of shape (nb_task, K, 1)\n",
    "        # Process each task individually\n",
    "        nn_output = model.forward_nn(train_x[i])  # Forward pass through the NN for task i\n",
    "        gp_pred = model.forward_gp(nn_output)  # GP forward using NN output for task i\n",
    "        loss = -model.likelihood(gp_pred, train_y[i]).log_prob(train_y[i])  # Compute NLL for task i\n",
    "        total_loss += loss  # Accumulate loss\n",
    "\n",
    "    # Calculate mean loss across all tasks\n",
    "    mean_loss = total_loss / n_tasks_per_epoch\n",
    "    \n",
    "    # Backpropagate the mean loss\n",
    "    mean_loss.backward()\n",
    "    \n",
    "    # Optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch {epoch+1}, Mean Loss: {mean_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec71192-4fe2-4603-b63a-10872abd7933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61a4a26-5e39-4f17-8976-db1b6ce36e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNwithGP(nn.Module):\n",
    "    def __init__(self, n_neurons, K, activation, reg_dim):\n",
    "        super(NNwithGP, self).__init__()\n",
    "        # Neural network layers\n",
    "        self.dense1 = nn.Linear(K, n_neurons)\n",
    "        if activation == \"relu\":\n",
    "            self.act_fn = nn.ReLU()\n",
    "        elif activation == \"tanh\":\n",
    "            self.act_fn = nn.Tanh()\n",
    "        self.dense2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.dense3 = nn.Linear(n_neurons, reg_dim)\n",
    "\n",
    "        # Additional parameter used in the GP kernel\n",
    "        self.extra_param = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        # GP Model Components\n",
    "        self.gp_mean = gpytorch.means.ConstantMean()\n",
    "        self.gp_covar = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "    def forward_nn(self, x):\n",
    "        x = self.act_fn(self.dense1(x))\n",
    "        x = self.act_fn(self.dense2(x))\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "\n",
    "    def forward_gp(self, x):\n",
    "        mean_x = self.gp_mean(x)\n",
    "        # Use extra_param in the kernel\n",
    "        self.gp_covar.base_kernel.lengthscale = torch.abs(self.extra_param)\n",
    "        covar_x = self.gp_covar(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382b2391-26c1-4d7c-a60d-a2fd22453a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1 and 2x40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize the Jacobian matrix\u001b[39;00m\n\u001b[1;32m     28\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mflatten()), \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()))\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out))\n\u001b[1;32m     15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(out)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x40)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 40)\n",
    "        self.layer2 = nn.Linear(40,40)\n",
    "        self.layer3 = nn.Linear(40,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define an input tensor\n",
    "input_tensor = torch.randn(2, 1, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(input_tensor)\n",
    "\n",
    "# Initialize the Jacobian matrix\n",
    "jacobian = torch.zeros(len(outputs.flatten()), sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# Compute Jacobian\n",
    "for i, output in enumerate(outputs.flatten()):\n",
    "    # Zero all parameter gradients\n",
    "    model.zero_grad()\n",
    "    # Compute gradient of output[i] with respect to all parameters\n",
    "    output.backward(retain_graph=True)\n",
    "    # Store gradients in the Jacobian row\n",
    "    j = 0\n",
    "    for param in model.parameters():\n",
    "        jacobian[i, j:j+param.numel()] = param.grad.flatten()\n",
    "        j += param.numel()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "print(\"Non-trainable parameters:\", non_trainable_params)\n",
    "print(\"Jacobian matrix:\")\n",
    "print(jacobian.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd22d0d-32ea-4311-bf26-2b6c320fd121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1801\n",
      "Non-trainable parameters: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The inputs given to jacobian must be either a Tensor or a tuple of Tensors but the given inputs has type <class 'builtin_function_or_method'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainable parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable_params)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-trainable parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, non_trainable_params)\n\u001b[0;32m---> 37\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mempirical_ntk_jacobian_contraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfnet_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mempirical_ntk_jacobian_contraction\u001b[0;34m(fnet_single, params, x1, x2)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempirical_ntk_jacobian_contraction\u001b[39m(fnet_single, params, x1, x2):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Compute J(x1)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     jac1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(jac1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Compute J(x2)\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/autograd/functional.py:588\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _jacfwd(func, inputs, strict, vectorize)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 588\u001b[0m     is_inputs_tuple, inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_as_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjacobian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[38;5;241m=\u001b[39mcreate_graph, need_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/autograd/functional.py:37\u001b[0m, in \u001b[0;36m_as_tuple\u001b[0;34m(inp, arg_name, fn_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m given to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must be either a Tensor or a tuple of Tensors but the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m value at index \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg_name, fn_name, i, \u001b[38;5;28mtype\u001b[39m(el)))\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m given to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m must be either a Tensor or a tuple of Tensors but the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(arg_name, fn_name, arg_name, \u001b[38;5;28mtype\u001b[39m(el)))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_inp_tuple, inp\n",
      "\u001b[0;31mTypeError\u001b[0m: The inputs given to jacobian must be either a Tensor or a tuple of Tensors but the given inputs has type <class 'builtin_function_or_method'>."
     ]
    }
   ],
   "source": [
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "x_train = torch.randn(20, 5, device=device)\n",
    "x_test = torch.randn(20, 5, device=device)\n",
    "\n",
    "\n",
    "net = SimpleNN().to(device)\n",
    "\n",
    "# Detaching the parameters because we won't be calling Tensor.backward().\n",
    "params = {k: v.detach() for k, v in net.named_parameters()}\n",
    "\n",
    "def apply_net(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "\n",
    "def empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2):\n",
    "    # Compute J(x1)\n",
    "    jac1 = torch.autograd.functional.jacobian(lambda params: apply_net(params, x1))\n",
    "    print(jac1.shape)\n",
    "\n",
    "    # Compute J(x2)\n",
    "    jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)\n",
    "    jac2 = jac2.values()\n",
    "    jac2 = [j.flatten(2) for j in jac2]\n",
    "    print(jac1[1].shape)\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    result = torch.stack([torch.einsum('Nf,Mf->NM', j1, j2) for j1, j2 in zip(jac1, jac2)])\n",
    "    result = result.sum(0)\n",
    "    return result\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "print(\"Non-trainable parameters:\", non_trainable_params)\n",
    "result = empirical_ntk_jacobian_contraction(fnet_single, params, x_train, x_train)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b635a-bbe6-4a86-9119-974874ed3cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9d3c5-6bd2-438c-8e69-47dcafde8b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4503570-ebd8-4dd0-9680-fdd244ddec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67a0ce8f-88c3-434e-89ba-34bd0b7e4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, net, **kwargs):\n",
    "        super(NTKernel, self).__init__(**kwargs)\n",
    "        self.net = net\n",
    "\n",
    "    def compute_gradients(self, inputs):\n",
    "        params = {k: v for k, v in self.net.named_parameters()}\n",
    "        def fnet_single(params, x):\n",
    "            return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        grads_x1 = self.compute_gradients(x1)\n",
    "        grads_x2 = self.compute_gradients(x2) if x1 is not x2 else grads_x1\n",
    "        result = torch.matmul(grads_x1, grads_x2.transpose(-1, -2))\n",
    "        \n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result\n",
    "\n",
    "# Make sure the network and the kernel are correctly initialized and used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6017998-6777-4694-b8d7-fa4acb4c5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPWithNTK(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, neural_net):\n",
    "        super(GPWithNTK, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = NTKernel(neural_net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ab25391-58c4-4024-8557-8ad7a64fc434",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mset_train_data(inputs\u001b[38;5;241m=\u001b[39mtrain_x, targets\u001b[38;5;241m=\u001b[39mtrain_y)  \n\u001b[1;32m     19\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/distributions/multivariate_normal.py:192\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    185\u001b[0m         covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;241m*\u001b[39m(diff_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m covar_size \u001b[38;5;28;01mfor\u001b[39;00m diff_size, covar_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(diff\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], padded_batch_shape)),\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    189\u001b[0m         )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m covar \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39minv_quad_logdet(inv_quad_rhs\u001b[38;5;241m=\u001b[39mdiff\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), logdet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/linear_operator/operators/added_diag_linear_operator.py:209\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     added_diag_linear_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_linear_op \u001b[38;5;241m+\u001b[39m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_diag_tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2064\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentation_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m    Returns a\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;124;03m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;124;03m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLinearOperatorRepresentationTree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/linear_operator/operators/linear_operator_representation_tree.py:15\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[0;34m(self, linear_op)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(linear_op\u001b[38;5;241m.\u001b[39m_args, linear_op\u001b[38;5;241m.\u001b[39m_differentiable_kwargs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(arg\u001b[38;5;241m.\u001b[39mrepresentation):  \u001b[38;5;66;03m# Is it a lazy tensor?\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         representation_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mslice\u001b[39m(counter, counter \u001b[38;5;241m+\u001b[39m representation_size, \u001b[38;5;28;01mNone\u001b[39;00m), arg\u001b[38;5;241m.\u001b[39mrepresentation_tree()))\n\u001b[1;32m     17\u001b[0m         counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m representation_size\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:397\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrepresentation()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# representation\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepresentation()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[0;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mNTKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 23\u001b[0m     grads_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     grads_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(x2) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x2 \u001b[38;5;28;01melse\u001b[39;00m grads_x1\n\u001b[1;32m     25\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(grads_x1, grads_x2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mNTKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 23\u001b[0m     grads_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     grads_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(x2) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x2 \u001b[38;5;28;01melse\u001b[39;00m grads_x1\n\u001b[1;32m     25\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(grads_x1, grads_x2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "    \u001b[0;31m[... skipping similar frames: Kernel.__call__ at line 530 (588 times), Module.__call__ at line 31 (588 times), NTKernel.forward at line 23 (588 times), NTKernel.compute_gradients at line 8 (587 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/kernels/kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx1_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_dim_is_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mNTKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m---> 23\u001b[0m     grads_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     grads_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gradients(x2) \u001b[38;5;28;01mif\u001b[39;00m x1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x2 \u001b[38;5;28;01melse\u001b[39;00m grads_x1\n\u001b[1;32m     25\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(grads_x1, grads_x2\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[77], line 7\u001b[0m, in \u001b[0;36mNTKernel.compute_gradients\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(inputs)\n\u001b[1;32m      9\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2348\u001b[0m, in \u001b[0;36mModule.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_replica\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2342\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling .zero_grad() from a module created with nn.DataParallel() has no effect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe parameters are copied (in a differentiable manner) from the original module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means they are not leaf nodes in autograd and so don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt accumulate gradients. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you need gradients in your forward method, consider using autograd.grad instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2348\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m   2349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2081\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \n\u001b[1;32m   2062\u001b[0m \u001b[38;5;124;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2079\u001b[0m \n\u001b[1;32m   2080\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2081\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters(recurse\u001b[38;5;241m=\u001b[39mrecurse):\n\u001b[1;32m   2082\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m param\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2115\u001b[0m, in \u001b[0;36mModule.named_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters, yielding both the\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;124;03mname of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2110\u001b[0m \n\u001b[1;32m   2111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_named_members(\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module\u001b[38;5;241m.\u001b[39m_parameters\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   2114\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix, recurse\u001b[38;5;241m=\u001b[39mrecurse, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate)\n\u001b[0;32m-> 2115\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2049\u001b[0m, in \u001b[0;36mModule._named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2047\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   2048\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules(prefix\u001b[38;5;241m=\u001b[39mprefix, remove_duplicate\u001b[38;5;241m=\u001b[39mremove_duplicate) \u001b[38;5;28;01mif\u001b[39;00m recurse \u001b[38;5;28;01melse\u001b[39;00m [(prefix, \u001b[38;5;28mself\u001b[39m)]\n\u001b[0;32m-> 2049\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_prefix, module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   2050\u001b[0m     members \u001b[38;5;241m=\u001b[39m get_members_fn(module)\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m members:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2266\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:2260\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n\u001b[0;32m-> 2260\u001b[0m         \u001b[43mmemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "train_x = torch.randn(50, 10)  # Example data\n",
    "train_y = torch.randn(50)  # Example targets\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "net = SimpleNN()\n",
    "model = GPWithNTK(train_x, train_y, likelihood, net)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters(), 'lr': 0.01}\n",
    "], lr=0.1)\n",
    "\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    model.set_train_data(inputs=train_x, targets=train_y)  \n",
    "    predictions = model(train_x)\n",
    "    loss = -mll(predictions, model.train_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, 50, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f1d3c-23b0-4af3-a23b-1bdfa5f7d491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d7d60-2445-4e49-8dc1-cfff8eeca5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517d9a4-dd15-40dd-b72c-a6638c4d1898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "253c2c25-a592-4038-8302-b22d7bce1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, vmap, vjp, jvp, jacrev\n",
    "device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(2, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "52313dfb-3d0f-4f9f-a686-db5aa14d8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.randn(20, 1, device=device)\n",
    "x_test = torch.randn(5, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "2269b86a-ef81-49d3-8d41-a88356eaebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 31\n",
      "Non-trainable parameters: 0\n",
      "['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2])\n",
      "torch.Size([4, 2])\n",
      "torch.Size([4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNN().to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "non_trainable_params = sum(p.numel() for p in net.parameters() if not p.requires_grad)\n",
    "\n",
    "print(\"Trainable parameters:\", trainable_params)\n",
    "print(\"Non-trainable parameters:\", non_trainable_params)\n",
    "\n",
    "params = {k: v for k, v in net.named_parameters()}\n",
    "print(list(params.keys()))\n",
    "print(params['fc1.weight'].shape)\n",
    "print(params['fc1.bias'].shape)\n",
    "print(params['fc2.weight'].shape)\n",
    "print(params['fc2.bias'].shape)\n",
    "print(params['fc3.weight'].shape)\n",
    "print(params['fc3.bias'].shape)\n",
    "def fnet_single(params, x):\n",
    "    return functional_call(net, params, (x.unsqueeze(0),)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "bc50a023-569c-4989-8bf6-0493d37877b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n",
      "torch.Size([31, 60])\n",
      "torch.Size([31, 15])\n",
      "torch.Size([60, 15])\n"
     ]
    }
   ],
   "source": [
    "def empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2):\n",
    "    # Compute J(x1)\n",
    "    jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x1)\n",
    "    print(jac1.keys())\n",
    "    jac1 = jac1.values()\n",
    "    # jac1 = [Nb Layers, Nb input / Batch, dim(y), Nb param/layer left, Nb param/layer right]\n",
    "    def reshape_jac(jacobians):\n",
    "        reshaped_tensors = [\n",
    "            j.flatten(2)                # Flatten starting from the 3rd dimension to acount for weights and biases layers\n",
    "              .permute(2, 0, 1)         # Permute to align dimensions correctly for reshaping\n",
    "              .reshape(-1, j.shape[0] * j.shape[1])  # Reshape to (c, a*b) using dynamic sizing\n",
    "            for j in jacobians\n",
    "        ]\n",
    "        return torch.cat(reshaped_tensors, dim=0)\n",
    "    \n",
    "    jac1 = reshape_jac(jac1)\n",
    "    print(jac1.shape)\n",
    "\n",
    "    # Compute J(x2)\n",
    "    jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)\n",
    "    jac2 = jac2.values()\n",
    "    jac2 = reshape_jac(jac2)\n",
    "    print(jac2.shape)\n",
    "\n",
    "    # Compute J(x1) @ J(x2).T\n",
    "    result = torch.einsum('fN,fM->NM', jac1, jac2)\n",
    "    return result\n",
    "\n",
    "result = empirical_ntk_jacobian_contraction(fnet_single, params, x_train, x_test)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd48c6-c475-4f8c-9746-6fbe7bf5dc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
