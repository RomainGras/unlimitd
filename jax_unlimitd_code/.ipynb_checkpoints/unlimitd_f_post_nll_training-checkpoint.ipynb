{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e490cb-34a9-4163-9053-f24161a24b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_UNIFIED_MEMORY=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_UNIFIED_MEMORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f9178a-a7e8-45a5-a963-d606cbdfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unlimtd_f\n",
    "import time\n",
    "from jax import random, jit, pmap, value_and_grad, lax\n",
    "import dataset_multi_infinite\n",
    "import dataset_lines_infinite\n",
    "import test\n",
    "import plots\n",
    "import ntk\n",
    "import nll\n",
    "import jax\n",
    "from jax import numpy as np\n",
    "import pickle\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a7d732-78fc-4f92-8ca7-d8f7cb2a07fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655235988902897757\n"
     ]
    }
   ],
   "source": [
    "seed = 1655235988902897757\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815a68c8-dec0-4665-be22-23a13d4f2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(kernel_self, x_a, y_a, maddox_noise):\n",
    "    \"\"\"\n",
    "    Computes the NLL of this data (one task only) wrt the kernel\n",
    "    x_a is a (batch_size, input_dims) array (! has lost n_tasks)\n",
    "    y_a is a (batch_size, reg_dim) array (! has lost n_tasks)\n",
    "    \"\"\"\n",
    "    cov_a_a = kernel_self(x_a)\n",
    "    K = cov_a_a.shape[0]\n",
    "    cov_a_a = cov_a_a + maddox_noise ** 2 * np.eye(K)\n",
    "    \n",
    "    # prior mean is 0\n",
    "    y_a = np.reshape(y_a, (-1))\n",
    "\n",
    "    L = jax.scipy.linalg.cho_factor(cov_a_a)\n",
    "    alpha = jax.scipy.linalg.cho_solve(L, y_a)\n",
    "    return 0.5 * y_a.T @ alpha + np.sum(np.log(np.diag(L[0]))) + 0.5 * K * np.log(2 * np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4dea8a-7338-4799-8dc1-21fbf6e80e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_batch_one_kernel(kernel_self, x_a, y_a, maddox_noise, jacobian, mean):\n",
    "    \"\"\"\n",
    "    NLL for a batch of tasks, when there is only one kernel (singGP)\n",
    "    x_a is (n_tasks, batch_size, input_dims) (input_dims are (128, 128, 1) for vision, (1,) for toy problems)\n",
    "    y_a is (n_tasks, batch_size, reg_dim)\n",
    "    \"\"\"\n",
    "    def f(carry, task_data):\n",
    "        x_a, y_a = task_data\n",
    "        y_a = y_a - utils.falseaffine_correction0(jacobian, mean, x_a)\n",
    "        loss_here = nll(kernel_self, x_a, y_a, maddox_noise)\n",
    "        return None, loss_here\n",
    "\n",
    "    _, losses = lax.scan(f, None, (x_a, y_a))\n",
    "\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4f811d-a0ac-49ea-ad24-39f3f56142b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def nll_batch_average_identity_cov(current_params, current_mean, apply_fn, current_batch_stats, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n",
    "    \n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, x_a, y_a, maddox_noise, jacobian, current_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f2b355-f145-46fb-9e58-299ac9a43530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_identity_cov(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_m) = value_and_grad(nll_batch_average_identity_cov, argnums = (0, 1) )(current_state.params,\n",
    "                                                              current_state.mean,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f8da8f-2182-47e9-9af3-9aa84fcd0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_stats_updater(current_state, x_a):\n",
    "    # shape of x_a is (n_tasks, batch_size, inputs_dims...)\n",
    "    \n",
    "    batch_stats = current_state.batch_stats\n",
    "    \n",
    "    def f(old_batch_stats, _x_a):\n",
    "        # shape of _x_a is (batch_size, input_dims)\n",
    "        _, mutated_vars = current_state.apply_fn_raw({\"params\":current_state.params,\n",
    "                                                      \"batch_stats\": old_batch_stats},\n",
    "                                                     _x_a,\n",
    "                                                     mutable=[\"batch_stats\"])\n",
    "        \n",
    "        new_batch_stats = mutated_vars[\"batch_stats\"]\n",
    "        return new_batch_stats, None\n",
    "\n",
    "    batch_stats = dict(batch_stats)\n",
    "    print(type(batch_stats))\n",
    "    batch_stats, _ = lax.scan(f, batch_stats, x_a)\n",
    "    return batch_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a6bfef-9868-4c45-aece-3f4514c99f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def grad_applier_identity_cov(current_state, gradients_p, gradients_m, new_batch_stats):\n",
    "    return current_state.apply_gradients(grads_params=gradients_p, grads_mean=gradients_m, new_batch_stats=new_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b951430-4a3a-477f-947c-c0b71b69d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer\n",
    "\n",
    "def step_identity_cov(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_m) = pmap(pmapable_loss_identity_cov,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_m = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_m)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_identity_cov(current_state, current_gradients_p, current_gradients_m, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad8598-b20a-4235-9a59-2b29010815f5",
   "metadata": {},
   "source": [
    "## Unlimitd f training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c5acfed-43d4-48d9-ab79-44472a07f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntk import get_kernel_and_jac_lowdim_cov\n",
    "\n",
    "def nll_batch_average_lowdim_cov_singGP(current_params, current_mean, current_scale, apply_fn, current_batch_stats, proj, x_a, y_a, maddox_noise):\n",
    "    _, kernel_self, jacobian = get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_scale, current_batch_stats, proj)\n",
    "\n",
    "    return np.mean(nll_batch_one_kernel(kernel_self, x_a, y_a, maddox_noise, jacobian, current_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72165a52-b07d-4f90-b308-0773e747a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmapable_loss_lowdim_cov_singGP(current_state, x_a, y_a, maddox_noise):\n",
    "    # we can't pass current_state because we have to explicitely show the variable\n",
    "    loss, (gradients_p, gradients_m, gradients_s) = value_and_grad(nll_batch_average_lowdim_cov_singGP, argnums = (0, 1, 2) )(current_state.params,\n",
    "                                                              current_state.mean,\n",
    "                                                              current_state.scale,\n",
    "                                                              current_state.apply_fn,\n",
    "                                                              current_state.batch_stats,\n",
    "                                                              current_state.proj,\n",
    "                                                              x_a,\n",
    "                                                              y_a,\n",
    "                                                              maddox_noise)\n",
    "    \n",
    "    return loss, (gradients_p, gradients_m, gradients_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b845a68b-ee0f-473f-bf0f-1b105fa0bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import grad_applier_lowdim_cov_singGP\n",
    "\n",
    "def step_lowdim_cov_singGP(key, current_state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn):\n",
    "    # Draw the samples for this step, and split it to prepare for pmap (jit'd)\n",
    "    x_a, y_a, x_a_div, y_a_div = get_train_batch_fn(key, n_tasks, K, data_noise, n_devices)\n",
    "    \n",
    "    # Compute loss and gradient through gpu parallelization\n",
    "    unaveraged_losses, (unaveraged_gradients_p, unaveraged_gradients_m, unaveraged_gradients_s) = pmap(pmapable_loss_lowdim_cov_singGP,\n",
    "                             in_axes=(None, 0, 0, None),\n",
    "                             static_broadcasted_argnums=(3)\n",
    "                            )(current_state, x_a_div, y_a_div, maddox_noise)\n",
    "    \n",
    "    current_loss = np.mean(unaveraged_losses)\n",
    "    current_gradients_p = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_p)\n",
    "    current_gradients_m = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_m)\n",
    "    current_gradients_s = jax.tree_map(lambda array: np.mean(array, axis=0), unaveraged_gradients_s)\n",
    "    \n",
    "    # Update batch_stats \"manually\" (jit'd)\n",
    "    new_batch_stats = batch_stats_updater(current_state, x_a)\n",
    "    \n",
    "    # Update state (parameters and optimizer)\n",
    "    current_state = grad_applier_lowdim_cov_singGP(current_state, current_gradients_p, current_gradients_m, current_gradients_s, new_batch_stats)\n",
    "    \n",
    "    return current_state, current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154087b2-a619-4bc9-8b46-8ebf39ae7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(key, step, n_epochs, state, n_tasks, K, data_noise, maddox_noise, get_train_batch_fn, eval_during_training_fn):\n",
    "    \"\"\"\n",
    "    Available step functions:\n",
    "    * step_identity_cov\n",
    "    * step_lowdim_cov_singGP\n",
    "    * step_lowdim_cov_mixture\n",
    "\n",
    "    Available get_train_batch_fn functions:\n",
    "    * dataset_sines_infinite.get_training_batch\n",
    "    * dataset_sines_finite.get_training_batch\n",
    "    * dataset_lines_infinite.get_training_batch\n",
    "    * dataset_multi_infinite.get_training_batch\n",
    "    * dataset_shapenet1d.get_training_batch\n",
    "    \n",
    "    \"\"\"\n",
    "    n_devices = jax.local_device_count()\n",
    "\n",
    "    print(\"Starting training with:\")\n",
    "    print(f\"-n_epochs={n_epochs}\")\n",
    "    print(f\"-n_tasks={n_tasks}\")\n",
    "    print(f\"-K={K}\")\n",
    "    print(f\"-data_noise={data_noise}\")\n",
    "    print(f\"-maddox_noise={maddox_noise}\")\n",
    "\n",
    "    losses = []\n",
    "    evals = []\n",
    "    t = time.time_ns()\n",
    "\n",
    "    for epoch_index in range(n_epochs):\n",
    "        key, subkey = random.split(key)\n",
    "        state, current_loss = step(subkey, state, n_tasks, K, data_noise, maddox_noise, n_devices, get_train_batch_fn)\n",
    "\n",
    "        if(np.isnan(current_loss)):\n",
    "            print(\"Nan, aborting\")\n",
    "            break\n",
    "        \n",
    "        losses.append(current_loss)\n",
    "\n",
    "        if epoch_index % 10 == 0:\n",
    "            print(f\"{epoch_index}  | {current_loss:.4f} ({(time.time_ns() - t)/ 10**9:.4f} s)\")\n",
    "        t = time.time_ns()\n",
    "\n",
    "        if epoch_index % 500 == 0:\n",
    "            key, subkey = random.split(key)\n",
    "            current_eval = eval_during_training_fn(subkey, state)\n",
    "            evals.append( current_eval )\n",
    "            print(f\"Eval: {current_eval}\")\n",
    "    print(\"Completed training\")\n",
    "\n",
    "    return state, losses, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f35d6e7-4a6c-4cb8-adf9-354b12428689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nll import gaussian_posterior\n",
    "\n",
    "def test_nll_one_kernel(key, kernel_self, jacobian, get_test_batch_fn, K, n_tasks, data_noise, maddox_noise, current_mean):\n",
    "    \"\"\"\n",
    "    Returns the NLLs for n_tasks random tasks, in the singGP case.\n",
    "    \"\"\"\n",
    "    x_a, y_a, _, _ = get_test_batch_fn(key, n_tasks, K, 0, data_noise)\n",
    "    all_nlls = nll_batch_one_kernel(kernel_self, x_a, y_a, maddox_noise, jacobian, current_mean)\n",
    "\n",
    "    return all_nlls\n",
    "\n",
    "def test_error_one_kernel(key, kernel, kernel_self, jacobian, get_test_batch_fn, error_fn, K, L, n_tasks, data_noise, maddox_noise, current_mean):\n",
    "    \"\"\"\n",
    "    Returns the error for n_tasks random tasks, in the singGP case.\n",
    "    \"\"\"\n",
    "    x_a, y_a, x_b, y_b = get_test_batch_fn(key, n_tasks, K, L, data_noise)\n",
    "\n",
    "    def f(carry, task_data):\n",
    "        _x_a, _y_a, _x_b, _y_b = task_data\n",
    "        _y_a = _y_a - utils.falseaffine_correction0(jacobian, current_mean, _x_a)\n",
    "        predictions = gaussian_posterior(kernel, kernel_self, _x_a, _y_a, _x_b, maddox_noise)\n",
    "        predictions = predictions + utils.falseaffine_correction0(jacobian, current_mean, _x_b)\n",
    "        return None, error_fn(predictions, _y_b)\n",
    "    \n",
    "    _, all_errors = lax.scan(f, None, (x_a, y_a, x_b, y_b))\n",
    "\n",
    "    return all_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa26b2e5-e61e-45e2-bc15-75c589e8a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer\n",
    "import ntk\n",
    "import test\n",
    "import train_states\n",
    "import models\n",
    "import utils\n",
    "import fim\n",
    "\n",
    "import dataset_sines_infinite\n",
    "import dataset_sines_finite\n",
    "import dataset_multi_infinite\n",
    "\n",
    "from jax import random\n",
    "from jax import numpy as np\n",
    "from flax.core import FrozenDict\n",
    "import optax\n",
    "\n",
    "from ntk import get_kernel_and_jac_identity_cov\n",
    "from ntk import get_kernel_and_jac_lowdim_cov\n",
    "import unlimtd_f\n",
    "\n",
    "def unlimtd_f_uni_modal_infinite(seed, pre_n_epochs, pre_n_tasks, pre_K, post_n_epochs, post_n_tasks, post_K, data_noise, maddox_noise, meta_lr, subspace_dimension):\n",
    "    key = random.PRNGKey(seed)\n",
    "    key_init, key = random.split(key)\n",
    "    \n",
    "    print(\"===============\")\n",
    "    print(\"This is UNLIMTD-F\")\n",
    "    print(\"For the uni-modal dataset: infinite sine dataset\")\n",
    "    print(\"This variant of UNLIMTD-F approaches the distribution with a single GP\")\n",
    "    print(\"===============\")\n",
    "    print(\"Creating model\")\n",
    "    model = models.small_network(40, \"relu\", 1)\n",
    "    batch = random.uniform(key_init, shape=(5,1), minval=-5, maxval=5)\n",
    "    init_vars = model.init(key_init, batch)\n",
    "    apply_fn = utils.apply_fn_wrapper(model.apply, True)\n",
    "    apply_fn_raw = model.apply\n",
    "\n",
    "    # Training before finding the FIM matrix\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_identity_cov\n",
    "    get_train_batch_fn = dataset_sines_infinite.get_training_batch\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_mean = optax.adam(learning_rate = meta_lr)\n",
    "    mean_init = np.zeros( (utils.get_param_size(init_vars[\"params\"]),) )\n",
    "\n",
    "    pre_state = train_states.TrainStateIdentityCovariance.create(apply_fn=apply_fn, apply_fn_raw=apply_fn_raw, params=init_vars[\"params\"], mean=mean_init, tx_params=optimizer_params, tx_mean=optimizer_mean, batch_stats=FrozenDict())\n",
    "    \n",
    "    def eval_during_pre_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_batch_stats = state.batch_stats\n",
    "        current_mean = state.mean\n",
    "        kernel, kernel_self, jacobian = ntk.get_kernel_and_jac_identity_cov(apply_fn, current_params, current_batch_stats)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        part_apply_fn = partial(apply_fn, current_params, current_batch_stats)\n",
    "        nlls = test_nll_one_kernel(subkey_1, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "        mses = test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, dataset_sines_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting first part of training (identity covariance)\")\n",
    "    key_pre, key = random.split(key)\n",
    "    pre_state, pre_losses, pre_evals = train_and_eval(key_pre, step, pre_n_epochs, pre_state, pre_n_tasks, pre_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_pre_training)\n",
    "    print(\"Finished first part of training\")\n",
    "\n",
    "    # FIM\n",
    "    print(\"Finding projection matrix\")\n",
    "    key_fim, key_data, key = random.split(key, 3)\n",
    "    # here we use the exact FIM, we do not need to approximate given the (small) size of the network\n",
    "    # P1 = fim.proj_exact(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, current_batch_stats=pre_state.batch_stats, subspace_dimension=subspace_dimension)\n",
    "    P1 = fim.proj_sketch(key=key_fim, apply_fn=apply_fn, current_params=pre_state.params, batch_stats=pre_state.batch_stats, batches=random.uniform(key_data, shape=(100, 1761, 1), minval=-5, maxval=5), subspace_dimension=subspace_dimension)\n",
    "    print(\"Found projection matrix\")\n",
    "\n",
    "    # Usual training with projection\n",
    "    print(\"Creating optimizers\")\n",
    "    step = step_lowdim_cov_singGP\n",
    "    optimizer_params = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_mean = optax.adam(learning_rate = meta_lr)\n",
    "    optimizer_scale = optax.adam(learning_rate = meta_lr)\n",
    "    init_scale = np.ones( (subspace_dimension,) )\n",
    "\n",
    "    post_state = train_states.TrainStateLowDimCovSingGP.create(apply_fn = apply_fn, apply_fn_raw=apply_fn_raw, params = pre_state.params, mean=pre_state.mean, scale=init_scale, tx_params = optimizer_params, tx_mean = optimizer_mean, tx_scale = optimizer_scale, batch_stats=pre_state.batch_stats, proj = P1)\n",
    "\n",
    "    def eval_during_post_training(key, state):\n",
    "        current_params = state.params\n",
    "        current_batch_stats = state.batch_stats\n",
    "        current_mean = state.mean\n",
    "        current_scale = state.scale\n",
    "        kernel, kernel_self, jacobian = ntk.get_kernel_and_jac_lowdim_cov(apply_fn, current_params, current_scale, current_batch_stats, P1)\n",
    "\n",
    "        subkey_1, subkey_2 = random.split(key)\n",
    "        part_apply_fn = partial(apply_fn, current_params, current_batch_stats)\n",
    "        nlls = test_nll_one_kernel(subkey_1, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, K=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "        mses = test_error_one_kernel(subkey_2, kernel, kernel_self, jacobian, dataset_sines_infinite.get_test_batch, dataset_sines_infinite.error_fn, K=pre_K, L=pre_K, n_tasks=1000, data_noise=data_noise, maddox_noise=maddox_noise, current_mean=current_mean)\n",
    "\n",
    "        return np.mean(nlls), np.mean(mses)\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    key_post, key = random.split(key)\n",
    "    post_state, post_losses, post_evals = train_and_eval(key_post, step, post_n_epochs, post_state, post_n_tasks, post_K, data_noise, maddox_noise, get_train_batch_fn, eval_during_post_training)\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # Returning everything\n",
    "    return init_vars, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24763136-051f-4eb5-aa90-7d962166d704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "This is UNLIMTD-F\n",
      "For the uni-modal dataset: infinite sine dataset\n",
      "This variant of UNLIMTD-F approaches the distribution with a single GP\n",
      "===============\n",
      "Creating model\n",
      "Creating optimizers\n",
      "Starting first part of training (identity covariance)\n",
      "Starting training with:\n",
      "-n_epochs=7500\n",
      "-n_tasks=24\n",
      "-K=10\n",
      "-data_noise=0.05\n",
      "-maddox_noise=0.05\n",
      "<class 'dict'>\n",
      "0  | 1452.8462 (1.7736 s)\n",
      "Eval: (Array(1719.1808, dtype=float32), Array(4.926873, dtype=float32))\n",
      "<class 'dict'>\n",
      "10  | 914.1870 (0.0294 s)\n",
      "20  | 406.1330 (0.0347 s)\n",
      "30  | 336.9009 (0.0330 s)\n",
      "40  | 284.8087 (0.0367 s)\n",
      "50  | 84.1991 (0.0339 s)\n",
      "60  | 216.7860 (0.0292 s)\n",
      "70  | 102.9554 (0.0286 s)\n",
      "80  | 127.6422 (0.0310 s)\n",
      "90  | 137.4526 (0.0340 s)\n",
      "100  | 35.8149 (0.0342 s)\n",
      "110  | 63.5981 (0.0344 s)\n",
      "120  | 34.0029 (0.0359 s)\n",
      "130  | 29.0722 (0.0290 s)\n",
      "140  | 30.6017 (0.0344 s)\n",
      "150  | 33.1381 (0.0371 s)\n",
      "160  | 29.1669 (0.0335 s)\n",
      "170  | 27.3821 (0.0351 s)\n",
      "180  | 31.0826 (0.0357 s)\n",
      "190  | 35.5156 (0.0279 s)\n",
      "200  | 25.0848 (0.0351 s)\n",
      "210  | 39.6674 (0.0347 s)\n",
      "220  | 41.1593 (0.0305 s)\n",
      "230  | 22.0195 (0.0296 s)\n",
      "240  | 27.4280 (0.0371 s)\n",
      "250  | 31.7969 (0.0323 s)\n",
      "260  | 25.4781 (0.0370 s)\n",
      "270  | 20.3954 (0.0342 s)\n",
      "280  | 26.8131 (0.0261 s)\n",
      "290  | 20.9410 (0.0268 s)\n",
      "300  | 49.3454 (0.0304 s)\n",
      "310  | 32.9027 (0.0341 s)\n",
      "320  | 19.9323 (0.0310 s)\n",
      "330  | 46.8775 (0.0267 s)\n",
      "340  | 24.6622 (0.0291 s)\n",
      "350  | 21.8819 (0.0335 s)\n",
      "360  | 20.7863 (0.0296 s)\n",
      "370  | 25.0215 (0.0356 s)\n",
      "380  | 20.5289 (0.0315 s)\n",
      "390  | 23.9404 (0.0330 s)\n",
      "400  | 21.8243 (0.0256 s)\n",
      "410  | 22.6660 (0.0312 s)\n",
      "420  | 21.2978 (0.0319 s)\n",
      "430  | 20.6148 (0.0261 s)\n",
      "440  | 18.6916 (0.0274 s)\n",
      "450  | 17.7849 (0.0257 s)\n",
      "460  | 21.5136 (0.0317 s)\n",
      "470  | 20.3125 (0.0327 s)\n",
      "480  | 23.1582 (0.0280 s)\n",
      "490  | 19.3622 (0.0303 s)\n",
      "500  | 21.5391 (0.0257 s)\n",
      "Eval: (Array(20.289743, dtype=float32), Array(0.78621405, dtype=float32))\n",
      "510  | 18.7176 (0.0257 s)\n",
      "520  | 18.8122 (0.0363 s)\n",
      "530  | 20.0977 (0.0295 s)\n",
      "540  | 19.0961 (0.0296 s)\n",
      "550  | 19.3224 (0.0309 s)\n",
      "560  | 18.8542 (0.0324 s)\n",
      "570  | 20.3612 (0.0326 s)\n",
      "580  | 19.2270 (0.0363 s)\n",
      "590  | 20.1111 (0.0307 s)\n",
      "600  | 21.8254 (0.0333 s)\n",
      "610  | 19.4115 (0.0385 s)\n",
      "620  | 17.6788 (0.0347 s)\n",
      "630  | 16.9871 (0.0362 s)\n",
      "640  | 16.8424 (0.0388 s)\n",
      "650  | 18.6341 (0.0395 s)\n",
      "660  | 18.3156 (0.0317 s)\n",
      "670  | 20.4360 (0.0327 s)\n",
      "680  | 20.2224 (0.0314 s)\n",
      "690  | 18.1799 (0.0304 s)\n",
      "700  | 17.7949 (0.0326 s)\n",
      "710  | 17.2972 (0.0297 s)\n",
      "720  | 20.7500 (0.0342 s)\n",
      "730  | 15.7765 (0.0328 s)\n",
      "740  | 16.9163 (0.0333 s)\n",
      "750  | 17.9942 (0.0348 s)\n",
      "760  | 18.9886 (0.0287 s)\n",
      "770  | 19.0774 (0.0310 s)\n",
      "780  | 17.5688 (0.0343 s)\n",
      "790  | 19.6068 (0.0296 s)\n",
      "800  | 17.8584 (0.0358 s)\n",
      "810  | 16.2359 (0.0335 s)\n",
      "820  | 16.8537 (0.0353 s)\n",
      "830  | 17.7693 (0.0378 s)\n",
      "840  | 16.1253 (0.0323 s)\n",
      "850  | 16.6511 (0.0333 s)\n",
      "860  | 16.8703 (0.0393 s)\n",
      "870  | 16.1200 (0.0376 s)\n",
      "880  | 15.4755 (0.0322 s)\n",
      "890  | 16.2607 (0.0288 s)\n",
      "900  | 14.2391 (0.0368 s)\n",
      "910  | 15.4767 (0.0337 s)\n",
      "920  | 15.4697 (0.0350 s)\n",
      "930  | 14.9864 (0.0369 s)\n",
      "940  | 14.0818 (0.0321 s)\n",
      "950  | 14.0803 (0.0316 s)\n",
      "960  | 13.8043 (0.0394 s)\n",
      "970  | 14.2803 (0.0344 s)\n",
      "980  | 14.5451 (0.0361 s)\n",
      "990  | 13.8149 (0.0348 s)\n",
      "1000  | 15.8932 (0.0376 s)\n",
      "Eval: (Array(14.310231, dtype=float32), Array(0.3645603, dtype=float32))\n",
      "1010  | 12.4427 (0.0286 s)\n",
      "1020  | 13.6813 (0.0302 s)\n",
      "1030  | 14.8416 (0.0270 s)\n",
      "1040  | 11.5437 (0.0353 s)\n",
      "1050  | 14.1646 (0.0291 s)\n",
      "1060  | 11.1679 (0.0273 s)\n",
      "1070  | 14.2027 (0.0295 s)\n",
      "1080  | 12.4583 (0.0275 s)\n",
      "1090  | 12.1041 (0.0325 s)\n",
      "1100  | 12.1178 (0.0328 s)\n",
      "1110  | 12.9305 (0.0346 s)\n",
      "1120  | 12.9714 (0.0296 s)\n",
      "1130  | 12.7924 (0.0331 s)\n",
      "1140  | 12.8365 (0.0310 s)\n",
      "1150  | 11.0222 (0.0289 s)\n",
      "1160  | 11.1099 (0.0324 s)\n",
      "1170  | 12.4782 (0.0326 s)\n",
      "1180  | 12.2098 (0.0347 s)\n",
      "1190  | 10.1081 (0.0291 s)\n",
      "1200  | 11.1991 (0.0328 s)\n",
      "1210  | 12.2374 (0.0328 s)\n",
      "1220  | 12.0360 (0.0257 s)\n",
      "1230  | 11.7930 (0.0297 s)\n",
      "1240  | 14.1843 (0.0326 s)\n",
      "1250  | 10.6675 (0.0292 s)\n",
      "1260  | 10.7052 (0.0275 s)\n",
      "1270  | 11.5207 (0.0342 s)\n",
      "1280  | 11.0260 (0.0319 s)\n",
      "1290  | 10.3173 (0.0299 s)\n",
      "1300  | 11.6682 (0.0287 s)\n",
      "1310  | 10.8476 (0.0271 s)\n",
      "1320  | 8.3352 (0.0292 s)\n",
      "1330  | 11.6752 (0.0282 s)\n",
      "1340  | 11.2468 (0.0324 s)\n",
      "1350  | 9.5845 (0.0278 s)\n",
      "1360  | 11.3762 (0.0327 s)\n",
      "1370  | 7.9240 (0.0293 s)\n",
      "1380  | 9.7773 (0.0331 s)\n",
      "1390  | 11.1559 (0.0291 s)\n",
      "1400  | 9.8819 (0.0323 s)\n",
      "1410  | 7.9673 (0.0320 s)\n",
      "1420  | 9.3606 (0.0278 s)\n",
      "1430  | 10.1168 (0.0364 s)\n",
      "1440  | 9.2659 (0.0257 s)\n",
      "1450  | 9.5743 (0.0284 s)\n",
      "1460  | 10.2774 (0.0307 s)\n",
      "1470  | 6.6577 (0.0299 s)\n",
      "1480  | 6.9778 (0.0287 s)\n",
      "1490  | 8.2332 (0.0357 s)\n",
      "1500  | 9.9496 (0.0260 s)\n",
      "Eval: (Array(8.252758, dtype=float32), Array(0.15725206, dtype=float32))\n",
      "1510  | 7.9776 (0.0302 s)\n",
      "1520  | 8.0303 (0.0304 s)\n",
      "1530  | 7.3697 (0.0312 s)\n",
      "1540  | 8.0347 (0.0275 s)\n",
      "1550  | 7.3301 (0.0308 s)\n",
      "1560  | 10.9907 (0.0277 s)\n",
      "1570  | 10.4424 (0.0307 s)\n",
      "1580  | 6.6768 (0.0300 s)\n",
      "1590  | 7.5482 (0.0296 s)\n",
      "1600  | 5.8929 (0.0294 s)\n",
      "1610  | 6.7306 (0.0319 s)\n",
      "1620  | 6.7786 (0.0337 s)\n",
      "1630  | 7.9709 (0.0285 s)\n",
      "1640  | 6.5845 (0.0262 s)\n",
      "1650  | 8.1573 (0.0286 s)\n",
      "1660  | 6.1220 (0.0291 s)\n",
      "1670  | 6.9877 (0.0259 s)\n",
      "1680  | 6.6184 (0.0282 s)\n",
      "1690  | 6.1699 (0.0292 s)\n",
      "1700  | 6.1464 (0.0280 s)\n",
      "1710  | 6.5070 (0.0292 s)\n",
      "1720  | 8.0121 (0.0276 s)\n",
      "1730  | 6.9647 (0.0282 s)\n",
      "1740  | 7.2980 (0.0301 s)\n",
      "1750  | 7.6095 (0.0281 s)\n",
      "1760  | 6.6804 (0.0293 s)\n",
      "1770  | 6.0886 (0.0302 s)\n",
      "1780  | 5.3197 (0.0278 s)\n",
      "1790  | 5.8931 (0.0302 s)\n",
      "1800  | 7.9408 (0.0299 s)\n",
      "1810  | 8.3572 (0.0315 s)\n",
      "1820  | 7.2894 (0.0282 s)\n",
      "1830  | 7.4682 (0.0308 s)\n",
      "1840  | 5.5067 (0.0287 s)\n",
      "1850  | 6.6807 (0.0286 s)\n",
      "1860  | 6.0345 (0.0296 s)\n",
      "1870  | 8.7587 (0.0293 s)\n",
      "1880  | 7.3255 (0.0308 s)\n",
      "1890  | 5.7560 (0.0300 s)\n",
      "1900  | 6.6584 (0.0357 s)\n",
      "1910  | 5.2280 (0.0311 s)\n",
      "1920  | 7.4081 (0.0256 s)\n",
      "1930  | 7.0379 (0.0280 s)\n",
      "1940  | 4.8477 (0.0280 s)\n",
      "1950  | 5.2447 (0.0255 s)\n",
      "1960  | 6.7915 (0.0255 s)\n",
      "1970  | 6.7152 (0.0255 s)\n",
      "1980  | 6.9626 (0.0256 s)\n",
      "1990  | 6.5541 (0.0254 s)\n",
      "2000  | 5.7471 (0.0255 s)\n",
      "Eval: (Array(5.9070196, dtype=float32), Array(0.06665387, dtype=float32))\n",
      "2010  | 6.1464 (0.0260 s)\n",
      "2020  | 5.7274 (0.0264 s)\n",
      "2030  | 5.1205 (0.0316 s)\n",
      "2040  | 5.8362 (0.0430 s)\n",
      "2050  | 5.0532 (0.0374 s)\n",
      "2060  | 7.0190 (0.0372 s)\n",
      "2070  | 5.7416 (0.0407 s)\n",
      "2080  | 6.6444 (0.0382 s)\n",
      "2090  | 4.1754 (0.0374 s)\n",
      "2100  | 5.0365 (0.0402 s)\n",
      "2110  | 6.1729 (0.0374 s)\n",
      "2120  | 6.8355 (0.0334 s)\n",
      "2130  | 5.4769 (0.0408 s)\n",
      "2140  | 4.3525 (0.0391 s)\n",
      "2150  | 7.4414 (0.0421 s)\n",
      "2160  | 4.5903 (0.0386 s)\n",
      "2170  | 3.6347 (0.0395 s)\n",
      "2180  | 5.9927 (0.0365 s)\n",
      "2190  | 4.5954 (0.0386 s)\n",
      "2200  | 5.0396 (0.0321 s)\n",
      "2210  | 5.3789 (0.0366 s)\n",
      "2220  | 5.9552 (0.0380 s)\n",
      "2230  | 5.5579 (0.0404 s)\n",
      "2240  | 4.9120 (0.0360 s)\n",
      "2250  | 5.7873 (0.0374 s)\n",
      "2260  | 3.3586 (0.0360 s)\n",
      "2270  | 5.6271 (0.0371 s)\n",
      "2280  | 4.7405 (0.0357 s)\n",
      "2290  | 5.1977 (0.0384 s)\n",
      "2300  | 5.0989 (0.0341 s)\n",
      "2310  | 4.9183 (0.0326 s)\n",
      "2320  | 5.1206 (0.0347 s)\n",
      "2330  | 5.1414 (0.0333 s)\n",
      "2340  | 4.1812 (0.0357 s)\n",
      "2350  | 4.4557 (0.0330 s)\n",
      "2360  | 4.4117 (0.0342 s)\n",
      "2370  | 3.3379 (0.0283 s)\n",
      "2380  | 6.6494 (0.0339 s)\n",
      "2390  | 3.2789 (0.0316 s)\n",
      "2400  | 4.9998 (0.0333 s)\n",
      "2410  | 8.2734 (0.0321 s)\n",
      "2420  | 5.0281 (0.0317 s)\n",
      "2430  | 7.0126 (0.0300 s)\n",
      "2440  | 4.7602 (0.0321 s)\n",
      "2450  | 6.1238 (0.0279 s)\n",
      "2460  | 4.1885 (0.0302 s)\n",
      "2470  | 5.2138 (0.0304 s)\n",
      "2480  | 3.9926 (0.0333 s)\n",
      "2490  | 5.0746 (0.0321 s)\n",
      "2500  | 5.6910 (0.0305 s)\n",
      "Eval: (Array(5.0965495, dtype=float32), Array(0.07366151, dtype=float32))\n",
      "2510  | 4.3481 (0.0289 s)\n",
      "2520  | 5.1918 (0.0300 s)\n",
      "2530  | 4.5464 (0.0298 s)\n",
      "2540  | 4.7932 (0.0305 s)\n",
      "2550  | 4.0385 (0.0283 s)\n",
      "2560  | 3.9120 (0.0280 s)\n",
      "2570  | 4.2693 (0.0284 s)\n",
      "2580  | 6.1845 (0.0293 s)\n",
      "2590  | 4.5810 (0.0296 s)\n",
      "2600  | 4.6926 (0.0293 s)\n",
      "2610  | 4.5833 (0.0298 s)\n",
      "2620  | 5.5990 (0.0350 s)\n",
      "2630  | 3.7811 (0.0306 s)\n",
      "2640  | 4.1362 (0.0350 s)\n",
      "2650  | 4.0800 (0.0288 s)\n",
      "2660  | 5.3080 (0.0323 s)\n",
      "2670  | 4.3434 (0.0321 s)\n",
      "2680  | 4.2921 (0.0301 s)\n",
      "2690  | 4.6645 (0.0284 s)\n",
      "2700  | 4.1248 (0.0301 s)\n",
      "2710  | 5.2767 (0.0281 s)\n",
      "2720  | 4.8426 (0.0287 s)\n",
      "2730  | 5.3634 (0.0280 s)\n",
      "2740  | 3.6237 (0.0282 s)\n",
      "2750  | 4.7920 (0.0303 s)\n",
      "2760  | 4.2368 (0.0287 s)\n",
      "2770  | 3.4511 (0.0304 s)\n",
      "2780  | 4.0035 (0.0316 s)\n",
      "2790  | 3.9502 (0.0312 s)\n",
      "2800  | 4.1429 (0.0324 s)\n",
      "2810  | 3.4574 (0.0340 s)\n",
      "2820  | 3.7903 (0.0315 s)\n",
      "2830  | 4.3601 (0.0317 s)\n",
      "2840  | 3.2068 (0.0309 s)\n",
      "2850  | 6.0859 (0.0299 s)\n",
      "2860  | 4.4750 (0.0298 s)\n",
      "2870  | 5.5545 (0.0334 s)\n",
      "2880  | 3.8391 (0.0280 s)\n",
      "2890  | 2.9373 (0.0283 s)\n",
      "2900  | 3.2293 (0.0291 s)\n",
      "2910  | 4.0893 (0.0393 s)\n",
      "2920  | 3.9960 (0.0350 s)\n",
      "2930  | 4.6881 (0.0342 s)\n",
      "2940  | 4.2821 (0.0383 s)\n",
      "2950  | 4.4620 (0.0402 s)\n",
      "2960  | 4.6531 (0.0382 s)\n",
      "2970  | 4.3120 (0.0418 s)\n",
      "2980  | 3.4969 (0.0428 s)\n",
      "2990  | 3.5192 (0.0377 s)\n",
      "3000  | 2.7301 (0.0405 s)\n",
      "Eval: (Array(4.3788567, dtype=float32), Array(0.04351256, dtype=float32))\n",
      "3010  | 4.0013 (0.0292 s)\n",
      "3020  | 3.7007 (0.0296 s)\n",
      "3030  | 3.5966 (0.0290 s)\n",
      "3040  | 3.6075 (0.0298 s)\n",
      "3050  | 3.6410 (0.0324 s)\n",
      "3060  | 3.8840 (0.0283 s)\n",
      "3070  | 3.7998 (0.0316 s)\n",
      "3080  | 5.5977 (0.0290 s)\n",
      "3090  | 3.4265 (0.0311 s)\n",
      "3100  | 5.5687 (0.0293 s)\n",
      "3110  | 4.1413 (0.0300 s)\n",
      "3120  | 3.7820 (0.0285 s)\n",
      "3130  | 4.2690 (0.0306 s)\n",
      "3140  | 5.7063 (0.0301 s)\n",
      "3150  | 4.8899 (0.0279 s)\n",
      "3160  | 3.8687 (0.0281 s)\n",
      "3170  | 3.0793 (0.0281 s)\n",
      "3180  | 4.7662 (0.0297 s)\n",
      "3190  | 4.2780 (0.0292 s)\n",
      "3200  | 5.0705 (0.0309 s)\n",
      "3210  | 3.9675 (0.0311 s)\n",
      "3220  | 3.9744 (0.0410 s)\n",
      "3230  | 3.5184 (0.0399 s)\n",
      "3240  | 4.8569 (0.0387 s)\n",
      "3250  | 3.5098 (0.0389 s)\n",
      "3260  | 3.1975 (0.0389 s)\n",
      "3270  | 2.9304 (0.0416 s)\n",
      "3280  | 3.4968 (0.0386 s)\n",
      "3290  | 3.9902 (0.0375 s)\n",
      "3300  | 3.7823 (0.0386 s)\n",
      "3310  | 3.3780 (0.0406 s)\n",
      "3320  | 3.3894 (0.0361 s)\n",
      "3330  | 5.5905 (0.0391 s)\n",
      "3340  | 3.4839 (0.0376 s)\n",
      "3350  | 2.6816 (0.0396 s)\n",
      "3360  | 2.7978 (0.0396 s)\n",
      "3370  | 4.4141 (0.0361 s)\n",
      "3380  | 2.6877 (0.0354 s)\n",
      "3390  | 3.3906 (0.0357 s)\n",
      "3400  | 3.8732 (0.0384 s)\n",
      "3410  | 2.4281 (0.0384 s)\n",
      "3420  | 2.5416 (0.0401 s)\n",
      "3430  | 2.1418 (0.0385 s)\n",
      "3440  | 4.6911 (0.0360 s)\n",
      "3450  | 4.8318 (0.0367 s)\n",
      "3460  | 4.3268 (0.0391 s)\n",
      "3470  | 3.1224 (0.0354 s)\n",
      "3480  | 5.5375 (0.0379 s)\n",
      "3490  | 4.3479 (0.0406 s)\n",
      "3500  | 3.3922 (0.0419 s)\n",
      "Eval: (Array(3.7637143, dtype=float32), Array(0.02937813, dtype=float32))\n",
      "3510  | 3.3398 (0.0254 s)\n",
      "3520  | 2.8897 (0.0255 s)\n",
      "3530  | 2.7114 (0.0254 s)\n",
      "3540  | 3.4937 (0.0254 s)\n",
      "3550  | 3.3587 (0.0255 s)\n",
      "3560  | 3.0071 (0.0255 s)\n",
      "3570  | 4.8733 (0.0254 s)\n",
      "3580  | 4.7980 (0.0254 s)\n",
      "3590  | 3.5214 (0.0255 s)\n",
      "3600  | 4.1595 (0.0276 s)\n",
      "3610  | 4.3453 (0.0302 s)\n",
      "3620  | 3.9352 (0.0261 s)\n",
      "3630  | 4.0875 (0.0284 s)\n",
      "3640  | 3.9914 (0.0317 s)\n",
      "3650  | 3.6141 (0.0309 s)\n",
      "3660  | 2.1352 (0.0291 s)\n",
      "3670  | 3.3390 (0.0272 s)\n",
      "3680  | 3.9529 (0.0273 s)\n",
      "3690  | 2.7588 (0.0301 s)\n",
      "3700  | 2.4239 (0.0286 s)\n",
      "3710  | 5.9855 (0.0286 s)\n",
      "3720  | 4.9886 (0.0286 s)\n",
      "3730  | 4.4788 (0.0287 s)\n",
      "3740  | 2.3491 (0.0300 s)\n",
      "3750  | 3.4406 (0.0288 s)\n",
      "3760  | 3.5542 (0.0309 s)\n",
      "3770  | 4.0399 (0.0296 s)\n",
      "3780  | 3.3371 (0.0285 s)\n",
      "3790  | 2.9794 (0.0287 s)\n",
      "3800  | 2.7365 (0.0287 s)\n",
      "3810  | 3.1957 (0.0294 s)\n",
      "3820  | 1.8796 (0.0291 s)\n",
      "3830  | 2.7289 (0.0292 s)\n",
      "3840  | 2.1272 (0.0304 s)\n",
      "3850  | 2.6383 (0.0298 s)\n",
      "3860  | 3.5916 (0.0262 s)\n",
      "3870  | 4.1713 (0.0257 s)\n",
      "3880  | 3.5870 (0.0259 s)\n",
      "3890  | 2.5176 (0.0257 s)\n",
      "3900  | 4.0874 (0.0258 s)\n",
      "3910  | 5.6174 (0.0261 s)\n",
      "3920  | 2.8212 (0.0260 s)\n",
      "3930  | 3.5862 (0.0263 s)\n",
      "3940  | 3.1889 (0.0255 s)\n",
      "3950  | 3.0969 (0.0255 s)\n",
      "3960  | 2.4704 (0.0255 s)\n",
      "3970  | 3.1280 (0.0254 s)\n",
      "3980  | 4.8266 (0.0254 s)\n",
      "3990  | 3.5765 (0.0256 s)\n",
      "4000  | 2.7764 (0.0255 s)\n",
      "Eval: (Array(3.445064, dtype=float32), Array(0.03250875, dtype=float32))\n",
      "4010  | 3.7846 (0.0255 s)\n",
      "4020  | 3.5758 (0.0256 s)\n",
      "4030  | 2.4370 (0.0254 s)\n",
      "4040  | 3.5606 (0.0255 s)\n",
      "4050  | 4.0468 (0.0256 s)\n",
      "4060  | 3.5196 (0.0255 s)\n",
      "4070  | 3.8907 (0.0254 s)\n",
      "4080  | 4.0926 (0.0255 s)\n",
      "4090  | 2.3423 (0.0255 s)\n",
      "4100  | 2.6298 (0.0255 s)\n",
      "4110  | 4.9901 (0.0255 s)\n",
      "4120  | 2.6636 (0.0255 s)\n",
      "4130  | 3.0473 (0.0256 s)\n",
      "4140  | 2.4185 (0.0255 s)\n",
      "4150  | 3.2098 (0.0255 s)\n",
      "4160  | 3.3133 (0.0255 s)\n",
      "4170  | 3.2571 (0.0255 s)\n",
      "4180  | 3.3859 (0.0255 s)\n",
      "4190  | 2.6528 (0.0255 s)\n",
      "4200  | 1.7399 (0.0255 s)\n",
      "4210  | 2.5330 (0.0255 s)\n",
      "4220  | 2.1591 (0.0255 s)\n"
     ]
    }
   ],
   "source": [
    "init_params, pre_state, pre_evals, post_state, pre_losses, post_losses, post_evals = unlimtd_f_uni_modal_infinite(seed=seed,\n",
    "                                                                                     pre_n_epochs=7500,\n",
    "                                                                                     pre_n_tasks=24,\n",
    "                                                                                     pre_K=10,\n",
    "                                                                                     post_n_epochs=7500,\n",
    "                                                                                     post_n_tasks=24,\n",
    "                                                                                     post_K=10,\n",
    "                                                                                     data_noise=0.05, \n",
    "                                                                                     maddox_noise=0.05,\n",
    "                                                                                     meta_lr=0.001,\n",
    "                                                                                     subspace_dimension=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2e0d5-c4c9-49eb-9c10-9b734e15a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output[\"seed\"] = seed\n",
    "\n",
    "output[\"pre_n_epochs\"]=30000\n",
    "output[\"pre_n_tasks\"]=24\n",
    "output[\"pre_K\"]=10\n",
    "output[\"post_n_epochs\"]=30000\n",
    "output[\"post_n_tasks\"]=24\n",
    "output[\"post_K\"]=10\n",
    "output[\"data_noise\"]=0.05\n",
    "output[\"maddox_noise\"]=0.05\n",
    "output[\"meta_lr\"]=0.001\n",
    "output[\"subspace_dimension\"]=10\n",
    "output[\"pre_losses\"]=pre_losses\n",
    "output[\"post_losses\"]=post_losses\n",
    "output[\"init_params\"]=init_params\n",
    "output[\"intermediate_params\"]=pre_state.params\n",
    "output[\"trained_params\"]=post_state.params\n",
    "output[\"intermediate_mean\"]=pre_state.mean\n",
    "output[\"trained_mean\"]=post_state.mean\n",
    "output[\"intermediate_batch_stats\"]=pre_state.batch_stats\n",
    "output[\"trained_batch_stats\"]=post_state.batch_stats\n",
    "output[\"trained_scale\"]=post_state.scale\n",
    "output[\"proj\"]=post_state.proj\n",
    "output[\"pre_evals\"]=pre_evals\n",
    "output[\"post_evals\"]=post_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2feddb-f127-4fa6-87c8-39c291866f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_no_zeroth_order.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b512739-7cb9-4ada-b6e3-b6dfbdd81c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs_final/fim_infinite_no_zeroth_order.pickle\", \"rb\") as handle:\n",
    "    output = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d670f-320d-4d4f-b4e8-c488e2396979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.small_network(40, \"relu\", 1)\n",
    "apply_fn = utils.apply_fn_wrapper(model.apply, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3980a3-ffcf-4e75-865c-3bea596accc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel, kernel_self, jac = ntk.get_kernel_and_jac_lowdim_cov(apply_fn, output[\"trained_params\"], output[\"trained_scale\"], output[\"trained_batch_stats\"], output[\"proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e894-9ccf-4674-a9e1-a93981b51b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nll import gaussian_posterior_full\n",
    "\n",
    "def plot_notebooks(key, part_apply_fn, kernel, kernel_self, jac, mean, K, dataset_provider):\n",
    "    \"\"\"\n",
    "    Make an informative prediction plot in the singGP case (for the kernel specified)\n",
    "    K is the number of context inputs\n",
    "    Change dataset_provider to test on other datasets (e.g. dataset_sines_infinite)\n",
    "    \"\"\"\n",
    "    x, y, fun = dataset_provider.get_fancy_test_batch(key, K=10, L=0, data_noise=0.05)\n",
    "\n",
    "    x_a_all = x[0, :10]\n",
    "    y_a_all = y[0, :10]\n",
    "    x_b = np.linspace(-5, 5, 100)[:, np.newaxis]\n",
    "    y_b = fun(x_b)\n",
    "\n",
    "    y_min, y_max = np.min(y_b) - 0.5, np.max(y_b) + 0.5\n",
    "\n",
    "    correction_a_all = utils.falseaffine_correction0(jac, mean, x_a_all)\n",
    "    correction_b = utils.falseaffine_correction0(jac, mean, x_b)\n",
    "\n",
    "    x_a = x_a_all[:K]\n",
    "    y_a = y_a_all[:K]\n",
    "    correction_a = correction_a_all[:K]\n",
    "\n",
    "    prediction, cov = gaussian_posterior_full(kernel, kernel_self, x_a, y_a - correction_a, x_b, 0.05)\n",
    "    prediction = prediction + correction_b\n",
    "\n",
    "    error = dataset_provider.error_fn(prediction, y_b)\n",
    "    loss = nll(kernel_self, x_a, y_a - correction_a, maddox_noise=0.05)\n",
    "\n",
    "    variances = np.diag(cov)\n",
    "    stds = np.sqrt(variances)\n",
    "\n",
    "    plt.plot(x_b, y_b, \"g--\", label=\"Target\")\n",
    "    plt.plot(x_b, part_apply_fn(x_b), \"k--\", label=\"apply_fn\")\n",
    "    plt.plot(x_b, correction_b, \"p--\", label=\"correction_b\")\n",
    "    plt.plot(x_a, y_a, \"ro\", label=\"Context data\")\n",
    "    plt.plot(x_b, prediction, \"b\", label=\"Prediction\")\n",
    "    plt.fill_between(x_b[:, 0], prediction[:, 0] - 1.96 * stds, prediction[:, 0] + 1.96 * stds, color='blue', alpha=0.1, label=\"+/- 1.96$\\sigma$\")\n",
    "    plt.title(f\"NLL={loss:.4f}, MSE={error:.4f} ($K$={K})\")\n",
    "    plt.legend()\n",
    "    plt.gca().set_ylim([y_min, y_max])\n",
    "    plt.gca().set_xlabel(\"$x$\")\n",
    "    plt.gca().set_ylabel(\"$y$\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a535e-3b83-4637-b609-47e65df82237",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ce2a6-e479-4d27-a69c-2f8e15b25782",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_apply_fn = partial(apply_fn, output[\"trained_params\"], output[\"trained_batch_stats\"])\n",
    "key, subkey = random.split(key)\n",
    "plot_notebooks(subkey, part_apply_fn, kernel, kernel_self, jac, output[\"trained_mean\"], 10, dataset_sines_infinite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9843d7-29df-4e97-9f90-97c4020c62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_nll_no_mean = [x[0] for x in output[\"pre_evals\"]]\n",
    "posttraining_nll_no_mean = [x[0] for x in output[\"post_evals\"]]\n",
    "\n",
    "n_samples_pretraining = len(pretraining_nll_no_mean)\n",
    "n_samples_posttraining = len(posttraining_nll_no_mean)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(range(n_samples_pretraining), pretraining_nll_no_mean, \"g\", label=\"pretraining_nll\")\n",
    "plt.plot(range(n_samples_posttraining-1, n_samples_posttraining+n_samples_posttraining-1), pretraining_nll_no_mean, \"r\", label=\"projtraining_nll\")\n",
    "plt.gca().set_ylim([-10, 20])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac1c19-a8f1-4e05-8ff0-9b6de711242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_mse_no_mean = [x[1] for x in output[\"pre_evals\"]]\n",
    "posttraining_mse_no_mean = [x[1] for x in output[\"post_evals\"]]\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(range(n_samples_pretraining), pretraining_mse_no_mean, \"g--\", label=\"pretraining_mse\")\n",
    "plt.plot(range(n_samples_posttraining-1, n_samples_posttraining+n_samples_posttraining-1), posttraining_mse_no_mean, \"r--\", label=\"projtraining_mse\")\n",
    "plt.gca().set_ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54ea22-2c63-4dd6-b9ee-88830a62196b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
