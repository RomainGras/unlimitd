{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa917c2-6bd8-49d3-8b63-b801f15544b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_UNIFIED_MEMORY=1\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_UNIFIED_MEMORY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6196ab9-c7bb-4359-b830-7fb160062044",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from jax import numpy as np\n",
    "from flax import struct\n",
    "from typing import Any, Callable\n",
    "from flax import core\n",
    "import optax\n",
    "import models\n",
    "from jax import random\n",
    "import dataset_sines_finite\n",
    "from jax import value_and_grad, grad\n",
    "from functools import partial\n",
    "from jax.tree_util import tree_map\n",
    "from jax import jit\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from jax.lax import scan\n",
    "import pickle\n",
    "import dataset_sines_infinite\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b063698-f9d8-48c3-8f10-54f94824f06f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cec7c4c-8f55-4a3d-93a0-81610c38cdb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sine config\n",
    "config = {}\n",
    "config[\"n_epochs\"] = 70000\n",
    "config[\"n_tasks_per_epoch\"] = 24\n",
    "config[\"K\"] = 10\n",
    "config[\"L\"] = 10\n",
    "config[\"n_updates\"] = 5\n",
    "config[\"n_updates_test\"]= 10\n",
    "config[\"lr\"] = 0.001\n",
    "config[\"data_noise\"] = 0.05\n",
    "config[\"n_test_tasks\"] = 100\n",
    "\n",
    "config[\"x_dim\"] = 1\n",
    "config[\"y_dim\"] = 1\n",
    "config[\"nn_layers\"] = [128,128, 32]\n",
    "config[\"activation\"] = 'tanh'\n",
    "config[\"sigma_eps\"] = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df32868c-3a72-4b21-9e77-fce3fb20aa42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_test_batch(key, n_tasks, K, L, data_noise):\n",
    "    x, y = get_raw_batch(key, n_tasks, K, L, data_noise)\n",
    "    \n",
    "    return x[:, :K], y[:, :K], x[:, K:], y[:, K:]\n",
    "\n",
    "def get_raw_batch(key, n_tasks, K, L, data_noise):\n",
    "    # set this higher for a multi-dimensional regression\n",
    "    reg_dim = 1\n",
    "\n",
    "    key_x, key = random.split(key)\n",
    "    x = random.uniform(key_x, shape = (n_tasks, K+L, 1), minval=-5, maxval=5)\n",
    "    \n",
    "    y = np.empty( (n_tasks, K+L, reg_dim) )\n",
    "    \n",
    "    def f(task_index, value):\n",
    "        y, key = value\n",
    "\n",
    "        key_fun, key_noise, key = random.split(key, 3)\n",
    "\n",
    "        function = draw_multi(key_fun, reg_dim)\n",
    "        y = y.at[task_index, :K, :].set(function(x[task_index, :K]) + random.normal(key_noise, shape=(K, reg_dim)) * data_noise)\n",
    "        y = y.at[task_index, K:, :].set(function(x[task_index, K:]))\n",
    "\n",
    "        return (y, key)\n",
    "            \n",
    "    return x, lax.fori_loop(0, n_tasks, f, (y, key) )[0]\n",
    "\n",
    "def draw_multi(key, reg_dim, amp_low=0.1, amp_high=5, phase_low=0, phase_high=np.pi):\n",
    "    key_amp, key_phase = random.split(key)\n",
    "    \n",
    "    amps = random.uniform(key_amp, shape=(reg_dim,), minval=amp_low, maxval=amp_high)\n",
    "    phases = random.uniform(key_phase, shape=(reg_dim,), minval=phase_low, maxval=phase_high)\n",
    "    \n",
    "    def function(x):\n",
    "        return amps * np.sin(x + phases) + 1\n",
    "        \n",
    "    return vmap(function)\n",
    "\n",
    "def get_train_batch_fn(key):\n",
    "    # uncomment this line to train on infinite dataset\n",
    "    return dataset_sines_infinite.get_test_batch(key, config[\"n_tasks_per_epoch\"], config[\"K\"], config[\"L\"], config[\"data_noise\"])\n",
    "\n",
    "    # uncomment this line to train on finite dataset\n",
    "    #return sine_dataset_offset_finite.get_train_batch_as_val_batch(key, config[\"n_tasks_per_epoch\"], config[\"K\"], config[\"L\"], config[\"data_noise\"])\n",
    "    \n",
    "def get_test_batch_fn(key):\n",
    "    return dataset_sines_infinite.get_test_batch(key, config[\"n_test_tasks\"], config[\"K\"], config[\"L\"], config[\"data_noise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd9934a9-236b-4bd7-90d9-ef0cca5a2637",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # draw n_sample (x,y) pairs drawn from n_func functions\n",
    "    # returns (x,y) where each has size [n_func, n_samples, x/y_dim]\n",
    "    def sample(self, n_funcs, n_samples):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SinusoidDataset(Dataset):\n",
    "    def __init__(self, config, key, noise_var=None, rng=None):\n",
    "        self.key = key\n",
    "        if noise_var is None:\n",
    "            self.noise_std = np.sqrt( config['data_noise'] )\n",
    "        else:\n",
    "            self.noise_std = np.sqrt( noise_var )\n",
    "\n",
    "    def sample(self, n_funcs, n_samples, return_lists=False):\n",
    "        batch = get_test_batch(self.key, n_funcs, n_samples, 0, self.noise_std)\n",
    "        return batch[0], batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f408e4-78cf-4b24-bd5d-6916399b2845",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[[2.2542226 ],\n",
      "        [0.8597934 ],\n",
      "        [2.4556649 ]],\n",
      "\n",
      "       [[0.13030052],\n",
      "        [3.881017  ],\n",
      "        [3.837769  ]]], dtype=float32), Array([[[ 0.8957337 ],\n",
      "        [ 3.2626424 ],\n",
      "        [-0.37934366]],\n",
      "\n",
      "       [[ 1.4671689 ],\n",
      "        [ 0.14199564],\n",
      "        [-0.1838236 ]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "noise1 = 0.1\n",
    "noise2 = 0.3\n",
    "noise3 = 0.5\n",
    "dataset1 = SinusoidDataset(config, noise_var=noise1, key=random.PRNGKey(0))\n",
    "dataset2 = SinusoidDataset(config, noise_var=noise2, key=random.PRNGKey(0))\n",
    "dataset3 = SinusoidDataset(config, noise_var=noise3, key=random.PRNGKey(0))\n",
    "\n",
    "print(dataset1.sample(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ba40e-50ca-44d5-bb93-cebc5b6ed85c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_a[0], y_a[0], \"ro\", label=\"Context\")\n",
    "plt.plot(x_b[0], y_b[0], \"rx\", label=\"Query\")\n",
    "plt.plot(x_b[0], predictions, \"+b\", label=\"Pred\")\n",
    "plt.plot(np.linspace(-5, 5, 100), apply_fn(output[\"trained_params\"], np.linspace(-5, 5, 100)[:, np.newaxis]), \"--\", label=\"Raw\", alpha=0.4)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4abdebd2-e4b9-4749-a034-8376325b16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, grad, value_and_grad, vmap\n",
    "from flax import linen as nn\n",
    "from jax.scipy.linalg import inv\n",
    "from jax.numpy.linalg import slogdet\n",
    "from jax.numpy import transpose, expand_dims, log, squeeze\n",
    "from flax.training import train_state\n",
    "from jax import random\n",
    "import optax\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "392f2982-bf63-45a4-bc53-ce0b4c0b0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALPaCA(nn.Module):\n",
    "    config: dict\n",
    "\n",
    "    def setup(self):\n",
    "        self.lr = self.config['lr']\n",
    "        self.x_dim = self.config['x_dim']\n",
    "        self.phi_dim = self.config['nn_layers'][-1]\n",
    "        self.y_dim = self.config['y_dim']\n",
    "        self.sigma_eps = self.config['sigma_eps']\n",
    "        self.preprocess = self.config['preprocess']\n",
    "        self.f_nom = self.config['f_nom']\n",
    "\n",
    "        last_layer = self.config['nn_layers'][-1]\n",
    "\n",
    "        if isinstance(self.sigma_eps, list):\n",
    "            self.SigEps = jnp.diag(jnp.array(self.sigma_eps))\n",
    "        else:\n",
    "            self.SigEps = self.sigma_eps * jnp.eye(self.y_dim)\n",
    "        self.SigEps = self.SigEps.reshape((1, 1, self.y_dim, self.y_dim))\n",
    "\n",
    "        self.K = self.param('K_init', nn.initializers.normal(), (last_layer, self.y_dim))\n",
    "        self.L_asym = self.param('L_asym', nn.initializers.normal(), (last_layer, last_layer))\n",
    "        self.L = self.L_asym @ self.L_asym.T\n",
    "\n",
    "        \n",
    "    def __call__(self, x, context_x, context_y, num_context):\n",
    "        phi = self.basis(x)\n",
    "        context_phi = self.basis(context_x)\n",
    "        f_nom_x = jnp.zeros_like(context_y)\n",
    "        f_nom_cx = jnp.zeros_like(context_y)\n",
    "        if self.f_nom is not None:\n",
    "            f_nom_x = self.f_nom(x)\n",
    "            f_nom_cx = self.f_nom(context_x)\n",
    "\n",
    "\n",
    "    def compute_total_loss(self, param, context_x, context_y, x, y, f_nom_cx, f_nom_x, num_context):\n",
    "        # Subtract f_nom from context points before BLR\n",
    "        phi = self.basis(x)\n",
    "        context_phi = self.basis(context_x)\n",
    "        f_nom_x = jnp.zeros_like(context_y)\n",
    "        f_nom_cx = jnp.zeros_like(context_y)\n",
    "        if self.f_nom is not None:\n",
    "            f_nom_x = self.f_nom(x)\n",
    "            f_nom_cx = self.f_nom(context_x)\n",
    "        context_y_blr = context_y - f_nom_cx\n",
    "        posterior_K, posterior_L_inv = self.batch_blr(context_phi, context_y_blr, num_context)\n",
    "        mu_pred, Sig_pred, predictive_nll = self.compute_pred_and_nll(phi, y, posterior_K, posterior_L_inv, f_nom_x)\n",
    "        total_loss = jnp.mean(predictive_nll)\n",
    "        return total_loss\n",
    "\n",
    "    @nn.compact\n",
    "    def basis(self, x):\n",
    "        inp = x if self.preprocess is None else self.preprocess(x)\n",
    "        for i, units in enumerate(self.config['nn_layers']):\n",
    "            dense_layer = nn.Dense(features=units, name=f\"layer_{i}\")  # Create Dense layer\n",
    "            inp = dense_layer(inp)  # Apply Dense layer\n",
    "            activation_func = getattr(nn, self.config['activation'])\n",
    "            inp = activation_func(inp)  # Apply activation function\n",
    "        return inp\n",
    "\n",
    "    def batch_blr(self, X, Y, num):\n",
    "        X = X[:num, :]\n",
    "        Y = Y[:num, :]\n",
    "        Ln_inv = inv(X.T @ X + self.L)\n",
    "        Kn = Ln_inv @ (X.T @ Y + self.L @ self.K)\n",
    "        return jax.lax.cond(num > 0, lambda: (Kn, Ln_inv), lambda: (self.K, inv(self.L)))\n",
    "\n",
    "    def compute_pred_and_nll(self, phi, y, posterior_K, posterior_L_inv, f_nom_x):\n",
    "        \"\"\"\n",
    "        Uses self.posterior_K and self.posterior_L_inv and self.f_nom_x to generate the posterior predictive.\n",
    "        Arguments:\n",
    "            posterior_K: Posterior weights K matrix\n",
    "            posterior_L_inv: Posterior inverse covariance matrix of weights\n",
    "            phi: Feature matrix for input x\n",
    "            f_nom_x: Nominal function values at x (if any)\n",
    "            y: Actual target values to compare against\n",
    "        Returns:\n",
    "            mu_pred: Posterior predictive mean at query points\n",
    "            Sig_pred: Posterior predictive variance at query points\n",
    "            predictive_nll: Negative log likelihood of y under the posterior predictive density\n",
    "        \"\"\"\n",
    "        mu_pred = posterior_K.T @ phi + f_nom_x\n",
    "        spread_fac = 1 + batch_quadform(posterior_L_inv, phi)\n",
    "        Sig_pred = expand_dims(spread_fac, axis=-1) * expand_dims(self.SigEps, (0, 0))\n",
    "    \n",
    "        # Score y under predictive distribution to obtain loss\n",
    "        logdet = self.y_dim * log(spread_fac) + slogdet(self.SigEps)\n",
    "        Sig_pred_inv = inv(Sig_pred)\n",
    "        quadf = batch_quadform(Sig_pred_inv, (y - mu_pred))\n",
    "    \n",
    "        predictive_nll = squeeze(logdet + quadf, axis=-1)\n",
    "    \n",
    "        return mu_pred, Sig_pred, predictive_nll\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30309fee-0de5-4c86-9c41-db0b7874ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul(mat, batch_v):\n",
    "    \"\"\"Batch matrix multiplication adjusted for the transposing logic in TensorFlow's batch_matmul.\"\"\"\n",
    "    return jnp.einsum('ijk,kj->ik', mat, batch_v)\n",
    "\n",
    "def batch_quadform(A, b):\n",
    "    \"\"\"Batch quadratic form using JAX. Handles different cases based on dimensions of A.\"\"\"\n",
    "    if A.ndim == b.ndim + 1:\n",
    "        # Same matrix A for all N vectors in b\n",
    "        return jnp.squeeze(jnp.einsum('ijk,kj->ki', A, jnp.expand_dims(b, axis=-1)), axis=-1)\n",
    "    elif A.ndim == b.ndim:\n",
    "        # Different A for each b\n",
    "        Ab = jnp.einsum('ijk,kj->ik', A, b)  # ... x N x n\n",
    "        return jnp.squeeze(jnp.einsum('ij,ij->i', b, Ab), axis=-1)  # ... x N\n",
    "    else:\n",
    "        raise ValueError('Matrix size of %d is not supported.' % A.ndim)\n",
    "\n",
    "def batch_2d_jacobian(y, x):\n",
    "    \"\"\"Compute the Jacobian of y with respect to x, handling batch dimensions.\"\"\"\n",
    "    y_dim = y.shape[-1]\n",
    "    x_dim = x.shape[-1]\n",
    "\n",
    "    def single_jacobian(yi, xi):\n",
    "        return jax.jacfwd(lambda x: yi)(xi)\n",
    "\n",
    "    batched_jacobian = vmap(single_jacobian, in_axes=(0, 0), out_axes=0)\n",
    "    return batched_jacobian(y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca579d4f-ced6-4cbe-884f-fce72e4298cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(key, rng, model, learning_rate=0.001):\n",
    "    # Create dummy inputs for all expected arguments\n",
    "    dummy_x = random.uniform(key, shape=(1, model.config['x_dim']), minval=-5, maxval=5)\n",
    "    dummy_y = random.uniform(key, shape=(1, model.config['y_dim']), minval=-5, maxval=5)\n",
    "    dummy_context_x = random.uniform(key, shape=(1, model.config['x_dim']), minval=-5, maxval=5)\n",
    "    dummy_context_y = random.uniform(key, shape=(1, model.config['y_dim']), minval=-5, maxval=5)\n",
    "    \n",
    "    dummy_num_context = 1  # A simple dummy number of contexts, e.g., 1\n",
    "\n",
    "    # Initialize parameters with full set of dummy inputs\n",
    "    params = model.init(rng, dummy_x, dummy_context_x, dummy_context_y, dummy_num_context)\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed264356-8292-4e3f-8024-2264140f0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_sines_infinite import get_training_batch \n",
    "\n",
    "@jit\n",
    "def train_step(state, batch_x, batch_y, num_context):\n",
    "    def loss_fn(params):\n",
    "        mu_pred, Sig_pred, predictive_nll = model.compute_total_loss(self, param, context_x, context_y, x, y, f_nom_cx, f_nom_x, num_context)\n",
    "        return jnp.mean(predictive_nll), (mu_pred, Sig_pred)\n",
    "    grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (mu_pred, Sig_pred)), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, mu_pred, Sig_pred\n",
    "\n",
    "def train(key, model, state, get_training_batch, num_train_updates):\n",
    "    for i in range(num_train_updates):\n",
    "        x_a, y_a, x_a_div, y_a_div = get_training_batch(key, n_tasks=model.config['meta_batch_size'], K=model.config['data_horizon'] + model.config['test_horizon'], data_noise=0.1, n_devices=1)\n",
    "        num_context = random.randint(key, shape=(model.config['meta_batch_size'],), minval=0, maxval=model.config['data_horizon'] + 1)\n",
    "        state, loss, mu_pred, Sig_pred = train_step(state, x[:, :model.config['data_horizon'], :], y[:, :model.config['data_horizon'], :], num_context)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'Iteration {i}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76557211-e97f-40dd-932c-bf863d5031f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def test(model, params, x_c, y_c, x):\n",
    "    mu_pred, Sig_pred, _ = model.apply(params, x_c, y_c, x)\n",
    "    return mu_pred, Sig_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86192d99-fb51-4c17-af0a-855a0e91d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(model, params, x):\n",
    "    phi = model.apply(params, x, method=model.encode)\n",
    "    return phi\n",
    "\n",
    "def save(params, model_path):\n",
    "    np.save(model_path, params)\n",
    "    print(f'Saved to: {model_path}')\n",
    "\n",
    "def restore(model_path):\n",
    "    params = np.load(model_path, allow_pickle=True).item()\n",
    "    print(f'Restored model from: {model_path}')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6532a591-e9df-445f-9db5-5480ad290067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and random key generation\n",
    "config = {\n",
    "    'lr': 0.001,\n",
    "    'x_dim': 10,\n",
    "    'nn_layers': [64, 32, 16],\n",
    "    'y_dim': 5,\n",
    "    'sigma_eps': 0.1,\n",
    "    'activation': 'relu',\n",
    "    'preprocess': None,\n",
    "    'f_nom': None,\n",
    "    'meta_batch_size': 10,\n",
    "    'data_horizon': 10,\n",
    "    'test_horizon': 20\n",
    "}\n",
    "key = random.PRNGKey(0)\n",
    "key_init, key = random.split(key)\n",
    "\n",
    "rng = random.PRNGKey(0)\n",
    "model = ALPaCA(config)\n",
    "state = create_train_state(key_init, rng, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e592601-8544-4813-9b9c-c3ff0dab7cbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ALPaCA.compute_total_loss() missing 3 required positional arguments: 'f_nom_cx', 'f_nom_x', and 'num_context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m key_train, key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_training_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(key, model, state, get_training_batch, num_train_updates)\u001b[0m\n\u001b[1;32m     15\u001b[0m x_a, y_a, x_a_div, y_a_div \u001b[38;5;241m=\u001b[39m get_training_batch(key, n_tasks\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], K\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_horizon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_horizon\u001b[39m\u001b[38;5;124m'\u001b[39m], data_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, n_devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m num_context \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(key, shape\u001b[38;5;241m=\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],), minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_horizon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m state, loss, mu_pred, Sig_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_horizon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_horizon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[61], line 9\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, batch_x, batch_y, num_context)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(predictive_nll), (mu_pred, Sig_pred)\n\u001b[1;32m      8\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m value_and_grad(loss_fn, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m (loss, (mu_pred, Sig_pred)), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, loss, mu_pred, Sig_pred\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[61], line 6\u001b[0m, in \u001b[0;36mtrain_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params):\n\u001b[0;32m----> 6\u001b[0m     mu_pred, Sig_pred, predictive_nll \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_total_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(predictive_nll), (mu_pred, Sig_pred)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/linen/module.py:1226\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1225\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1226\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: ALPaCA.compute_total_loss() missing 3 required positional arguments: 'f_nom_cx', 'f_nom_x', and 'num_context'"
     ]
    }
   ],
   "source": [
    "key_train, key = random.split(key)\n",
    "train(key_train, model, state, get_training_batch, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7225530-5435-4f19-b8c2-ec24d32af8b9",
   "metadata": {},
   "source": [
    "## Vérification de l'équivalence des méthodes de sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fd687e5-0608-40e3-ba00-4d3b9f8236e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # draw n_sample (x,y) pairs drawn from n_func functions\n",
    "    # returns (x,y) where each has size [n_func, n_samples, x/y_dim]\n",
    "    def sample(self, n_funcs, n_samples):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a9dab11-82ab-4578-89da-2e774cb38ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidDataset(Dataset):\n",
    "    def __init__(self, config, noise_var=None, rng=None):\n",
    "        self.amp_range = config['amp_range']\n",
    "        self.phase_range = config['phase_range']\n",
    "        self.freq_range = config['freq_range']\n",
    "        self.x_range = config['x_range']\n",
    "        if noise_var is None:\n",
    "            self.noise_std = np.sqrt( config['sigma_eps'] )\n",
    "        else:\n",
    "            self.noise_std = np.sqrt( noise_var )\n",
    "            \n",
    "        self.np_random = rng\n",
    "        if rng is None:\n",
    "            self.np_random = np.random\n",
    "\n",
    "    def sample(self, n_funcs, n_samples, return_lists=False):\n",
    "        x_dim = 1\n",
    "        y_dim = 1\n",
    "        x = np.zeros((n_funcs, n_samples, x_dim))\n",
    "        y = np.zeros((n_funcs, n_samples, y_dim))\n",
    "\n",
    "        amp_list = self.amp_range[0] + self.np_random.rand(n_funcs)*(self.amp_range[1] - self.amp_range[0])\n",
    "        phase_list = self.phase_range[0] + self.np_random.rand(n_funcs)*(self.phase_range[1] - self.phase_range[0])\n",
    "        freq_list = self.freq_range[0] + self.np_random.rand(n_funcs)*(self.freq_range[1] - self.freq_range[0])\n",
    "        for i in range(n_funcs):\n",
    "            x_samp = self.x_range[0] + self.np_random.rand(n_samples)*(self.x_range[1] - self.x_range[0])\n",
    "            y_samp = amp_list[i]*np.sin(freq_list[i]*x_samp + phase_list[i]) + self.noise_std*self.np_random.randn(n_samples)\n",
    "\n",
    "            x[i,:,0] = x_samp\n",
    "            y[i,:,0] = y_samp\n",
    "\n",
    "        if return_lists:\n",
    "            return x,y,freq_list,amp_list,phase_list\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf3e6147-fb76-44dd-b304-581b94b1755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 1) (10, 30, 1)\n",
      "(10, 30, 1) (10, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "import dataset_sines_infinite\n",
    "x_a, y_a, x_a_div, y_a_div = dataset_sines_infinite.get_training_batch(key, n_tasks=model.config['meta_batch_size'], K=model.config['data_horizon'] + model.config['test_horizon'], data_noise=0.1, n_devices=1)\n",
    "print(x_a.shape, y_a.shape)\n",
    "\n",
    "config_dataset = {\n",
    "    'amp_range': [0.1, 5.0],\n",
    "    'phase_range': [0, 3.14],\n",
    "    'freq_range': [0.999, 1.0],\n",
    "    'x_range': [-5., 5.],\n",
    "    'sigma_eps': 0.02,\n",
    "}\n",
    "dataset = SinusoidDataset(config_dataset)\n",
    "x, y = dataset.sample(n_funcs=model.config['meta_batch_size'], n_samples=model.config['data_horizon'] + model.config['test_horizon'])\n",
    "print(x.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
